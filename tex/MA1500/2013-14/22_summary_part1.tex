\documentclass[lecture]{csm}

% set meta information
\modulecode{MA1500}
\moduletitle{Introduction to Probability Theory}
\academicyear{2013/14}
\doctype{Lecture}
\doctitle{Module Summary Part I}
\docnumber{22}

% local
\newcommand{\prob}{\mathbb{P}}
\newcommand{\expe}{\mathbb{E}}
\newcommand{\var}{\text{Var}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\def\it{\item}
\def\bit{\begin{itemize}}
\def\eit{\end{itemize}} 
\def\ben{\begin{enumerate}}
\def\een{\end{enumerate}}
\newcommand{\lt}{<}
\newcommand{\gt}{>}

\newcommand{\proofomitted}{\par[\textit{Proof omitted.}]\par}

\renewcommand{\thesection}{Lecture~\arabic{section}.}

%======================================================================
\begin{document}
\maketitle
%======================================================================
%----------------------------------------------------------------------
\newpage
\section{Set Theory}
%----------------------------------------------------------------------
\bit
\it Set relations: $A\subseteq B$, $A\subset B$, $A=B$.
\it Set operations: 
	\bit
	\it Union: $A\cup B= \{a: a\in A \text{ or }a\in B\}$.
	\it Intersection: $A\cap B = \{a: a\in A \text{ and }a\in B\}$.
	\it Complementation: $A^c =\{a: a\notin A\}$.
	\eit
\it Fundamental laws of set algebra: 
	\bit
	\it Commutative law.
	\it Associative law.
	\it Distributive law.
	\eit
\it De Morgan's laws:
	\bit
	\it $(A\cup B)^c = A^c\cap B^c$.
	\it $(A\cap B)^c = A^c\cup B^c$.
	\eit
\eit
%----------------------------------------------------------------------
\newpage
\section{Events and Probability}
%----------------------------------------------------------------------
\bit
\it A \emph{random experiment} has a number of possible outcomes, exactly one of which will occur.
\it The set of possible outcomes is called the \emph{sample space}, $\Omega$.
\it An outcome is an \emph{element} of the sample space: $\omega\in\Omega$.
\it An event is a \emph{subset} of the sample space: $A\subset\Omega$.
\it The set of all subsets of $\Omega$ is called its \emph{power set}, denoted by $\mathcal{P}(\Omega)$.
\eit

\textbf{Finite sample spaces.}
\bit
\it \emph{Probability mass function}: a function $p:\Omega\to[0,1]$ such that 
$\displaystyle\sum_{\omega\in\Omega} p(\omega) = 1.$
\it \emph{Probability measure}: a function $\prob:\mathcal{P}(\Omega)\to[0,1]$ defined by
$\displaystyle\prob(A) = \sum_{\omega\in A} p(\omega).$
\eit
\textbf{Properties of probability measures on finite sample spaces.}
	\bit
	\it $\prob(\Omega)=1$.
	\it $\prob(A^c) = 1 - \prob(A)$.
	\it If $A$ and $B$ are disjoint, then $\prob(A\cup B) = \prob(A)+\prob(B)$.
	\eit
%----------------------------------------------------------------------
\newpage
\section{Classical Probability}
%----------------------------------------------------------------------
\bit
\it The principle of indifference: 
	\bit
	\it If $\Omega=\{1,2,\ldots,n\}$, then $p(\omega)=1/n$ for all $\omega\in\Omega$.
	\it We say that each outcome is \emph{equally likely}.
	\eit
\it Sampling with replacement: 
	\bit
	\it There are $n^k$ distinct choices of $k$ elements
	\eit
\it Sampling without replacement: 
	\bit
	\it There are $^nP_k = \displaystyle\frac{n!}{(n-k)!}$ distinct choices of $k$ ordered elements (sequences).
	\it There are $^nC_k = \displaystyle\frac{n!}{(n-k)!k!}$ distinct choices of $k$ un-ordered elements (sets).
	\eit
\eit
%----------------------------------------------------------------------
\newpage
\section{Conditional Probability}
%----------------------------------------------------------------------
\bit
\it The \emph{conditional probability of $A$ given $B$} is $\displaystyle\prob(A|B) = \frac{\prob(A\cap B)}{\prob(B)}$ (provided $\prob(B)>0$).
\it \emph{Law of total probability}: if $\{A_1,A_2,\ldots,A_n\}$ is a partition of $B$, then
\[
\prob(B) = \sum_{i=1}^n \prob(B|A_i)\prob(A_i).
\]
\it \emph{Bayes' theorem}: if $\{A_1,A_2,\ldots,A_n\}$ is a partition of $B$ and $\prob(B)>0$, then
\[
\prob(A_i|B) = \frac{\prob(B|A_i)\prob(A_i)}{\sum_{j=1}^n \prob(B|A_j)\prob(A_j)}.
\]
\it $A$ and $B$ are \emph{independent} if $\prob(A\cap B)=\prob(A)\prob(B)$.
\it $\{A_1,A_2,\ldots,A_n\}$ is \emph{pairwise independent} if $\prob(A_i\cap A_j)=\prob(A_i)\prob(A_j)$ for all $i\neq j$.
\it $\{A_1,A_2,\ldots,A_n\}$ is \emph{totally independent} if for any subset $\{B_1,\ldots,B_m\}\subseteq\{A_1,\ldots,A_n\}$,
\[
\prob(B_1\cap B_2\cap \ldots \cap B_m) = \prob(B_1)\prob(B_2)\cdots\prob(B_m)
\]
\eit
%----------------------------------------------------------------------
\newpage
\section{Relative Frequency}
%----------------------------------------------------------------------
\bit
\it A random experiment is repeated $N$ times.
\it Suppose that event $A$ occurs exactly $N(A)$ times.
\it Under the \emph{frequentist model}, the probability of event $A$ is defined by
\[
\text{Pr}(A) = \lim_{N\to\infty} \frac{N(A)}{N}.
\]
\it This definition of probability has nice properties
	\bit
	\it $\text{Pr}(\Omega) = 1$.
	\it $\text{Pr}(A^c) = 1-\text{Pr}(A)$.
	\it If $A\cap B=\emptyset$, then $\text{Pr}(A\cup B) = \text{Pr}(A)+\text{Pr}(B)$.
	\it If $\text{Pr}(B)>0$, then $\displaystyle\text{Pr}(A|B) = \frac{\text{Pr}(A\cap B)}{\text{Pr}(B)}$.
	\eit
\it This definition cannot be extended to experiments with infinitely many outcomes. 
\eit
%----------------------------------------------------------------------
\newpage
\section{Random Variables}
%----------------------------------------------------------------------
\bit
\it Let $\Omega$ be a finite sample space. A \emph{random variable} on $\Omega$ is any real-valued function 
\[
\begin{array}{rlcl}
X: 	& \Omega & \to  		& \R \\
	& \omega & \mapsto	& X(\omega).
\end{array}
\]	
\it Once the experiment has been performed, $X$ takes a particular value $X(\omega)\in\R$. 
\it Events such as $\{\omega:X(\omega)\in B\}$ are abbreviated to $\{X\in B\}$.
\it Probabilities such as $\prob(\{\omega:X(\omega)\in B\})$ are abbreviated to $\prob(X\in B)$.
\it The \emph{cumulative distribution function} (CDF) of a random variable $X:\Omega\to\R$ is 
\[
\begin{array}{cccl}
F:	& \mathbb{R}	& \longrightarrow	& [0,1] \\
	& x 			& \mapsto			& \prob(X\leq x).
\end{array}
\]
\it
The \emph{probability mass function} (PMF) of a random variable $X:\Omega\to\R$ is 
\[
\begin{array}{cccl}
p:	& \mathbb{R}	& \longrightarrow	& [0,1] \\
	& x 			& \mapsto			& \prob(X = x).
\end{array}
\]
\eit
%----------------------------------------------------------------------
\newpage
\section{Expectation}
%----------------------------------------------------------------------
Let $X:\Omega\to\R$ be a random variable on a finite sample space $\Omega$.
\bit
\it  The \emph{expectation} of $X$ is defined by a sum over the \emph{domain} of $X$:
\[
\expe(X) = \sum_{\omega\in\Omega} X(\omega)p(\omega)
\]
where $p(\omega)$ is the probability that $\omega\in\Omega$ occurs.
\it  
The expectation $\expe(X)$ can be computed as a sum over the \emph{range} $\{x_1,x_2,\ldots,x_n\}$ of $X$:
\[
\expe(X) = \sum_{i=1}^n x_i\,p(x_i)
\]
where $p(x_i)$ is the probability that $X$ takes the value $x_i\in\R$.
\eit
\textbf{Properties of expectation}:
\bit
\it Positivity: if $X\geq 0$ then $\expe(X)\geq 0$.
\it Linearity: $\expe(aX+bY) = a\expe(X) + b\expe(Y)$ for all $a,b\in\R$.
\it Monotonicity: if $X\geq Y$ then $\expe(X)\geq\expe(Y)$.
\eit
%----------------------------------------------------------------------
\newpage
\section{Moments}
%----------------------------------------------------------------------
Let $X:\Omega\to\R$ be a random variable on a finite sample space $\Omega$.
\bit
\it For any function $g:\R\to\R$, 
\[
\expe\big[g(X)\big] = \sum_{i=1}^n g(x_i)p(x_i)
\]
\it $\expe(X^k)$ is called the \emph{$k$th moment of $X$ about the origin}.
\it $\expe\big([X-\expe(X)]^k\big)$ is called the \emph{$k$th moment of $X$ about the mean}.
\it $\expe\big([X-\expe(X)]^2\big)$ is called the \emph{variance} of $X$, usually denoted by $\var(X)$.
\bit
\it $\var(X) = \expe(X^2) - \expe(X)^2$.
\it $\var(aX + b) = a^2\var(X)$.
\eit
\it $\expe(X)$ describes the \emph{location} of a distribution.
\it $\var(X)$ describes the \emph{scale} (or \emph{size}) of a distribution.
\it Higher moments (skewness, kurtosis, ...) describe the \emph{shape} of a distribution.
\eit
%----------------------------------------------------------------------
\newpage
\section{Uniform, Bernoulli and Binomial}
%----------------------------------------------------------------------
\textbf{Uniform distribution}. One parameter: $n\in\N$.
\bit
\it Range: $\{1,2,\ldots,n\}$.
\it $\prob(X=k) = 1/n$.
\it $\expe(X) = (n+1)/2$ and $\var(X) = (n-1)(n+1)/12$.
\eit
\vspace*{2ex}
\textbf{Bernoulli distribution}. One parameter: $p\in(0,1)$.
\bit
\it Range: $\{0,1\}$
\it $\prob(X=0) = 1-p$, $\prob(X=1) = p$.
\it $\expe(X) = p$ and $\var(X) = p(1-p)$.
\eit
\vspace*{2ex}
\textbf{Binomial distribution}. Two parameters: $n\in\N$ and $p\in(0,1)$.
\bit
\it Range: $\{0,1,2,\ldots,n\}$
\it $\prob(X=k) = \binom{n}{k}p^k(1-p)^{n=k}$.
\it $\expe(X) = np$ and $\var(X) = np(1-p)$.
\eit

%======================================================================
\end{document}
%======================================================================
