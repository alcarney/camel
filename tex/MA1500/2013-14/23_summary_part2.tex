\documentclass[lecture]{csm}

% set meta information
\modulecode{MA1500}
\moduletitle{Introduction to Probability Theory}
\academicyear{2013/14}
\doctype{Lecture}
\doctitle{Module Summary Part II}
\docnumber{23}

% local
\newcommand{\prob}{\mathbb{P}}
\newcommand{\expe}{\mathbb{E}}
\newcommand{\var}{\text{Var}}
\newcommand{\cov}{\text{Cov}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\def\it{\item}
\def\bit{\begin{itemize}}
\def\eit{\end{itemize}} 
\def\ben{\begin{enumerate}}
\def\een{\end{enumerate}}
\newcommand{\lt}{<}
\newcommand{\gt}{>}

\newcommand{\proofomitted}{\par[\textit{Proof omitted.}]\par}

\renewcommand{\thesection}{Lecture~\arabic{section}.}

%======================================================================
\begin{document}
\maketitle
%\tableofcontents
%======================================================================

\setcounter{section}{9}
%----------------------------------------------------------------------
\newpage
\section{Joint Distributions}
%----------------------------------------------------------------------
Let $X,Y:\Omega\to\R$ be random variables on a finite probability space $(\Omega,\prob)$.
\bit
\it The \emph{joint CDF} of $X$ and $Y$ is the function $F_{X,Y}(x,y)=\prob(X\leq x, Y\leq y)$.
\it The \emph{marginal CDF} of $X$ is the function $F_X(x)=\prob(X\leq x)$.
\it The \emph{marginal CDF} of $Y$ is the function $F_Y(y)=\prob(Y\leq y)$.
\eit
\vspace*{1ex}
\bit
\it The \emph{joint PMF} of $X$ and $Y$ is the function $p_{X,Y}(x,y)=\prob(X=x,Y=y)$.
\it The \emph{marginal PMF} of $X$ is the function $p_X(x)=\prob(X=x)$.
\it The \emph{marginal PMF} of $Y$ is the function $p_Y(y)=\prob(Y=y)$.
\eit
\vspace*{1ex}
\bit
\it $X$ and $Y$ are \emph{independent} if
\[
p_{X,Y}(x,y) = p_X(x)p_Y(y)\text{\quad for all $x,y\in\R$.}
\]
or equivalently, if
\[
\prob(X=x,Y=y)=\prob(X=x)\prob(Y=y)\text{\quad for all $x,y\in\R$.}
\]
\eit

%----------------------------------------------------------------------
\newpage
\section{Covariance and Correlation}
%----------------------------------------------------------------------
Let $X,Y:\Omega\to\R$ be random variables on a finite probability space $(\Omega,\prob)$.
\bit
%\it For any function $g:\R^2\to\R$, 
%\[
%\expe\big[g(X,Y)\big] = \sum_{i=1}^m\sum_{j=1}^n g(x_i,y_j)p_{X,Y}(x_i,y_j)
%\]
\it $\expe(XY)$ is called the \emph{product moment} of $X$ and $Y$.
%\it The product moments quantifies the \emph{dependence} between $X$ and $Y$.
	\bit
	\it $X$ and $Y$ are \emph{correlated} if $\expe(XY) \neq \expe(X)\expe(Y)$.
	\it If $X$ and $Y$ are independent, then $\expe(XY) = \expe(X)\expe(Y)$ (i.e.\ they are uncorrelated).
	\it If $X$ and $Y$ are uncorrelated, then $\var(X+Y) = \var(X)+\var(Y)$.
	\eit
\it $\expe\big[(X-\expe X)(Y-\expe Y)\big]$ is called the \emph{covariance} of $X$ and $Y$, denoted by $\cov(X,Y)$.
	\bit
	\it $\cov(X) = \expe(XY) - \expe(X)\expe(Y)$.
	\eit
\it The \emph{correlation coefficient} of $X$ and $Y$ is 
\[
\rho(X,Y) = \frac{\cov(X,Y)}{\sqrt{\var(X)\cdot\var(Y)}}.
\]
\it Cauchy-Schwarz Inequality: $\expe(XY)^2 \leq \expe(X^2)\expe(Y^2)$.
\it Consequently, we have that $|\rho(X,Y)|\leq 1$.
\eit


%----------------------------------------------------------------------
\newpage
\section{Conditional Distributions}
%----------------------------------------------------------------------
Let $X,Y:\Omega\to\R$ be random variables on a finite probability space $(\Omega,\prob)$.
\bit
\it Let $x\in\R$ be a fixed value, with $\prob(X=x)>0$.
\it The \emph{conditional CDF of $Y$ given $X=x$} is the function $F_{Y|X}(y|x)=\prob(Y\leq y|X=x)$.
\it The \emph{conditional PMF of $Y$ given $X=x$} is the function $p_{Y|X}(y|x)=\prob(Y=y|X=x)$.
\it $X$ and $Y$ are independent if and only if $p_{Y|X}(y|x) = p_Y(y)$ for all $x,y\in\R$.
\it The \emph{conditional expectation of $Y$ given $X=x$} is a fixed number:
\[
\expe(Y|X=x) 
	= \sum_y y\,p_{Y|X}(y|x).
	\]
\it The \emph{conditional expectation of $Y$ given $X$} is a random variable:
\[\begin{array}{llll}
\expe(Y|X):	& \R 	& \to 		& \R \\
			& x		& \mapsto 	& \expe(Y|X=x)
\end{array}\]
\it Law of Total Expectation: $\expe\big[\expe(Y|X)\big] = \expe(Y)$.
\eit

%----------------------------------------------------------------------
\newpage
\section{Countability}
%----------------------------------------------------------------------

\bit
\it Natural numbers: $\N = \{1,2,3,\ldots\}$.
\it Integers: $\Z = \{\ldots,-2,-1,0,1,2,\ldots\}$.
\it Rational numbers: $\Q = \{a/b:a\in\Z,b\in\N\}$.
\it Real numbers: $\R$.
\eit

\vspace*{1ex}
\bit
\it A \emph{bijection} is a \emph{one-to-one correspondence} $f:A\to B$ between the elements of $A$ and $B$.
\it A set $A$ is called \emph{finite} if there is a bijection $\phi:A\to\{1,2,\ldots,n\}$.
\it An infinite set $A$ is called \emph{countable} if there is a bijection $\phi:A\to\N$.
\eit

\vspace*{1ex}
\bit
\it $\Z$ is countable.
\it $\Q$ is countable.
\it $\R$ is uncountable.
\it The power set of $\N$ is uncountable.
\eit
%----------------------------------------------------------------------
\newpage
\section{Discrete Probability}
%----------------------------------------------------------------------
Let $\Omega$ be a countable sample space. 
\bit
\it \emph{Probability mass function}: a function $p:\Omega\to[0,1]$ such that 
$\displaystyle\sum_{\omega\in\Omega} p(\omega) = 1.$
\it \emph{Probability measure}: a function $\prob:\mathcal{P}(\Omega)\to[0,1]$ defined by
$\displaystyle\prob(A) = \sum_{\omega\in A} p(\omega).$
\it The pair $(\Omega,\prob)$ is called a \emph{discrete probability space}.
\eit
\vspace*{1ex}
A random variable $X:\Omega\to\R$ is called
\bit
\it \emph{simple} if it can take only a finite number of distinct values, and 
\it \emph{discrete} if it can take at most a countable number of distinct values.
\eit
The PMF of a discrete random variable $X$ is the function $p_X(x)=\prob(X=x)$.
\bit
\it Expectation:
\[
\expe(X) = \displaystyle\sum_{\omega\in\Omega} X(\omega)p(\omega)
\text{\qquad or\qquad}
\expe(X) = \sum_{i=1}^{\infty} x_i\,\prob(X=x_i).
\]
\eit
%----------------------------------------------------------------------
\newpage
\section{The Geometric and Poisson Distributions}
%----------------------------------------------------------------------
\textbf{Geometric distribution}. One parameter: $p\in(0,1)$.
\bit
\it Range: $\{1,2,\ldots\}$.
\it $\prob(X=k) = (1-p)^{k-1}p$.
\it $\expe(X) = 1/p$ and $\var(X) = (1-p)/p^2$.
\eit
\vspace*{1ex}
\textbf{Negative Binomial distribution}. Two parameters: $r\in\N$ and $p\in(0,1)$.
\bit
\it Range: $\{r,r+1,\ldots\}$
\it $\prob(X=k) = \binom{k-1}{r-1}(1-p)^{k-r}p^{r}$.
\it $\expe(X) = r/p$ and $\var(X) = r(1-p)/p^2$.
\eit
\vspace*{1ex}
\textbf{Poisson distribution}. One parameter: $\lambda>0$.
\bit
\it Range: $\{0,1,2,\ldots\}$
\it $\prob(X=k) = \displaystyle\frac{\lambda^k}{k!}e^{-\lambda}$.
%\it $\prob(X=k) = \frac{\lambda^k}{k!}e^{-\lambda}$.
\it $\expe(X) = \lambda$ and $\var(X) = \lambda$.
\eit
%----------------------------------------------------------------------
\newpage
\section{Fields of Sets}
%----------------------------------------------------------------------
Let $\Omega$ be any set.
\bit
\it The set of all subsets of $\Omega$ is called its \emph{power set}: $\mathcal{P}(\Omega) = \{A:A\subseteq\Omega\}$.
\it Any subset of $\mathcal{P}(\Omega)$ is called a \emph{collection of sets} over $\Omega$. 
\it A collection of sets $\mathcal{F}$ over $\Omega$ is called a \emph{field of sets} over $\Omega$ if 
	\bit
	\it $\Omega\in\mathcal{F}$,
	\it $\mathcal{F}$ is closed under complementation: if $A\in\mathcal{F}$ then $A^c\in\mathcal{F}$, and
	\it $\mathcal{F}$ is closed under pairwise unions: if $A,B\in\mathcal{F}$ then $A\cup B\in\mathcal{F}$.
	\eit
%Let $\{A_1,A_2,\ldots\}$ be a countable collection of sets over $\Omega$.
%\ben
%\it The \emph{union} of $A_1,A_2,\ldots$ is the set
%$\bigcup_{i=1}^\infty A_i = \{\omega:\omega\in A_i \text{ for some }A_i\}$.
%\it The \emph{intersection} of $A_1,A_2,\ldots$  is the set
%$\bigcap_{i=1}^\infty A_i = \{\omega:\omega\in A_i \text{ for all } A_i\}$.
%\een
\it A collection of sets $\mathcal{F}$ is called a \emph{$\sigma$-field} over $\Omega$ if 
\bit
\it $\Omega\in\mathcal{F}$,
\it $\mathcal{F}$ is closed under complementation: if $A\in\mathcal{F}$ then $A^c\in\mathcal{F}$, and
\it $\mathcal{F}$ is closed under countable unions: if $A_1,A_2,\ldots\in\mathcal{F}$ then $\bigcup_{i=1}^{\infty}A_i \in\mathcal{F}$.
\eit
\eit
%----------------------------------------------------------------------
\newpage
\section{Probability Spaces}
%----------------------------------------------------------------------
Let $\Omega$ be any set. Let $\mathcal{F}$ be a $\sigma$-field over $\Omega$. 
\bit
\it A \emph{probability measure} on $(\Omega,\mathcal{F})$ is a function $\prob:\mathcal{F}\to[0,1]$ such that
	\bit
	\it $\prob(\Omega) = 1$, and
	\it for any countable collection of pairwise disjoint events $\{A_1,A_2,\ldots\}$,
	\[
	\prob\left(\bigcup_{i=1}^\infty A_i\right) = \sum_{i=1}^{\infty} \prob(A_i).
	\text{\qquad (\emph{countable additivity})}
	\]
	\eit
\it The triple $(\Omega,\mathcal{F},\prob)$ is called a \emph{probability space} over $\Omega$.
%\it If $\prob(A)=0$, we say that $A$ is a \emph{null event}.
%\it If $\prob(A)=1$, we say that $A$ occurs \emph{almost surely} or \emph{with probability one}.
\eit

\vspace*{1ex}
\textbf{Properties of probability measures}
\bit
\it $\prob(\emptyset) = 0$.
\it Complementarity: $\prob(A^c) = 1 - \prob(A)$.
\it Monotonicity: if $A\subseteq B$ then $\prob(A)\leq \prob(B)$.
\it Addition rule: $\prob(A\cup B) = \prob(A) + \prob(B) - \prob(A\cap B)$.
\eit
%----------------------------------------------------------------------
\newpage
\section{Continuous Distributions}
%----------------------------------------------------------------------
A random variable $X$ is called \emph{continuous} if its CDF can be written as 
\[
F(x) = \int_{-\infty}^x f(t)\,dt\qquad (x\in\R)
\]
\bit
\it $f(x) = F'(x)$ is called the \emph{probability density function} (PDF) of $X$.
\it $\prob(X=x)=0$ for all $x\in\R$.
%\it $\displaystyle\prob(a\leq X\leq b) = \int_a^b f(x)\,dx = F(b) - F(a)$.
\it $\displaystyle\int_{-\infty}^{\infty} f(x)\,dx = 1$.
\eit

\vspace*{1ex}
\textbf{Expectation}:
\bit
\it $\expe(X) = \displaystyle\int_{-\infty}^{\infty} x f(x)\,dx$.
\eit

\bit
\it Variance, covariance and the correlation coefficient are defined as before.
\it The properties of expectation (linearity etc.) also carry over.
\eit

%----------------------------------------------------------------------
\newpage
\section{Uniform, Exponential and Normal}
%----------------------------------------------------------------------
\textbf{Uniform distribution}. Two parameters: $a,b\in\R$.
\bit
\it Range: $[a,b]$.
\it $f(x) = 1/(b-a)$.
\it $\expe(X) = (a+b)/2$ and $\var(X) = (b-a)^2/12$.
\eit
\vspace*{1ex}
\textbf{Exponential distribution}. One parameter: $\lambda\in\R$ with $\lambda>0$.
\bit
\it Range: $(0,\infty)$
\it $f(x) = \lambda e^{-\lambda x}$.
\it $\expe(X) = 1/\lambda$ and $\var(X) = 1/\lambda^2$.
\eit
\vspace*{1ex}
\textbf{Normal distribution}. Two parameters: $\mu,\sigma^2\in\R$ with $\sigma^2 > 0$.
\bit
\it Range: $\R$
\it $f(x) = \displaystyle\frac{1}{\sigma\sqrt{2\pi}}\exp\left[-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2\right]$.
\it $\expe(X) = \mu$ and $\var(X) = \sigma^2$.
\eit
%----------------------------------------------------------------------
\newpage
\section{Quantiles}
%----------------------------------------------------------------------
Let $X$ be a continuous random variable, and let $F$ denote its CDF.
\bit
\it A \emph{median} of $F$ is a number $m\in\R$ such that $F(m)=\prob(X\leq m)=0.5$%\frac{1}{2}$.
\it
A \emph{lower quartile} of $F$ is a number $x_L\in\R$ such that $F(x_L) = \prob(X\leq x_L) = 0.25$
\it
An \emph{upper quartile} of $F$ is a number $x_U\in\R$ such that $F(x_U) = \prob(X\leq x_U) = 0.75$.
\it 
An  \emph{inter-quartile range} of $F$ is the difference $x_U-x_L$.
\eit

\vspace*{1ex}
Suppose that $F$ is \emph{strictly} increasing.
\bit
\it For $q\in\N$, the \emph{$q$-quantiles} of $F$ are real values $x_1 < x_2 < \ldots < x_{q-1}$ such that 
\[
F(x_k) = \frac{k}{q} \qquad\text{for}\quad k=1,2,\ldots,q-1
\]
\it $q=4$ yields the \emph{quartiles} of $F$.
\it $q=10$ yields the \emph{deciles} of $F$.
\it $q=100$ yields the \emph{percentiles} of $F$.
\eit


%----------------------------------------------------------------------
\newpage
\section{Normal Approximation}
%----------------------------------------------------------------------

\textbf{Approximation of discrete distributions by continuous distributions}.
\bit
\it Let $X$ be a discrete random variable, taking values in the set $\{0,\pm 1,\pm 2,\ldots\}$.
\it Let $Y$ be a continuous random variable used as an approximation of $X$.
\it Then
\[
\prob(X=k) = \prob\left(k - \frac{1}{2} < Y < k + \frac{1}{2}\right).
\] 
\eit

\vspace*{1ex}
\textbf{Normal approximation of the binomial distribution}.
\bit
\it Let $X\sim\text{Binomial}(n,p)$.
\it If $n$ is sufficiently large, then $X\sim\text{N}(np,npq)$ approx. (where $q=1-p$).
\eit

\vspace*{1ex}
\textbf{Normal approximation of the Poisson distribution}.
\bit
\it Let $X\sim\text{Poisson}(\lambda)$.
\it If $\lambda$ is sufficiently large, then $X\sim\text{N}(\lambda,\lambda)$ approx.
\eit

%======================================================================
\end{document}
%======================================================================
