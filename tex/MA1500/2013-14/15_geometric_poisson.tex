\documentclass[lecture]{csm}

% set meta information
\modulecode{MA1500}
\moduletitle{Introduction to Probability Theory}
\academicyear{2013/14}
\doctype{Lecture}
\doctitle{The Geometric and Poisson Distributions}
\docnumber{15}

% local
\newcommand{\prob}{\mathbb{P}}
\newcommand{\expe}{\mathbb{E}}
\newcommand{\var}{\text{Var}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\def\it{\item}
\def\bit{\begin{itemize}}
\def\eit{\end{itemize}} 
\def\ben{\begin{enumerate}}
\def\een{\end{enumerate}}

%======================================================================
\begin{document}
\maketitle
\tableofcontents
%======================================================================

%Recall the following..
%\bit
%\it a random variable $X:\Omega\to\R$ is characterised by its \emph{distribution function},
%\begin{align*}
%F(x) 
%	& = \prob(X\leq x) \\
%	& = \prob\big(\{\omega: X(\omega)\leq x\}\big),
%\end{align*}	
%\bit
%\it a random variable is \emph{discrete} if it only takes countably many distinct values;
%\it a discrete variable is characterised by its \emph{probability mass function} (PMF),
%\begin{align*}
%f(x) 
%	& = \prob(X=x) \\
%	& = \prob\big(\{\omega: X(\omega)= x\}\big),
%\begin{align*} t
%\eit
   
%----------------------------------------------------------------------
\newpage
\section{Geometric distribution}
%----------------------------------------------------------------------
The geometric distribution is the distribution of the number of trials up to and including the first success in a sequence of independent Bernoulli trials. 

\begin{center}
\begin{tabular}{ll}\hline
Notation			& $X\sim\text{Geometric}(p)$ \\
Parameter(s)		& $p \in[0,1]$ \quad (probability of success) \\
Range			& $\{1,2,\ldots\}$ \\
PMF				& $\prob(X=k) = (1-p)^{k-1} p$\\ 
CDF				& $\prob(X\leq k) = 1-(1-p)^k$\\ \hline
\end{tabular}
\end{center}

% mean and variance
\subsubsection*{Mean and variance}

To compute the mean and variance of the geometric distribution, we make use of the geometric series
\[
\sum_{k=0}^\infty r^k = 1 + r + r^2 + \ldots = \frac{1}{1-r} \qquad\text{for }|r|<1
\]

\newpage

\begin{hidebox}
The PDF of $X\sim\text{Geometric}(p)$ can be written as $p(k)=q^{k-1}p$ where $q=1-p$. 

%First,
\begin{align*}
\expe(X) = \sum_{k=1}^{\infty} k \prob(X=k)
	& = \sum_{k=1}^\infty k q^{k-1} p \\
	& = \sum_{j=0}^\infty (j+1)q^j p 		\qquad \qquad\text{(where we have set $j=k-1$)} \\
	& = q\sum_{j=0}^\infty j q^{j-1} p + p\sum_{j=0}^\infty q^j \\
	& = q\sum_{j=1}^\infty j\,\prob(X=j) + \frac{p}{1-q} \\
	& = q\expe(X) + 1.
\end{align*}
Solving this equation, we get $\displaystyle \expe(X) = \frac{1}{p}$.

\newpage
\begin{align*}
\expe(X^2) = \sum_{k=1}^\infty k^2 q^{k-1}p 
	& = \sum_{j=0}^\infty (j+1)^2 q^j p \qquad \qquad\text{(where we have set $j=k-1$)} \\
	& = \sum_{j=0}^\infty (j^2+2j+1) q^j p \\
%	& = \sum_{j=0}^\infty j^2\,q^j p + 2\sum_{j=0}^\infty j\,q^j p + \sum_{j=0}^\infty q^j p \\
	& = q\sum_{j=1}^\infty j^2\,q^{j-1} p \ +\  2q\sum_{j=1}^\infty j\,q^{j-1} p \ +\  p\sum_{j=0}^\infty q^j \\
	& = q\sum_{j=1}^\infty j^2\,\prob(X=j) \ +\  2q\sum_{j=1}^\infty j\,\prob(X=j) \ +\  \frac{p}{1-q} \\
	& = q\expe(X^2) + 2q\expe(X) + 1.
\end{align*}
%from which we obtain
%\[
%\expe(X^2)	= \frac{2q\expe(X)+1}{1-q} = \frac{2(1-p)/p+1}{p} = \frac{(2-p)}{p^2}
%\]
%Solving this equation, we get $\displaystyle\expe(X^2) = \frac{2q\expe(X)+1}{1-q} = \frac{(2-p)}{p^2}$.
%\]
Substituting for $\expe(X)$, we obtain
\begin{align*}
\expe(X^2)	& = \frac{2q\expe(X)+1}{1-q} = \frac{(2-p)}{p^2}, \text{ and hence}\\
\var(X) 		& = \expe(X^2)-\expe(X)^2 = \frac{2-p}{p^2} \ -\  \frac{1}{p^2} = \frac{1-p}{p^2}.
\end{align*}
\vspace*{-2ex}
\end{hidebox}

% example
\begin{example}
Let $p=0.001$ be the probability that a certain type of lightbulb fails on any given day. 
\ben
\it What is the expected lifetime of this type of lightbulb?
\it What is the probability that a lightbulb lasts for more than 30 days?
\een
\end{example}

\begin{solution}
Let $X$ denote the lifetime of a bulb. Then $X\sim\text{Geometric}(p)$ where $p=0.001$. 
\ben
\it $\expe(X) = \displaystyle\frac{1}{p} = 1000\text{ days}$.
\it The distribution function of $X$ is 
\[
\prob(X\leq k) = 1-(1-p)^k,
\]
so $\prob(X> k) = (1-p)^k$ and hence
\[
\prob(X > 30) = (1 - 0.001)^{30} = 0.999^{30} = 0.9704.
\]
\een
\end{solution}


%----------------------------------------------------------------------
\newpage
\section{Negative binomial distribution}
%----------------------------------------------------------------------
The negative binomial distribution is the distribution of the number of trials required to get a fixed number of successes in a sequence of independent Bernoulli trials. 

\begin{center}
\begin{tabular}{ll}\hline
Notation			& $X\sim\text{NegBin}(r,p)$ \\
Parameter(s)		& $p \in[0,1]$ \quad (probability of success) \\
				& $r \in\N$ \qquad (stopping parameter) \\
Range			& $\{r,r+1,r+2\ldots\}$ \\[1ex]
PMF				& $\prob(X=k) = \displaystyle \binom{k-1}{r-1}(1-p)^{k-r}p^{r}$\\[2ex] \hline
%CDF				& $F(k) = \displaystyle \sum_{j=r}^k \binom{j-1}{r-1}(1-p)^{j-r}p^{r}$\\[2ex] \hline
\end{tabular}
\end{center}

If $X\sim\text{NegBin}(r,p)$, then $X$ can be written as the sum of $r$ independent geometric variables,
\[
X = \sum_{i=1}^r Y_i \quad\text{where}\quad Y_i\sim\text{Geometric}(p)\quad\text{for each}\quad i=1,2,\ldots,r.
\]

\begin{remark}[Waiting times]
As with the geometric and Poisson distributions, the negative binomial can be used to model situations in which we are waiting for the occurence of an event (namely, the event that a specified number of successes occur).
\end{remark}

% mean and variance
\subsubsection*{Mean and variance}

\begin{hidebox}
Let $Y_1,\ldots,Y_r$ be independent with $Y_i\sim\text{Geometric}(p)$, and let $X = \sum_{i=1}^r Y_i$.
\par
Then $X\sim\text{NegBin}(r,p)$, and by the linearity of expectation,
\[
\expe(X) = \expe(Y_1)+\expe(Y_2)+\ldots+\expe(Y_r) = \frac{r}{p}
\]
and because the trials are independent,
\[
\var(X) = \var(Y_1)+\var(Y_2)+\ldots+\var(Y_r) = \frac{r(1-p)}{p^2}
\]
\vspace*{-4ex}
\end{hidebox}

\newpage

% example
\begin{example}
Biological populations are often sampled using a technique called \emph{inverse binomial sampling}. Let $p$ be the proportion of individuals having a particular characteristic. We sample from the population until we obtain $r$ such individuals. The total number of individuals selected has negative binomial distribution with parameters $r$ and $p$. 

Suppose that a biologist wishes to obtain a sample of $100$ fruit flies having a certain genetic trait that occurs at a rate of one in every twenty fruit files in the population. What is the probability that the biologist has to examine at least $k$ flies?
\end{example}

\begin{solution}
Let $X\sim\text{NegBin}(r,p)$ with $r=100$ and $p=0.05$. Then
\[
\prob(X\geq k) = 1 - \prob(X < k) = 1 - \sum_{j=100}^{k-1} \binom{j-1}{99} (0.95)^{j-100}(0.05)^{100} 
\]
\end{solution}

% remark: statistical tables
\begin{remark}[Statistical tables]
If $r$ is large and/or $p$ is small, it is not easy to compute these probabilities. Instead, we can use \emph{statistical tables} find (approximate) values of $\prob(X\geq k)$ for various values of $k$, and various values of the distribution parameters $r$ and $p$. Statistical tables are available for many useful probability distributions. 
\end{remark}


%----------------------------------------------------------------------
\newpage
\section{Poisson distribution}
%----------------------------------------------------------------------
The Poisson distribution models the number of events (sometimes called \emph{arrivals}) occurring in a unit time interval, where the parameter $\lambda$ is the average number of events per unit time (rate), and the time intervals between successive events are assumed to be independent of each other. 

\begin{center}
\begin{tabular}{ll}\hline
Notation			& $X\sim\text{Poisson}(p)$ \\
Parameter(s)		& $\lambda > 0$\quad (rate parameter) \\
Range				& $\{0,1,2,\ldots\ldots\}$ \\
PMF				& $\prob(X=k) = \displaystyle\frac{\lambda^k}{k!}e^{-\lambda}$ \\[2ex] \hline
\end{tabular}
\end{center}

% mean and variance
\subsubsection*{Mean and variance}

%\begin{hidebox}
Let $X\sim\text{Poisson}(\lambda)$.
\begin{align*}
\expe(X) = \sum_{k=0}^\infty k \prob(X=k)
	& = e^{-\lambda}\sum_{k=0}^\infty \frac{k\lambda^k}{k!} \\
	& = e^{-\lambda}\sum_{k=1}^\infty \frac{k\lambda^k}{k!} \qquad\qquad\text{because the term for $k=0$ is zero,}\\
	& = \lambda e^{-\lambda}\sum_{k=1}^\infty \frac{\lambda^{k-1}}{(k-1)!} \qquad\text{because $k!=k(k-1)!$,}\\
	& = \lambda e^{-\lambda}\sum_{j=0}^\infty \frac{\lambda^j}{j!} \qquad\qquad\text{where we have set $j=k-1$,}\\
	& = \lambda \quad\qquad\qquad\qquad\qquad\text{because}\quad \sum_{j=0}^\infty \frac{\lambda^j}{j!}= e^{\lambda}.
\end{align*}
%\end{hidebox}

\newpage
\vspace*{-5ex}
%\begin{hidebox}
%Similarly,
\begin{align*}
\expe(X^2) = \sum_{k=0}^\infty k^2 \prob(X=k)
	& = e^{-\lambda}\sum_{k=0}^\infty \frac{k^2\lambda^k}{k!} \\
	& = e^{-\lambda}\sum_{k=1}^\infty \frac{k^2\lambda^k}{k!} \quad\qquad\qquad\qquad\text{because the term for $k=0$ is zero,}\\
	& = \lambda e^{-\lambda}\sum_{k=1}^\infty \frac{k\lambda^{k-1}}{(k-1)!} \quad\qquad\qquad\text{because $k!=k(k-1)!$,}\\
	& = \lambda e^{-\lambda}\sum_{j=0}^\infty \frac{(j+1)\lambda^j}{j!} \qquad\qquad\text{where we have set $j=k-1$,}\\
	& = \lambda e^{-\lambda}\left[\sum_{j=0}^\infty \frac{j\lambda^j}{j!} + \sum_{j=0}^\infty \frac{\lambda^j}{j!}\right]\\
	& = \lambda e^{-\lambda}\big[\lambda e^{\lambda} + e^{\lambda}\big] \quad\qquad\qquad\text{because $e^{-\lambda}\sum_{j=0}^\infty \frac{j\lambda^j}{j!} = \expe(X) = \lambda$,}\\
	& = \lambda(\lambda+1).
\end{align*}
Thus
\[
\var(X) = \expe(X^2)-\expe(X)^2 = \lambda(\lambda+1)-\lambda^2 = \lambda.
\]
%\end{hidebox}

% example
\begin{example}
A call centre receives an average of five calls every three minutes. What is the probability that there will be
\ben
\it no calls in the next minute, and
\it at least two calls in the next minute?
\een
\end{example}
\begin{solution}
Let $X$ be the number of calls received in any given minute. The average number of calls per minute is $\lambda=\frac{5}{3}$. If we model $X$ by a Poisson distribution rate parameter $\lambda$, then the probability that $k$ calls are received in any given minute is
\[
\prob(X=k) = \frac{1}{k!}\left(\frac{5}{3}\right)^ke^{-5/3}
\]
\ben
\it $\prob(X=0) = e^{-5/3} = 0.189$.
\it $\prob(X\geq 2) = 1 - \prob(X=0) - \prob(X=1) = 1 - e^{-5/3} - \displaystyle\frac{5}{3}e^{-5/3} = 0.496$.
\een
\end{solution}

%----------------------------------------------------------------------
\newpage
\section{The law of rare events}
%----------------------------------------------------------------------
The \emph{law of rare events}, also known as the \emph{Poisson limit theorem}, shows that the binomial distribution can be approximated by a Poisson distribution when the number of trials $n$ is large, and the probability of success $p$ is small.

% theorem
\begin{theorem}[The law of rare events]
Suppose that $X\sim\text{Binomial}(n,p)$, and suppose that $p\to 0$ as $n\to\infty$ in such a way that the product $\lambda=np$ remains constant. Then 
\[
\prob(X=k)\to \frac{\lambda^k}{k!}e^{-\lambda} \quad\text{as}\quad n\to\infty.
\]
\end{theorem}

% proof
\begin{proof}
To prove the theorem, we need the fact that for any $c\in\R$,
\[
\left(1-\frac{c}{n}\right)^n \to e^{-c}\quad\text{as}\quad n\to\infty
\]
Let $\lambda=np$.
\begin{align*}
\prob(X=k) = \binom{n}{k}p^k(1-p)^k 
	& = \frac{n!}{(n-k)!k!} p^k (1-p)^{n-k} \\
	& = \frac{n!}{(n-k)!k!} \left(\frac{\lambda}{n}\right)^k \left(1-\frac{\lambda}{n}\right)^{n-k} \\
	& =  \frac{n!}{(n-k)!n^k}\left(\frac{\lambda^k}{k!}\right) \left(1-\frac{\lambda}{n}\right)^n \left(1-\frac{\lambda}{n}\right)^{-k}
\end{align*}
Now,
\begin{align*}
\frac{n!}{(n-k)!n^k} 
	& = \frac{n(n-1)(n-2)\ldots(n-k+1)}{n^k} \\
	& = 1\cdot\left(1-\frac{1}{n}\right)\cdot\left(1-\frac{2}{n}\right)\cdots\left(1-\frac{k-1}{n}\right) \\
	& \to 1 \quad\text{as}\quad n\to\infty 
\end{align*}
%\[
%\frac{n!}{(n-k)!n^k} 
%	= \frac{n(n-1)(n-2)\ldots(n-k+1)}{n^k} 
%	= 1\cdot\left(1-\frac{1}{n}\right)\cdot\left(1-\frac{2}{n}\right)\cdots\left(1-\frac{k-1}{n}\right) 
%\]
%\begin{align*}
%\frac{n!}{(n-k)!n^k} 
%	& = \frac{n(n-1)(n-2)\ldots(n-k+1)}{n^k} \\
%	& = 1\cdot\left(1-\frac{1}{n}\right)\cdot\left(1-\frac{2}{n}\right)\cdots\left(1-\frac{k-1}{n}\right) \\
%	& \to 1 \quad\text{as}\quad n\to\infty \\
%\left(1-\frac{\lambda}{n}\right)^n 
%	& \to e^{-\lambda} \quad\text{as}\quad n\to\infty, \\
%\left(1-\frac{\lambda}{n}\right)^{-k}
%	& \to 1 \quad\text{as}\quad n\to\infty
%\end{align*}
\newpage


Furthermore,
\[
\left(1-\frac{\lambda}{n}\right)^n \to e^{-\lambda} \quad\text{and}\quad \left(1-\frac{\lambda}{n}\right)^{-k} \to 1 \quad\text{as}\quad n\to\infty.
\]

%\[\begin{array}{lll}
%\displaystyle\frac{n!}{(n-k)!n^k}
%	& \longrightarrow 1 & \quad\text{as}\quad n\to\infty, \text{and also} \\
%\displaystyle\left(1-\frac{\lambda}{n}\right)^n 
%	& \longrightarrow e^{-\lambda} & \quad\text{as}\quad n\to\infty \\
%\displaystyle\left(1-\frac{\lambda}{n}\right)^{-k}
%	& \longrightarrow 1 & \quad\text{as}\quad n\to\infty
%\end{array}\]
Thus it follows that 
\[
\prob(X=k) = \binom{n}{k}p^k(1-p)^k \to \frac{\lambda^k}{k!} e^{-\lambda}\quad\text{as}\quad n\to\infty.
\]
which is the PDF of a $\text{Poisson}(\lambda)$ random variable.
\end{proof}

\newpage

% example: typist
\begin{example}
On average, a typist makes one error in every 500 words. A typical page contains 300 words. What is the probability that there will be no more than two errors in five pages?
\end{example}

\begin{solution}
Assume that typing a single word incorrectly is a Bernoulli trial with probability of `success' equal to $\frac{1}{500}$, and that whether any given word is typed incorrectly is independent of any other word being typed incorrectly.

\vspace{2ex}
Let $X$ be the number of errors in five pages, or 1500 words. 

\vspace{2ex}
Then $X\sim\text{Binomial}\left(1500,\frac{1}{500}\right)$, so
\[
P(X\leq 2)
	= \sum_{k=0}^2\binom{1500}{k}\left(\frac{1}{500}\right)^k\left(\frac{499}{500}\right)^{1500-k} = 0.4230.
\]
Using the Poisson approximation with $\lambda=1500\times \frac{1}{500} = 3$,
\[
\prob(X\leq 2) \approx e^{-3}\left(1 + 3 + \frac{3^2}{2}\right) = 0.4232.
\]
\end{solution}


%======================================================================
\end{document}
%======================================================================
