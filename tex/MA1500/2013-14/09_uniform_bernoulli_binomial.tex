\documentclass[lecture]{csm}
%\documentclass[blanks,lecture]{csm}

% set meta information
\modulecode{MA1500}
\moduletitle{Introduction to Probability Theory}
\academicyear{2013/14}
\doctype{Lecture}
\doctitle{The Uniform, Bernoulli and Binomial Distributions}
\docnumber{9}

% local
\newcommand{\R}{\mathbb{R}}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\expe}{\mathbb{E}}
\newcommand{\var}{\text{Var}}
\def\it{\item}
\def\bit{\begin{itemize}}
\def\eit{\end{itemize}} 
\def\ben{\begin{enumerate}}
\def\een{\end{enumerate}}

%======================================================================
\begin{document}
\maketitle
\tableofcontents
%======================================================================

%----------------------------------------------------------------------
\section{The uniform distribution}
%----------------------------------------------------------------------
The uniform distribution on $\{1,2,\ldots,n\}$ assigns an equal probability to each value.
\begin{center}
\begin{tabular}{ll}\hline
Notation			& $X\sim\text{Uniform}\{1,2,\ldots,n\}$ \\
Parameter(s)		& $n\in\mathbb{N}$ \quad (number of outcomes) \\
Range			& $\{1,2,\ldots,n\}$ \\
PMF				& $p_k = 1/n$ for all $k=1,2,\ldots,n$ \\ \hline
\end{tabular}
\end{center}

% mean and variance
\subsubsection*{Mean and variance}
\begin{align*}
\expe(X) 
	& = \sum_{k=1}^n k\,p_k
	= \frac{1}{n}\sum_{k=1}^n k 
	= \frac{1}{n}\left(\frac{n(n+1)}{2}\right)
	= \frac{n+1}{2}. \\
\expe(X^2) 
	& = \sum_{k=1}^n k^2\,p_k
	= \frac{1}{n}\sum_{k=1}^n k^2 
	= \frac{1}{n}\left(\frac{n(n+1)(2n+1)}{6}\right)
	= \frac{(n+1)(2n+1)}{6}. \\
\var(X)
	& = \expe(X^2)-\expe(X)^2  
	= \frac{(n+1)(2n+1)}{6} - \frac{(n+1)^2}{4} 
	  = \frac{(n-1)(n+1)}{12}.
\end{align*}

\break % <<

%---------------------------------
%\subsection*{Entropy}

\begin{example}[Entropy]
Entropy is a way of quantifying the \emph{uncertainty} associated with a random variable.
Let $X$ be a random variable taking values in $\{1,2,\ldots,n\}$, and let $p_i=\prob(X=i)$ be its probability mass function. The \emph{entropy} of $X$ is defined by
\[
H(X) = \sum_{i=1}^n p_i\log p_i
\]
\bit
\it If $X$ is a non-random, say $p_i=1$ and $p_j=0$ for all $j\neq i$, then
\[
H(X) = - p_i\log p_i - \sum_{j\neq i} p_j\log p_j = 0.
\]
\it If $X$ is uniformly distributed i.e.\ $p_i=1/n$ for all $i\in\{1,2,\ldots,n\}$, 
\[
H(X) = -\sum_{i=1}^n \frac{1}{n}\log\left(\frac{1}{n}\right) = \log n.
\]
\it
For any random variable on $\{1,2,\ldots,n\}$, it can be shown that $0\leq H(X)\leq \log n$.
\it 
The uniform distribution has \emph{maximum entropy} (maximum uncertainty).
%
%Note that $H(X)$ increases as the number of possible outcomes increases.
\eit
%If $X$ is uniformly distributed on $\{1,2,\ldots,n\}$, we see that 
%\bit
%\it $H(X)$ achieves its maximum value among all possible distributions on $\{1,2,\ldots,n\}$.
%\eit
\end{example}

%\break % << 
%If $X$ is \emph{any} random varaiable taking values in $\{1,2,\ldots,n\}$, it can be shown that
%\[
%0\leq H(X)\leq \log n.
%\]
%
%If $X$ is uniformly distributed on $\{1,2,\ldots,n\}$, its entropy $H(X)=\log n$,
%\bit
%\it reaches its maximum value among all possible distributions on $n$ symbols, and 
%\it increases as the number of possible outcomes increases.
%\eit
%\end{example}

%\begin{theorem}[Properties of entropy]
%\end{theorem}

%----------------------------------------------------------------------
\section{The Bernoulli distribution}
%----------------------------------------------------------------------
The Bernoulli distribution has a single parameter, the \emph{probability of success}, $p$, and is the distribution of the indicator variable $I_A$ of an event $A$ for which $\prob(A)=p$.

\begin{center}
\begin{tabular}{ll}\hline
Notation			& $X\sim\text{Bernoulli}(p)$ \\
Parameter(s)		& $p \in [0,1]$ \quad (probability of success) \\
Range			& $\{0,1\}$ \\
PMF				& $p_0 = 1-p$ and $p_1 = p$ \\ \hline
\end{tabular}
\end{center}

% mean and variance
\subsection*{Mean and variance}
\begin{align*}
\expe(X) 
	& 	= \sum_{k=0}^1 k\,p_k
	 	= \big[0\times(1-p)\big] + \big[1\times p\big]
		= p. \\
\expe(X^2) 
	& 	= \sum_{k=0}^1 k^2\,p_k
		= \big[0^2\times(1-p)\big] + \big[1^2\times p\big]
		= p. \\
\var(X)	
	& 	= \expe(X^2)-\expe(X)^2
		= p - p^2
		= p(1-p).
\end{align*}

%The entropy of the Bernoulli distribution is
%\[
%H(X) = -\sum_{i=1}^n p_i\log p_i =  - (1-p)\log (1-p) - p\log p.
%\]

%----------------------------------------------------------------------
\section{The binomial distribution}
%----------------------------------------------------------------------

The binomial distribution has two parameters, 
\bit
\it the \emph{number of trials} $n$, and 
\it the \emph{probability of success} $p$. 
\eit
It is the distribution of the number of successes in a sequence of $n$ independent Bernoulli trials, where $p$ the probability of success in each trial.

\begin{center}
\begin{tabular}{ll}\hline
Notation			& $X\sim\text{Binomial}(n,p)$ \\
Parameter(s)		& $n \in\mathbb{N}$ \qquad (number of trials) \\
				& $p \in[0,1]$ \quad (probability of success) \\
Range			& $\{0,1,2,\ldots,n\}$ \\
PMF				& $p_k = \displaystyle\binom{n}{k}p^k(1-p)^{n-k}$\\[2ex] \hline
\end{tabular}
\end{center}

% mean and variance
\newpage
\subsubsection*{Mean and variance}
%Let $I_j$ be the indicator variable of success on the $j$th trial, and let $X$ be the total number of successes in $n$ trials. Then 
%\[
%X = \sum_{i=1}^n I_j \qquad\text{where}\quad I_j =
%  \begin{cases}
%   1 & \text{if success on the $j$th trial,} \\
%   0 & \text{otherwise.}
%  \end{cases}
%\]
%The indicator variable $I_j$ has Bernoulli distribution with parameter $p$, with 
%\[
%\expe(I_j)=p \text{\quad and\quad} \var(I_j)=p(1-p).
%\]
%By the linearity of expectation,
%\begin{align*}
%\expe(X) & = \expe(I_1)+\expe(I_2)+\ldots+\expe(I_n) = np, \\
%\intertext{and because the trials are independent,}
%\var(X) & = \var(I_1)+\var(I_2)+\ldots+\var(I_n) = np(1-p).
%\end{align*}
We show that if $X\sim\text{Binomial}(n,p)$, then $\expe(X)=np$ and $\var(X)=np(1-p)$.

\vspace*{2ex}
Let $q=1-p$. Then the PMF of $X$ can be written as 
\[
p_k = \binom{n}{k} p^k q^k,\qquad k=0,1,\ldots,n.
\]
The expected value is
\begin{align*}
\expe(X)
	= \sum_{k=0}^{n} kp_k
	& = \sum_{k=0}^{n}\binom{n}{k} k p^{k}q^{n-k} \\
	& = \sum_{k=1}^{n}\frac{n!}{(k-1)!(n-k)!} p^{k} q^{n-k} \\
	& = np\sum_{k=1}^{n}\frac{(n-1)!}{(k-1)!(n-k)!} p^{k-1} q^{n-k}.
\end{align*}

\textbf{Claim}: $\displaystyle\sum_{k=1}^{n}\frac{(n-1)!}{(k-1)!(n-k)!} p^{k-1} q^{n-k} = 1$.

Let $j=k-1$. Then
\begin{align*}
\sum_{k=1}^{n}\frac{(n-1)!}{(k-1)!(n-k)!} p^{k-1} q^{n-k}
	& = \sum_{j=0}^{n-1}\frac{(n-1)!}{j!(n-j-1)!} p^{j} q^{n-j-1} \\
	& = \sum_{j=0}^{n-1}\binom{n-1}{j} p^{j} q^{n-j-1} \\
	& = (p+q)^{n-1} = 1 \qquad\text{(by the binomial theorem).}
%	& = 1.%\qquad\text{(since $p+q=1$).}
\end{align*}
Hence $\expe(X)=np$.
To compute the variance, we use the identity 
\[
\expe(X^2) = \expe\big[X(X-1]\big]+\expe(X).
\]
%First,
\begin{align*}
\expe[X(X - 1)]
	= \sum_{k=0}^{n} k(k-1)p_k 
	& = \sum_{k=0}^{n} \binom{n}{k} k(k-1)p^{k} q^{n-k} \\
	& = \sum_{k=2}^{n}\frac{n!}{(k-2)!(n-k)!} p^{k} q^{n-k}  \\
	& = n(n-1)p^{2} \sum_{k=2}^{n}\frac{(n-2)!}{(k-2)!(n-k)!} p^{k-2} q^{n-k}.
\end{align*}

\textbf{Claim}: $\displaystyle\sum_{k=2}^{n}\frac{(n-2)!}{(k-2)!(n-k)!} p^{k-2} q^{n-k} = 1$.

\vspace*{2ex}
Let $j=k-2$. Then
\begin{align*}
\sum_{k=2}^{n}\frac{(n-2)!}{(k-2)!(n-k)!} p^{k-2} q^{n-k} 
	& = \sum_{j=0}^{n-2}\frac{(n-2)!}{j!(n-2-j)!} p^{j} q^{n-2-j} \\
	& = \sum_{j=0}^{n-2}\binom{n-2}{j} p^{j} q^{n-2-j} \\
	& = (p+q)^{n-2} = 1\qquad\text{(by the binomial theorem).}
%	& = n(n-1)p^2 \qquad\text{(since $p+q=1$).}
\end{align*}

%\begin{align*}
%\expe(X(X - 1))
%	& = n(n-1)p^{2} \sum_{j=0}^{n-2}\frac{(n-2)!}{j!(n-2-j)!} p^{j} q^{n-2-j} \qquad\text{where $j=k-2$,} \\
%	& = n(n-1)p^{2} \sum_{j=0}^{n-2}\binom{n-2}{j} p^{j} q^{n-2-j} \\
%	& = n(n-1)p^{2}(p+q)^{n-2}\qquad\text{(by the binomial theorem)} \\
%	& = n(n-1)p^2 \qquad\text{(since $p+q=1$).}
%\end{align*}
Thus $\expe[X(X - 1)]=n(n-1)p^2$, so
\[
\expe(X^2) 
	= \expe[X(X-1)] + \expe(X)
	= n(n-1)p^2 + np,
\]
and hence
\begin{align*}
\var(X)	
	& = \expe(X^2)-\expe(X)^2 \\
	& = n(n-1)p^2 + np - n^2p^2 \\
	& = np(1-p).
\end{align*}

\break % <<

% example
\begin{example}
A multiple choice test consists of ten questions, each with a choice of three different answers. A student decides to choose the answers at random. Find the mean and variance of the number of correct answers.
\end{example}

\begin{solution}
Let $X$ be the number of correct answers.\par
Then $X\sim\text{Binomial}(n,p)$ where
\bit
\it $n=10$  (the total number of questions), and
\it $p=1/3$ (the probability of a correct answer).
\eit
Thus
\[
\expe(X)=np=\frac{10}{3}\quad\text{and}\quad \var(X)=np(1-p)=\frac{20}{9}.
\]
\end{solution}

% skewness and kurtosis
\subsubsection*{Skewness and kurtosis}
\ben
\it The skewness of $X\sim\text{Binomial}(n,p)$ is
\[
\gamma_1 = \frac{1-2p}{\sqrt{np(1-p)}} = \begin{cases}
	< 0 	& \text{if } p > \frac{1}{2} \\
	= 0 	& \text{if } p = \frac{1}{2} \\
	> 0 	& \text{if } p < \frac{1}{2} \\
\end{cases}	
\]
\it The kurtosis of $X\sim\text{Binomial}(n,p)$ is
\[
\gamma_2 = \frac{1-6p(1-p)}{np(1-p)} = \begin{cases}
	< 0 				& \text{if } \left|p -\frac{1}{2}\right| < \frac{1}{2\sqrt{3}} \\
	> 0 				& \text{if } \left|p -\frac{1}{2}\right| > \frac{1}{2\sqrt{3}} \\
\end{cases}
\]
\een

\bit
\it $X$ has \emph{negative kurtosis} (low and wide peak) when p is close to $1/2$.
\it $X$ has \emph{positive kurtosis} (tall and narrow peak) when $p$ is close to $0$ of $1$.
\eit

\break % << 

\begin{remark}
\bit
\it The kurtosis $\gamma_2$ reaches its minimum value when $p=\frac{1}{2}$. 
\it For any fixed $p\in(0,1)$, $\gamma_2\to 0$ as $n\to\infty$.
\it The kurtosis of the \emph{normal distribution} is $0$.
\eit
\end{remark}

The \emph{de\,Moivre\,--\,Laplace} theorem states that the binomial distribution converges to a normal distribution as the number of trials $n\to\infty$.
\bit
%\it The binomial distribution converges to a normal distribution as $n\to\infty$.
\it This is a special case of the \emph{central limit theorem} (CLT).
\eit


%======================================================================
\end{document}
%======================================================================
