% !TEX root = main.tex
%----------------------------------------------------------------------
\chapter{Probability Generating Functions}\label{chap:pgfs}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\section{Generating functions}
%----------------------------------------------------------------------
Generating functions, first introduced by de~Moivre in 1730, are power series used to represent sequences of real numbers. It is often easier to work with generating functions than with the original sequences.

% definition
\begin{definition}
Let $a = (a_0,a_1,a_2,\ldots)$ be a sequence of real numbers. The \emph{generating function} of the sequence is the function $G_a(t)$, defined for every $t\in\R$ for which the sum converges, by
\[
G_a(t) = \sum_{k=0}^{\infty} a_k t^k.
\]
\end{definition}

The sequence can be reconstructed from $G_a(t)$ by setting 
\[
\displaystyle a_n = \frac{1}{n!} G_a^{(n)}(0),
\]
where $G_a^{(n)}(t)$ is $n$th derivative of $G_a(t)$. In particular,
\[
G(0) = a_0,\quad G'(0) = a_1,\quad G''(0) = a_2,\quad \text{and so on.}
\]

% example: convolution
\begin{example}
The \emph{convolution} of two sequences $a=(a_0,a_1,a_2,\ldots)$ and $b=(b_0,b_1,b_2,\ldots)$ is another sequence $c=(c_0,c_1,c_2,\ldots)$, whose $k$th term is
\[
c_k = a_0 b_k + a_1 b_{k-1} + \ldots + a_k b_0 = \sum_{i=0}^k a_i b_{k-i}.
\]
Convolutions can be difficult to handle. However, the generating function of a convolution is just the product of the generating functions of the original sequences:
\begin{align*}
G_c(t) = \sum_{k=0}^\infty c_k t^k
	& = \sum_{k=0}^\infty\left[\sum_{i=0}^n a_i b_{k-i}\right] t^k \\
	& = \sum_{i=0}^\infty a_i t^i \sum_{k=i}^\infty b_{k-i} t^{k-i} 
	= \sum_{i=0}^\infty a_i t^i \sum_{j=1}^\infty b_j t^j 
	 = G_a(t)G_b(t).
\end{align*}
A convolution of sequences is replaced by a product of generating functions.
\end{example}

%----------------------------------------------------------------------
\subsection{Properties of generating functions*} 
%----------------------------------------------------------------------

A generating function $G_a(t)$ is a power series whose coefficients are the terms of the sequence $a$. All power series have the following properties:
\bit
\it
\textbf{Convergence}. There exists a \emph{radius of convergence} $R\geq 0$ such that $G_a(t)$ is absolutely convergent when $|t|<R$, and divergent when $|t|>R$.
\it
\textbf{Differentiation}. $G_a(t)$ may be differentiated or integrated any number of times whenever $|t|<R$.
\it
\textbf{Uniqueness}. If $G_a(t)=G_b(t)$ for all $|t|<R'$, where $0<R'\leq R$, then $a_n=b_n$ $\forall n$.
\it
\textbf{Abel's theorem}. If $a_k>0$ for all $k$, and $G_a(t)$ converges for all $|t|<1$, then 
\[
G_a(1) = \lim_{t\uparrow 1}G_a(t) = \lim_{t\uparrow 1}\sum_{k=0}^{\infty} a_k t_k = \sum_{k=0}^{\infty} a_k.
\]
%Abel's theorem allows us to compute $G_a(1)$ as the limit of $G_a(t)$ as $t$ converges to $1$ from below.
\eit

%% remark: exponential GF
%\begin{remark}
%There are several different types of generating function. For example, the \emph{exponential} generating function of the sequence $a = (a_0,a_1,a_2,\ldots)$ is 
%\[
%E_a(t) = \sum_{k=1}^{\infty} \frac{a_k t^k}{k!} \qquad\text{for every $t\in\R$ for which the sum converges.}
%\]
%\end{remark}




%----------------------------------------------------------------------
\section{Probability generating functions}
%----------------------------------------------------------------------

% definition
\begin{definition}
Let $X$ be a discrete random variable taking values in the range $\{0,1,2,\ldots\}$, and let $f$ denote its PMF. The \emph{probability generating function} (PGF) of $X$ is the generating function of its PMF:
\[
G(t) = \expe(t^X) = \sum_{k=0}^{\infty} f(k) t^k 
\]
\end{definition}

% remarks
\begin{remark}
\bit
\it $G(t)$ converges for all $|t|\leq 1$.
\it $G(0) = 0$.
\it $G(1)=\sum_{k=0}^{\infty} f(k) = 1$.
\eit
\end{remark}



% example (famous distributions)
\begin{example}
The PGFs of some notable discrete distributions on $\{0,1,2,\ldots\}$ are computed as follows:
%\ben
%% constant
%\it \textbf{Constant}: if $\prob(X=c)=1$,
%\[
%G(t) = \sum_{k=0}^\infty f(k) t^k = t^c.
%\]
%% bernoulli
%\it \textbf{Bernoulli}: if $X\sim\text{Bernoulli}(p)$, its PMF is %$f(k) = \begin{cases} 1-p & \text{ if }k=0, \\ p & \text{ if }k=1, \text{ and zero otherwise.} \end{cases}$
%\begin{align*}
%f(k) & = \begin{cases} 1-p & \text{ if }k=0, \\ p & \text{ if }k=1, \end{cases} \\
%\intertext{and zero otherwise, so its PGF is}
%G(t) & = \sum_{k=0}^\infty f(k) t^k = (1-p)t^0 + pt^1 = 1 - p + pt.  \\
%\end{align*}
%% poisson
%\it \textbf{Poisson}: if X$\sim\text{Poisson}(\lambda)$, its PMF is $f(k) = \displaystyle\frac{\lambda^k e^{-\lambda}}{k!}$ for $k=0,1,2,\ldots$ (and zero otherwise), so its PGF is
%\[
%G(t) = \sum_{k=0}^\infty f(k) t^k
%		= \sum_{k=0}^{\infty}\left(\frac{\lambda^k e^{-\lambda}}{k!}\right)t^k
%		= e^{-\lambda}\sum_{i=1}^\infty \frac{(\lambda t)^k}{k!}
%		= e^{-\lambda}e^{\lambda t}
%		= e^{\lambda(t-1)}
%\]
%% geometric
%\it \textbf{Geometric}: if $X\sim\text{Geometric}(p)$ and $q=1-p$, its PMF is $f(k)=(1-p)^k p$ for $k=0,1,2,\ldots$ (and zero otherwise), so its PGF is
%\[
%G(t) = \sum_{k=0}^\infty f(k) t^k
%		= \sum_{k=0}^\infty (1-p)^k p t^k 
%		= p\sum_{k=0}^\infty \big[(1-p)t\big]^k 
%		= \frac{p}{1-(1-p)t}.
%\]
%Here, we have used the fact that $\displaystyle\sum_{k=0}^{\infty} r^k = \frac{1}{1-r}$ for $|r|<1$.
%\een
\ben
% constant
\it \textbf{Constant}: if $\prob(X=c)=1$,
\[
G(t) = \sum_{k=0}^\infty f(k) t^k = t^c.
\]
% bernoulli
\it \textbf{Bernoulli}: if $X\sim\text{Bernoulli}(p)$, its PMF is %$f(k) = \begin{cases} 1-p & \text{ if }k=0, \\ p & \text{ if }k=1, \text{ and zero otherwise.} \end{cases}$
\begin{align*}
f(k) & = \begin{cases} 1-p & \text{ if }k=0, \\ p & \text{ if }k=1,\end{cases} \\
\intertext{and zero otherwise, so its PGF is}
G(t) & = \sum_{k=0}^\infty f(k) t^k = (1-p)t^0 + pt^1 = 1 - p + pt.  \\
\end{align*}
% poisson
\it \textbf{Poisson}: if X$\sim\text{Poisson}(\lambda)$, its PMF is 
\begin{align*}
f(k) & = \displaystyle\frac{\lambda^k e^{-\lambda}}{k!} \quad\text{for } k=0,1,2,\ldots, \\
\intertext{and zero otherwise, so its PGF is}
G(t) & = \sum_{k=0}^\infty f(k) t^k
		= \sum_{k=0}^{\infty}\left(\frac{\lambda^k e^{-\lambda}}{k!}\right)t^k
		= e^{-\lambda}\sum_{i=1}^\infty \frac{(\lambda t)^k}{k!}
		= e^{-\lambda}e^{\lambda t}
		= e^{\lambda(t-1)}. \\
\end{align*}
% geometric
\it \textbf{Geometric}: if $X\sim\text{Geometric}(p)$, its PMF is
\begin{align*}
f(k) & = (1-p)^k p \text{ for } k=0,1,2,\ldots, \\
\intertext{and zero otherwise, so its PGF is}
G(t) & = \sum_{k=0}^\infty f(k) t^k
		= \sum_{k=0}^\infty (1-p)^k p t^k 
		= p\sum_{k=0}^\infty \big[(1-p)t\big]^k 
		= \frac{p}{1-(1-p)t} \quad\text{ for all } |t|<\frac{1}{1-p}.
\end{align*}
Here, we have used the fact that $\displaystyle\sum_{k=0}^{\infty} r^k = \frac{1}{1-r}$ for $|r|<1$.
\een
%\begin{align*}
%%\intertext{\textbf{Constant}: if $\prob(X=c)=1$,}
%%G(t)	
%%	& = \sum_{k=0}^\infty p(k) t^k = t^c. \\
%\intertext{\textbf{Bernoulli}: if $X\sim\text{Bernoulli}(p)$}
%G(t) 
%	& = \sum_{k=0}^\infty f(k) t^k = (1-p)t^0 + pt^1 = 1 - p + pt.  \\
%\intertext{\textbf{Poisson}: if X$\sim\text{Poisson}(\lambda)$,}
%G(t) 
%	& = \sum_{k=0}^\infty f(k) t^k
%		= \sum_{k=0}^{\infty}\left(\frac{\lambda^k e^{-\lambda}}{k!}\right)t^k
%		= e^{-\lambda}\sum_{i=1}^\infty \frac{(\lambda t)^k}{k!}
%		= e^{-\lambda}e^{\lambda t}
%		= e^{\lambda(t-1)} \\
%\intertext{\textbf{Geometric}: if $X\sim\text{Geometric}(p)$ and $q=1-p$, then for $|tq|<1$,}
%G(t) 
%	& = \sum_{k=0}^\infty f(k) t^k
%		= \sum_{k=0}^\infty q^k p t^k 
%		= p\sum_{k=0}^\infty \big(tq\big)^k 
%		= \frac{p}{1-tq} 
%\end{align*}
%Here, we have used the fact that $\displaystyle\sum_{k=0}^{\infty} r^k = \frac{1}{1-r}$ for $|r|<1$.
\end{example}

% theorem
\begin{theorem}
Let $X$ be a random variable, and let $G(t)$ denote its PGF. Then $\expe(X) = G'(1)$, and more generally,
\[
\expe\big[X(X-1)\ldots(X-n+1)\big] = G^{(n)}(1),
\]
where $G^{(n)}(1)$ is the $n$th derivative of $G(t)$ evaluated at $t=1$.
\end{theorem}

% proof
\begin{proof}
Take $t < 1$, and compute the $n$th derivative of $G$ to obtain
\begin{align*}
G^{(n)}(t) 
	& = \sum_{k=0}^{\infty} t^{k-n} k(k-1)\cdots(k-n+1) f(k) \\
	& = \expe\big[t^{X-n} X(X-1)\cdots(X-n+1)\big]
\end{align*}
\bit
\it If $R>1$ it follows immediately that $G^{(n)}(1) = \expe\big[X(X-1)\cdots(X-n+1)\big]$.
\it If $R=1$, Abel's theorem yields the same conclusion.
\eit
\end{proof}

% remark
\begin{remark}
$\expe\big[X(X-1)\ldots(X-n+1)\big]$ is called the $n$th \emph{factorial moment} of $X$.
\end{remark}

% example
\begin{example}
The variance of $X$ can be written in terms of $G(t)$ as follows:
\begin{align*}
\var(X)
	& = \expe(X^2) - \expe(X)^2 \\
	& = \expe\big[X(X-1) + X\big] - \expe(X)^2 \\
	& = \expe\big[X(X-1)\big] + \expe(X) - \expe(X)^2 \\
	& = G''(1) + G'(1) - G'(1)^2.
\end{align*}
\end{example}

%----------------------------------------------------------------------

\subsection{Sums of random variables} 
%----------------------------------------------------------------------

%\bit
%\it Generating functions provide a useful tool for studying \emph{sums of random variables}.
%\eit

Let $X$ and $Y$ be two independent discrete random variables, both taking values in $\{0,1,2,\ldots\}$. The PMF of their sum $X+Y$ is given by the convolution of the individual PMFs,
\[
\prob(X+Y=k) = \sum_{j=0}^{\infty}\prob(X=j)\prob(Y=k-j).
\]

The corresponding PGFs satisfy a more straightforward, multiplicative relationship:

\newpage
% theorem
\begin{theorem}\label{thm:GF-X+Y}
% theorem
%\begin{theorem}
If $X$ and $Y$ are independent, then $G_{X+Y}(t) = G_X(t)G_Y(t)$.
%\ben
%\it If $X$ and $Y$ are independent, then $G_{X+Y}(t) = G_X(t)G_Y(t)$.
%\it If $Y = a + bX$, then $G_Y(t) = t^{a}G_X(t^b)$
%\een
\end{theorem}
\begin{proof}
%\ben
%\it By independence, 
%$
%G_{X+Y}(t) = \expe(t^{X+Y}) = \expe(t^X t^Y) = \expe(t^X)\expe(t^Y) = G_X(t)G_Y(t).
%$
%\it For $Y=a+bX$, 
%$
%G_Y(t) = \expe\big(t^{a+bX}\big) = t^{a}\expe\big(t^{bX}\big) = t^{a}G_X(t^b).
%$
%\een
If $X$ and $Y$ are independent, then $t^X$ and $t^Y$ are also independent, so 
\[
G_{X+Y}(t) = \expe(t^{X+Y}) = \expe(t^X t^Y) = \expe(t^X)\expe(t^Y) = G_X(t)G_Y(t).
\]
%\vspace*{-3ex}
\end{proof}

% corollary
\begin{corollary}
If $S=X_1+X_2+\ldots+X_n$ is a sum of independent random variables taking values in the non-negative integers, its PGF is 
\[
G_S(t) = G_{X_1}(t)G_{X_2}(t)\cdots G_{X_n}(t)
\]
\end{corollary}

% example: PCF 
\begin{example}
Show that the PGF of the $\text{Binomial}(n,p)$ distribution is $G(t)=(1-p+pt)^n$.
\end{example}
\begin{solution}
Let $X_1,X_2,\ldots,X_n$ be independent Bernoulli variables with parameter $p$, and let 
\[
S = X_1 + X_2 + \ldots + X_n.
\]
Each $X_i$ has generating function $G(t) = 1 - p + pt$, so by Theorem~\ref{thm:GF-X+Y},
\[
G_S(t) = G_{X_1+X_2+\ldots+X_n}(t) = \big[G(t)\big]^n = (1 - p + pt)^n.
\]
\end{solution}

% example: sum of Poisson r.v.s
\begin{example}
Let $X\sim\text{Poisson}(\lambda)$ and $Y\sim\text{Poisson}(\mu)$ be independent. Show that $X+Y\sim\text{Poisson}(\lambda+\mu)$.
\end{example}
\begin{solution}
%Let $Z = X+Y$ and let $f_Z$ denote its probability mass function. Since $X$ and $Y$ are independent, $f_Z$ is the convolution of the probability mass functions of $X$ and $Y$:
%\begin{align*}
%f_Z(k) = \sum_{j=0}^{\infty}f_X(k-j)f_Y(j).\text{\qquad (This is sometimes written as $f_Z = f_X\ast f_Y$.)}
%\end{align*}
The PGF of $X$ is 
\[
G_X(t) 
	= \sum_{k=0}^{\infty} f_X(k) t^k
	= \sum_{k=0}^{\infty}\left(\frac{\lambda^k e^{-\lambda}}{k!}\right)t^k
	= e^{\lambda(t-1)}.
\]
Similarly we have $G_Y(t) = e^{\mu(t-1)}$, so the PGF of $Z=X+Y$ is 
\[
G_Z(t) = G_X(t)G_Y(t) = e^{(\lambda+\mu)(t-1)}.
\]
We recognise this as the PGF of a $\text{Poisson}(\lambda+\mu)$ random variable, so $Z\sim\text{Poisson}(\lambda+\mu)$.
\end{solution}

%
% NOT READY FOR THIS YET BECAUSE JOINT DISTRIBUTIONS HAS BEEN MOVED !!!!!!
%
%\bigskip
%In some applications we encounter sums of random variables with a random number of terms.
%
%% theorem
%\begin{theorem}\label{thm:GF-NofX}
%Let $X_1,X_2,\ldots$ be independent and identically distributed random variables, and let $G_X(t)$ denote their common PGF. Let $N$ be a random variable, taking values in the non-negative integers and which is independent of the $X_i$, and let $G_N(t)$ denote its PGF. Then the PGF of the sum $S=X_1+X_2+\ldots+X_N$ is
%\[
%G_S(t) = G_N\big[G_X(t)\big].
%\]
%\end{theorem}
%
%\begin{proof}
%\begin{align*}
%G_S(t) = \expe(t^S) 
%	& = \expe\big(\expe(t^S\,|\,N)\big) \text{\quad(law of total expectation)}\\
%	& = \sum_{n=0}^{\infty}\expe(t^S\,|\,N=n)\prob(N=n) \\
%	& = \sum_{n=0}^{\infty}\expe(t^{X_1+X_2+\ldots+X_n}\,|\,N=n)\prob(N=n) \\
%	& = \sum_{n=0}^{\infty}\expe(t^{X_1})\expe(t^{X_2})\cdots\expe(t^{X_n})\prob(N=n) \quad\text{(by independence)}\\
%	& = \sum_{n=0}^{\infty}G_X(t)^n\prob(N=n) \\
%	& = G_N\big[G_X(t)\big].
%\end{align*}
%\end{proof}

%% example: compound distribution
%\begin{example}
%A hen lays $N$ eggs, where $N$ has Poisson distribution with parameter $\lambda$. If each egg hatches independently with probability $p$, show that the total number of chicks has Poisson distribution with parameter $\lambda p$.
%\end{example}
%\begin{solution}
%Let $S = X_1+X_2+\ldots+X_N$ denote the total number of chicks. Each $X_i$ has $\text{Bernoulli}(p)$ distribution; let $G_X(t)$ denote their common generatig function. Then
%\[
%G_X(t) = 1 - p + pt \qquad\text{and}\qquad G_N(t) = e^{\lambda(t-1)},
%\]
%so by Theorem~\ref{thm:GF-NofX}, 
%\[
%G_S(t) = G_N\big(G_X(t)\big) = e^{\lambda(-p+pt)} = e^{\lambda p(t-1)}
%\]
%which we recognise as the generating function of the $\text{Poisson}(\lambda p)$ distribution.
%\end{solution}

%%----------------------------------------------------------------------
%\section{Crazy dice}
%%----------------------------------------------------------------------
%
%% example: crazy dice
%\begin{example}
%Two fair dice, with faces numbered from one to six, are rolled, and the total score $X$ is observed. The distribution of $X$ is shown in the following table. 
%\[\begin{array}{|c|ccccccccccc|}\hline
%x				& 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 \\\hline
%\prob(X=x)	& 1/36 & 2/36 & 3/36 & 4/36 & 5/36 & 6/36 & 5/36 & 4/36 & 3/36 & 2/36 & 1/36 \\\hline
%\end{array}\]
%Find another way of numbering two fair six-sided dice with positive integers such that the total score has this distribution?
%\end{example}
%
%%\begin{solution}
%The proof of Theorem~\ref{thm:wlln} shows thatLet $X_1$ and $X_2$ respectively denote the score on the first die and second die, and let $X=X_1+X_2$ be the total score. The PGF of both $X_1$ and $X_2$ is 
%\[
%G_{X_1}(t) = G_{X_2}(t) = \sum_{k=1}^{6} p(k) t^k = \frac{1}{6}(t + t^2 + t^3 + t^4 + t^5 + t^6)
%\]
%
%
%
%Hence the PGF of $X=X_1+X_2$ is
%\begin{align*}
%G_X(t) 
%	& = G_{X_1}(t) G_{X_2}(t) \\
%	& = \frac{1}{36} (t^2 + 2t^3 + 3t^4 + 4t^5 + 5t^6 + 6t^7 + 5t^8 + 4t^9 + 3t^{10} + 2t^{11} + t^{12}).
%\end{align*}
%
%A discrete distribution is uniquely determined by its PGF, so the question reduces to finding an alternative expression for $G_X(t)$. Thus we need to find non-negative integers $a_1,\ldots,a_6$ and $b_1,\ldots,b_6$ such that
%\begin{align}
%\MoveEqLeft[6]
%(t^{a_1} + \ldots + t^{a_6})(t^{b_1}+\ldots+t^{b_6}) \nonumber \\
%	& = t^2 + 2t^3 + 3t^4 + \ldots + 3t^{10} + 2t^{11} + t^{12} \nonumber \\
%	& = (t + t^2 + t^3 + t^4 + t^5 + t^6)(t + t^2 + t^3 + t^4 + t^5 + t^6) \tag{$\ast$}
%\end{align}
%The polynomial $t + t^2 + t^3 + t^4 + t^5 + t^6$ can be factorised as follows:
%\[
%t + t^2 + t^3 + t^4 + t^5 + t^6 = t(1+t+t^2)(1+t)(1-t+t^2)
%\]
%
%
%
%Equation ($\ast$) can thus be written as
%\begin{align*}
%\MoveEqLeft[6]
%(t^{a_1} + \ldots + t^{a_n})(t^{b_1}+\ldots+t^{b_6}) \\
%	& = \big[t(1+t+t^2)(1+t)(1-t+t^2)\big]\big[t(1+t+t^2)(1+t)(1-t+t^2)\big] \\
%	& = \big[t(1+t+t^2)(1+t)(1-t+t^2)^2\big]\big[t(1+t+t^2)(1+t)\big]
%\end{align*}
%Let us define the random variables $Y_1$ and $Y_2$ by their PGFs:
%\begin{align*}
%G_{Y_1}(t) = \frac{1}{6}\sum_{k=1}^6 t^{a_k} 
%	& = \frac{1}{6}t(1+t+t^2)(1+t)(1-t+t^2)^2 \\
%	& = \frac{1}{6}\big(t + t^3 + t^4 + t^5 + t^6 + t^8\big), \\
%G_{Y_2}(t) = \frac{1}{6}\sum_{k=1}^6 t^{b_k} 
%	& = \frac{1}{6}t(1+t+t^2)(1+t) \\
%	& = \frac{1}{6}\big(t + 2t^2 + 2t^3 + t^4\big).
%\end{align*}
%The PGF of the random variable $Y=Y_1+Y_2$ is therefore
%\begin{align*}
%G_{Y}(t) = G_{Y_1+Y_2}(t) 
%	& = G_{Y_1}(t) G_{Y_2}(t) \\
%	& = \frac{1}{36}(t + t^3 + t^4 + t^5 + t^6 + t^8)(t + 2t^2 + 2t^3 + t^4) \\
%	& = \frac{1}{36}(t^2 + 2t^3 + 3t^4 + \ldots + 3t^{10} + 2t^{11} + t^{12}).
%\end{align*}
%The distribution of the combined score on two fair six-sided dice, one numbered $(1,3,4,5,6,8)$ and the other numbered $(1,2,2,3,3,4)$, is therefore the same as the distribution of the combined score on two conventional six-sided dice.
%%\end{solution}

%----------------------------------------------------------------------
\section{Exercises}
\input{ex13_PGFs.tex}
%----------------------------------------------------------------------


%======================================================================
\endinput
%======================================================================
