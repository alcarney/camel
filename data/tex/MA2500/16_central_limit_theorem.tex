% !TEX root = main.tex
%----------------------------------------------------------------------
\chapter{The Central Limit Theorem}\label{chap:clt}
%----------------------------------------------------------------------
%%----------------------------------------------------------------------
%\section{Preliminaries}
%%----------------------------------------------------------------------
%
%% defn: landau notation
%\begin{definition}[Landau notation]
%\ben
%\it % sequences
%Let $a_1,a_2,\ldots$ and $b_1,b_2,\ldots$ be two sequences of real numbers, with 
%\[
%\displaystyle\frac{a_n}{b_n}\to 0\text{\quad as\quad} n\to\infty.
%\]
%	\bit
%	\it We say that $a_n$ is \emph{asymptotically dominated} by $b_n$ in the limit as $n\to\infty$.
%	\it This is denoted by $a_n = o(b_n)$ as $n\to\infty$.
%	\eit
%\it % functions
%More generally, let $f$ and $g$ be two functions of a real variable, with 
%\[
%\displaystyle\frac{f(x)}{g(x)}\to 0\text{\quad as\quad}x\to x_0.
%\]
%	\bit
%	\it We say that $f(x)$ is \emph{asymptotically dominated} by $g(x)$ in the limit as $x\to x_0$.
%	\it This is denoted by $f(x) = o\big[g(x)\big]$ as $x\to x_0$.
%	\eit
%\een
%\end{definition}
%
%%Examples:
%\begin{example}
%\ben
%\it 
%If $a_n=n$ and $b_n=n^2$, then
%\[
%a_n/b_n \to 0 \text{\quad as\quad} n\to\infty,
%\]
%so $a_n = o(b_n)$ as $n\to\infty$.
%\it 
%If $a_n=n$ and $b_n=2n$, then 
%\[
%a_n/b_n \to 1/2 \text{\quad as\quad} n\to\infty,
%\]
%so $a_n$ is not asymptotically dominated by $b_n$ as $n\to\infty$.
%%\it[]
%\it 
%If $f(x)=x$ and $g(x)=x^2$, then 
%\[
%f(x)/g(x) \to 0 \text{\quad as\quad} x\to\infty,
%\]
%so $x = o(x^2)$ as $x\to\infty$.
%\it 
%If $f(x)=x^2$ and $g(x)=x$, then 
%\[
%f(x)/g(x) \to 0 \text{\quad as\quad} x\to 0,
%\]
%so $x^2 = o(x)$ as $x\to 0$.
%\een
%\end{example}

We will need the following result from elementary analysis:
% lemma
\begin{lemma}\label{lem:standard_identity}
For any constant $c\in\R$,
\[
\left(1+\frac{c}{n}\right)^n\to e^c \quad\text{as}\quad n\to\infty.
\]
\end{lemma}
%\proofomitted
\begin{proof}
By the binomial theorem,
\begin{align*}
\left(1+\frac{c}{n}\right)^n
	& = \sum_{k=1}^n \binom{n}{k}\left(\frac{c}{n}\right)^k \\
	& = \sum_{k=1}^n \frac{n!}{(n-k)!k!}\left(\frac{c^k}{n^k}\right) \\
	& = \sum_{k=1}^n \frac{c^k}{k!}\left(\frac{n(n-1)\ldots(n-k+1)}{n^k}\right) \\
	& = \sum_{k=1}^n \frac{c^k}{k!}\left[1\left(1-\frac{1}{n}\right)\left(1-\frac{2}{n}\right)\cdots\left(1-\frac{k+1}{n}\right)\right] \\
%\Rightarrow \left(1+\frac{c}{n}\right)^n
	& \to \sum_{k=1}^{\infty} \frac{c^k}{k!} = e^c \quad\text{as}\quad n\to\infty.
\end{align*}
%Hence, $\displaystyle \left(1+\frac{c}{n}\right)^n \to \sum_{k=1}^n \frac{c^k}{k!} = e^c \text{\quad as }n\to\infty$.

\end{proof}

%\vspace*{2ex}

\bigskip
We will also need the following analogue of Theorem~\ref{taylor:mgf}, which is a consequence of Taylor's theorem for functions of a complex variable. Here, $o(t^k)$ denotes a quantity with the property that $o(t^k)/t^k\to 0$ in the limit as $t\to 0$, and represents an `error' term that is asymptotically smaller than the other terms of the expression in the limit as $t\to 0$, and which can therefore be neglected when $t$ is sufficiently small. (This is called \emph{Landau notation}.)
% theorem: taylor for functions of complex variable
\begin{theorem}\label{thm:taylor-complex}
If $\expe(|X^k|)<\infty$, then
\[
\phi(t) = \sum_{j=0}^k \frac{\expe(X^j)}{j!}(it)^j + o(t^k) \qquad\text{as}\quad t\to 0,
\]
%In particular, $\phi^{(k)}(0) = i^k\expe(X^k)$.
\end{theorem}
\proofomitted

%\begin{remark}
%In Theorem~\ref{thm:taylor-complex}, the term $o(t^k)$ represents a quantity that tends to zero \emph{faster} than $t^k$ as $t\to 0$. 
%This is known as \emph{Landau notation}: we say that $h(t) = o\big(g(t)\big)$ as $t\to t_0$ whenever $\displaystyle\frac{h(t)}{g(t)}\to 0$ as $t\to t_0$. 
%\end{remark}

%In what follows, we will only use at most the first three terms of the Taylor expansion:
%%\begin{equation}\label{eq:taylor-complex}
%\[
%\phi(t) = 1 + it\mu_1 -\frac{1}{2}t^2\mu_2 + o(t^2) \quad\text{as}\quad t\to 0. 
%\]
%%\end{equation}



%----------------------------------------------------------------------
%\section{The de~Moivre - Laplace theorem}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\section{Poisson limit theorem}
%----------------------------------------------------------------------
%The following theorem shows that when $n$ is large and $p$ is small, the $\text{Binomial}(n,p)$ distribution can be approximated by the $\text{Poisson}(np)$ distribution.

% theorem
\begin{theorem}[The Poisson limit theorem]
If $X_n\sim\text{Binomial}(n,\lambda/n)$ then the distribution of $X_n$ converges to the $\text{Poisson}(\lambda)$ distribution as $n\to\infty$.
\end{theorem}

% proof
\begin{proof}
By the continuity theorem for characteristic functions, it is enough to show that the characteristic function of $X_n$ converges to the characteristic function of the $\text{Poisson}(\lambda)$ distribution as $n\to\infty$. 

%Recall:
\bit
\it $\text{Binomial}(n,p)$: $$M(t)=(1-p+pe^t)^n \quad\Rightarrow\quad \phi(t)=M(it)=(1-p+pe^{it})^n.$$
\it $\text{Poisson}(\lambda)$: $$M(t)=\exp\big[\lambda(e^t-1)\big] \quad\Rightarrow\quad \phi(t)=M(it)=\exp\big[\lambda(e^{it}-1)\big].$$
\eit
The characteristic function of $X_n\sim\text{Binomial}(n,\lambda/n)$ is 
\[
\phi_n(t) = \expe(e^{itX_n}) 
	= \left(1-\frac{\lambda}{n} + \frac{\lambda}{n}e^{it}\right)^n
	= \left[1 + \frac{\lambda(e^{it}-1)}{n}\right]^n
\]
By Lemma~\ref{lem:standard_identity},
\[
\phi_n(t) \to \exp\big[\lambda(e^{it}-1)\big]\quad\text{as}\quad n\to\infty.
\]
This is the characteristic function of the $\text{Poisson}(\lambda)$ distribution, and the result follows by the continuity theorem for characteristic functions.
\end{proof}

%----------------------------------------------------------------------
\section{Law of large numbers}
%----------------------------------------------------------------------
% theorem
\begin{theorem}
Let $X_1,X_2,\ldots$ be a sequence of i.i.d. random variables with common mean $\mu<\infty$. Then %the sequence of partial sums $S_n = X_1+X_2+\ldots+X_n$ satisfies
\[
\bar{X}_n = \frac{1}{n}\sum_{i=1}^nX_i \to \mu \text{\quad in distribution as\quad} n\to\infty.
\]
\end{theorem}

\bit
\it Unlike Theorem~\ref{thm:wlln}, this result does not require that the $X_i$ have bounded variance.
\it Convergence in distribution is however a weaker property than convergence in probability.
\eit

% proof
\begin{proof}
By the continuity theorem for characteristic functions, it is sufficient to show that the characteristic function of $\bar{X}_n$ converges to the characteristic function of the constant $\mu$ as $n\to\infty$. 

\bit
\it Let $\phi_X$ denote the common characteristic function of the $X_i$.
\it Let $\phi_n$ denote the characteristic function of $\bar{X}_n$.
\eit
By the properties of characteristic functions,
\begin{align*}
\phi_n(t) 
	& = \phi_{\frac{1}{n}(X_1+X_2+\ldots+X_n)}(t) \\
	& = \phi_{(X_1+X_2+\ldots+X_n)}\left(\frac{t}{n}\right)
	= \left[\phi_X\left(\frac{t}{n}\right)\right]^n
\end{align*}
By Theorem~\ref{thm:taylor-complex} (with $k=1$), 
\[
\phi_X(t) = 1 + it\mu + o(t)\quad\text{as}\quad t\to 0,
\]
so by Lemma~\ref{lem:standard_identity}
\[
\phi_n(t) 
	= \left[\phi_X\left(\frac{t}{n}\right)\right]^n
	= \left[1 + \frac{it\mu}{n} + o\left(\frac{t}{n}\right)\right]^n
	\to e^{it\mu} \quad\text{as}\quad n\to\infty.
\]
%Thus by Lemma~\ref{lem:standard_identity},
%\[
%\phi_n(t) 
%	= \left[1 + \frac{it\mu}{n} + o\left(\frac{t}{n}\right)\right]^n \to e^{it\mu} \quad\text{as}\quad n\to\infty.
%\]
This is the characteristic function of the constant $\mu$, and the result follows by the continuity theorem for characteristic functions.
\end{proof}

%----------------------------------------------------------------------
\section{Central limit theorem}
%----------------------------------------------------------------------
Let $X_1,X_2,\ldots$ be i.i.d. random variables, and consider the partial sums
\[
S_n = X_1 + X_2 + \ldots + X_n.
\]
%\bit
%\it 
By independence, $\expe(S_n)=n\mu$ and $\var(S_n)=n\sigma^2$.
%\it
%The law of large numbers says that $S_n$ is approximately equal to its mean $n\mu$ for large $n$.
%\eit

\bigskip
The central limit theorem says that, \emph{irrespective of the distribution of the $X_i$}, the distribution of the standardised variables
\[
S^{*}_n = \frac{S_n-\expe(S_n)}{\sqrt{\var(S_n)}} = \frac{S_n - n\mu}{\sigma\sqrt{n}}
\]
%It turns out that, \emph{irrespective of the distribution of the $X_i$}, the distribution of $S^{*}_n$ 
converges to the standard normal distribution as $n\to\infty$.

% theorem
\begin{theorem}[Central limit theorem]
Let $X_1,X_2,\ldots$ be a sequence of independent and identically distributed with common mean $\mu$ and variance $\sigma^2$. If $\mu$ and $\sigma^2$ are both finite, then the distribution of the normalised sums
\[
S^{*}_n = \frac{S_n - n\mu}{\sigma\sqrt{n}}\qquad\text{where}\qquad S_n=X_1+\ldots+X_n,
\]
converges to the standard normal distribution $\text{N}(0,1)$ as $n\to\infty$.
\end{theorem}

\begin{proof}
Let $\displaystyle Y_i = \frac{X_i-\mu}{\sigma}$. Then $\expe(Y_i)=0$ and $\var(Y_i)=1$, and 
\[
S^{*}_n  = \frac{1}{\sqrt{n}}\sum_{i=1}^n Y_i %= \frac{Y_1+\ldots+Y_1}{\sqrt{n}} %	= \frac{S_n - n\mu}{\sigma\sqrt{n}}
\]

\bit
\it Let $\phi_Y(t)$ denote the common characteristic function of the $Y_i$.
\it Let $\phi_n(t)$ denote the characteristic function of $S^{*}_n$.
\eit
By Taylor's theorem, if $\expe(|Y^k|)<\infty$ we have that 
\[
\phi(t) = \expe(e^{itY}) = \sum_{j=0}^k \frac{\expe(Y^j)}{j!}(it)^j + o(t^k) \qquad\text{as}\quad t\to 0.
\]

Since $\expe(Y^2) = \var(Y) + \expe(Y)^2 = 1$ is finite, we can apply this with $k=2$ to obtain
\[
\phi_Y(t) 
	=  1 - \frac{1}{2}t^2 + o(t^2)
	\quad\text{as}\quad t\to 0
\]
By the properties of characteristic functions, 
\begin{align*}
\phi_n(t)
	& = \phi_{\frac{1}{\sqrt{n}}(Y_1+Y_2+\ldots+Y_n)}(t) \\
	& = \phi_{Y_1+Y_2+\ldots+Y_n}\left(\frac{t}{\sqrt{n}}\right) \\
	& = \left[\phi_Y\left(\frac{t}{\sqrt{n}}\right)\right]^n \\
	& = \left[1 - \frac{t^2}{2n} + o\left(\frac{t^2}{n}\right)\right]^n \\
	& \to e^{-\frac{1}{2}t^2} \quad\text{as}\quad n\to\infty,
\end{align*}
where the last step follows by Lemma~\ref{lem:standard_identity}.
%Finally, using the fact that for any $c\in\R$,
%\[
%\left(1+\frac{c}{n}\right)^n\to e^c \quad\text{as}\quad n\to\infty.
%\]
%we see that %\left(1+\frac{c}{n}\right)^n\to e^c$ as $n\to\infty$ for any $c\in\R$, we see that
%%\begin{align*}
%%\phi_n(t)
%%	& = \left[1 - \frac{t^2}{2n} + o\left(\frac{t^2}{n}\right)\right]^n \quad\text{as}\quad n\to\infty \\
%%	& \to e^{-\frac{1}{2}t^2} \quad\text{as}\quad n\to\infty.
%%\end{align*}
%\[
%\phi_n(t)
%	= \left[1 - \frac{t^2}{2n} + o\left(\frac{t^2}{n}\right)\right]^n 
%	\to e^{-\frac{1}{2}t^2} \quad\text{as}\quad n\to\infty.
%\]
This is the characteristic function of the $\text{N}(0,1)$ distribution, and the result follows by the continuity theorem for characteristic functions.
\end{proof}

% example: (Erlang)
\begin{example}[Erlang Distribution]
The \emph{Erlang distribution} with parameters $k\in\N$ and $\lambda>0$ is defined to be the sum of $k$ independent and identically distributed random variables $X_1,X_2,\ldots,X_k$, where each $X_i$ is exponentially distributed with (rate) parameter $\lambda$. Show that if $Y\sim\text{Erlang}(k,\lambda)$, then the random variable $$Z_k=\displaystyle\frac{\lambda Y-k}{\sqrt{k}}$$ has approximately the standard normal distribution when $k$ is large.
\end{example}

\begin{solution}
Let $Y\sim\text{Erlang}(k,\lambda)$. Then $Y$ can be written as the sum of $k$ independent and identically distributed random variables $X_i$:
\[
Y = X_1 + X_2 + \ldots + X_k\qquad\text{where}\quad X_i\sim\text{Exponential}(\lambda).
\]
Since $X_i\sim\text{Exponential}(\lambda)$ with $\lambda > 0$, we have 
\[
\expe(X_i) = \frac{1}{\lambda} <\infty
\text{\quad and\quad}
\var(X_i) = \frac{1}{\lambda^2} <\infty.
\]

Furthermore, by independence we have
\[
\expe(Y)= \sum_{i=1}^k \expe(X_i) = \frac{k}{\lambda}
\qquad\text{and}\qquad
\var(Y) =  \sum_{i=1}^k \var(X_i) = \frac{k}{\lambda^2}.
\]

Let $Z\sim\text{N}(0,1)$. By the central limit theorem,
\[
\frac{Y - \expe(Y)}{\sqrt{\var(Y)}} \to Z \quad\text{in distribution as $k\to\infty$.}
\]
%\[
%\frac{Y - k\expe(X)}{\sqrt{k\var(X)}} \xrightarrow{D} \text{N}(0,1) \quad\text{as}\quad k\to\infty.
%\]
i.e.\
\[
Z_k = \frac{Y - \expe(Y)}{\sqrt{\var(Y)}} 
	= \frac{Y - k/\lambda}{\sqrt{k/\lambda^2}} 
	= \frac{\lambda Y - k}{\sqrt{k}} 
	\to Z \quad\text{in distribution as $k\to\infty$.}
\]
\end{solution}


%\newpage
%----------------------------------------------------------------------
\section{Exercises}
\input{ex16_central_limit_theorem}
%----------------------------------------------------------------------

%======================================================================
\endinput
%======================================================================
