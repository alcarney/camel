% !TEX root = main.tex
%----------------------------------------------------------------------
\chapter{Transformations}\label{chap:transformations}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\section{Transformations of random variables}
%----------------------------------------------------------------------
%Let $(\Omega,\mathcal{F},\prob)$ be a probability space, and let $X:\Omega\to\R$ be a random variable on $(\Omega,\mathcal{F})$.
Let $\mathcal{B}$ denote the Borel $\sigma$-field over $\R$.
%
%\bit
%\it The elements of $\mathcal{B}$ are \emph{measurable sets}.	
%\it The pair $(\R,\mathcal{B})$ is a \emph{measurable space}.
%\eit
%
% defn: measureble function
\begin{definition}
A function $g:\R\to\R$ is said to be a \emph{measurable function} if $g^{-1}(B)\in \mathcal{B}$ for all $B\in\mathcal{B}$.
\end{definition}

%For $g(X)$ to be a random variable on $(\Omega,\mathcal{F})$, we need that it is $\mathcal{F}-measurable$, i.e.\ 
%\bit
%\it $\big\{\omega: g\big(X(\omega)\big)\leq t\big\}\in\mathcal{F}$ for every $t\in\R$, and
%\eit
%In fact, $g(X)$ is $\mathcal{F}-measurable$ if $g^{-1}(B)\in\mathcal{B}$ for all $B\in\mathcal{B}$. Such functions $g:\R\to\R$ are called \emph{Borel measureable}.
%

%% transformation
%Let $X:\Omega\to\R$ be a random variable, let $g:\R\to\R$ be a measurable function, and consider the function $Y=g\circ X$, 
%\[
%\begin{array}{rlcl}
%Y: 	& \Omega & \to 		& \R \\
%	& \omega & \mapsto	& g\big[X(\omega)\big]
%\end{array}
%\]

% lemma
\begin{theorem}\label{thm:gX_is_a_rv}
Let $X:\Omega\to\R$ be a random variable on $(\Omega,\mathcal{F})$, and let $g:\R\to\R$ be a measurable function. Then the function $Y=g(X)$, defined by
\[
\begin{array}{rlcl}
Y : 	& \Omega & \to 		& \R \\
	& \omega & \mapsto	& g\big[X(\omega)\big],
\end{array}
\]
is also a random variable on $(\Omega,\mathcal{F})$.
\end{theorem}

% proof
\begin{proof}
%To show that $Y$ is a random variable on $(\Omega,\mathcal{F})$, we need to show that $\{Y\in B\} \in \mathcal{F}$ for every Borel set $B\in\mathcal{B}$. 
Let $B\in\mathcal{B}$. Then
\[
\{Y\in B\} 
	\equiv \{\omega: Y(\omega)\in B\}
	= \{\omega: g\big[X(\omega)\big] \in B\}
	= \{\omega:X(\omega)\in g^{-1}(B)\}
	\equiv \{X\in g^{-1}(B)\}
\]
%\begin{align*}
%\{Y\in B\} 
%	& = \{\omega: Y(\omega)\in B\} \quad\text{(by definition)} \\
%	& = \{\omega: g\big[X(\omega)\big] \in B\} \\
%	& = \{\omega:X(\omega)\in g^{-1}(B)\} \\
%\end{align*}
\bit
\it $X$ is a random variable, so the event $\{X\in B'\}\in\mathcal{F}$ for all $B'\in\mathcal{B}$.
\it Since $B\in\mathcal{B}$ and $g$ is a measurable function, it follows that $g^{-1}(B)\in\mathcal{B}$.
\it Thus $\{X\in g^{-1}(B)\}\in\mathcal{F}$, because this is true for \emph{every} Borel set.
\eit
This holds for every $B\in\mathcal{B}$, so $\{Y\in B\}\in\mathcal{F}$ for all $B\in\mathcal{B}$, as required.

%Hence, for any $B\in\mathcal{B}$, the set $\{Y\in B\}$ can be written as $\{X\in B'\}$ for some other $B'\in\mathcal{B}$ (namely, $B'=g^{-1}(B)$), and because $X$ is a random variable, we knos that $\{X\in B'\}\in\mathcal{F}$ for all $B\'\in
%
%Let $B\in\mathcal{B}$, and let $B'=g^{-1}(B)$ be its inverse image under $g^{-1}$.
%\par
%Now, 
%$g$ is a measurable function so $B'\in\mathcal{B}$, and $X$ is a random variable, so $X^{-1}(B')\in\mathcal{F}$.
%%\it $g:\R\to\R$ is a measurable function so $B'\in\mathcal{B}$, and
%%\it $X$ is a random variable, so $X^{-1}(B')\in\mathcal{F}$.
%\par
%Hence,\[
%Y^{-1}(B) = (g\circ X)^{-1}(B) = X^{-1}\big[g^{-1}(B)\big] = X^{-1}(B')
%\]
%\qed
%i.e. $Y^{-1}(B)\in\mathcal{F}$ for all $B\in\mathcal{B}$. Hence $Y$ is a random variable on $(\Omega,\mathcal{F})$.
\end{proof}

\bigskip
%\begin{remark}
We say that $Y=g(X)$ is a \emph{transformation} of $X$. 
\bit
\it If we know the distribution of $X$, how can we deduce the distribution of $Y$?
\eit
%\end{remark}

%\begin{remark}
In fact, the distribution of $Y=g(X)$ is completely determined by the distribution of $X$:
\begin{align*}
\prob_Y(B) 
	= \prob(Y\in B) 
	& = \prob\big[\{\omega: Y(\omega)\in B\}\big] \\
	& = \prob\big[\{\omega: g[X(\omega)]\in B\}\big] \\
	& = \prob\big[\{\omega: X(\omega)\in g^{-1}(B)\}\big] \\
	& = \prob\big[X\in g^{-1}(B)\big] \\
	& = \prob_X\big[g^{-1}(B)\big]
\end{align*}
%\end{remark}

\begin{remark}\label{rmk:g_as_rv}
\bit
\it 
Theorem~\ref{thm:gX_is_a_rv} shows that $g(X):\Omega\to\R$ is a random variable over $(\Omega,\mathcal{F})$.
\it 
We can also think of $g:\R\to\R$ as a random variable over $(\R,\mathcal{B})$, whose distribution is given by
\[
\prob(g\in B) 
	= \prob\big[g(X)\in B\big] 
	= \prob\big[X\in g^{-1}(B)\big] 
	= \prob_X\big[g^{-1}(B)\big],
\]
where $\prob_X:\mathcal{B}\to[0,1]$ is the distribution of $X$. 
\it
The distribution of $g$ over $(\R,\mathcal{B})$ is well-defined, because $g^{-1}(B)\in\mathcal{B}$ for all $B\in\mathcal{B}$.
\eit
\end{remark}

%%----------------------------------------
%\subsection{Injective transformations}
%%----------------------------------------
%Let $g:\R\to\R$. 
%\bit
%\it If $g(x)\neq g(x')$ whenever $x\neq x'$, we say that the transformation is \emph{injective}.
%%\it The \emph{support} of $g$ is the smallest closed set for which $g(x)=0$ whenever $x\notin\supp(g)$.
%\it If $g$ is injective, there exists a unique function $g^{-1}:\R\to\R$ called the \emph{inverse function} of $g$, such that $(g^{-1}\circ g)(x)=x$ for all $x\in\R$.
%\it $x=g^{-1}(y)$ is the unique solution of the equation $y=g(x)$.
%\eit

%----------------------------------------------------------------------
\section{Support}
%----------------------------------------------------------------------
PMFs and many PDFs are defined to be zero over certain subsets of $\R$. We must ensure that the PMF or PDF of a transformed variable is defined correctly, over appropriate subsets of $\R$.

% defn
\begin{definition}
\ben
\it % closed set
A set $A\subset\R$ is said to be \emph{closed} if it contains all its limit points.
% (i.e. if $\lim_{n\to\infty}x_n = x$ where every $x_n\in A$, then $x\in A$).
\it % support of function
The \emph{support} of an arbitrary function $h:\R\to\R$, denoted by $\supp(h)$, is the smallest closed set for which $h(x)=0$ for all $x\notin\supp(h)$.
\it % support of rv
The \emph{support} of a random variable $X:\Omega\to\R$ is defined to be the support of its PMF (discrete case) or PDF (continuous case), denoted by $\supp(f_X)$. This is the smallest closed set that contains the \emph{range} of $X$. 
\een
\end{definition}

\begin{remark}
Let $X$ be a random variable and let $g:\R\to\R$. The support of $Y = g(X)$ is the set
\[
\supp(f_Y) = \big\{g(x) : x\in\supp(f_X)\big\}.
\]
\end{remark}

\begin{example}
The PDF of the continuous uniform distribution on $[0,1]$ is
\[
f_X(x) = \left\{\begin{array}{ll}
	1	& 0\leq x\leq 1, \\
	0	& \text{otherwise.}
\end{array}\right.	
\]
\bit
\it The support of $X$ is $\supp(f_X)=[0,1]$
\it For the transformation $g(x)=x^2+2x+3$, the support of $Y=g(X)$ is 
\[
\supp(f_Y) = \{x^2+2x+3 : x\in[0,1]\} = [3,6].
\]
\eit
\end{example}

%----------------------------------------------------------------------
\section{Transformations of CDFs}
%----------------------------------------------------------------------
%\bit
%\it So far, we have assumed that the inverse transformation $g^{-1}(x)$ exists for all $x\in\R$.
%\it In fact, we need only that $g^{-1}$ exists over the \emph{support} of $X$.
%\eit

% theorem: cdf
\begin{theorem}\label{thm:transf_cdf}
Let $X$ be a continuous random variable, and let $f_X$ denote its PDF. Let $g:\R\to\R$ be an injective transformation over $\supp(f_X)$ and let $Y=g(X)$. Finally, let $F_X(x)$ and $F_Y(y)$ respectively denote the CDFs of $X$ and $Y$. 
\ben
\it If $g$ is an increasing function, $F_Y(y) = F_X\big[g^{-1}(y)\big]$.
\it If $g$ is a decreasing function, $F_Y(y) = 1 - F_X\big[g^{-1}(y)\big]$.
\een
\end{theorem}

\begin{proof}
\ben
\it If $g$ is increasing, $g(x)\leq y$ implies that $x\leq g^{-1}(y)$ so
\[
F_Y(y) 
	= \prob(Y\leq y) 
	= \prob\big[g(X)\leq y\big] 
	= \prob\big[X\leq g^{-1}(y)\big]
	= F_X\big[g^{-1}(y)\big].
\]
\it If $g$ is decreasing, $g(x)\leq y$ implies that $x\geq g^{-1}(y)$ so
\begin{align*}
F_Y(y) 
	= \prob(Y\leq y) 
	= \prob\big[g(X)\leq y\big] 
	& = \prob\big[X\geq g^{-1}(y)\big] \\
	& = 1 - \prob\big[X\leq g^{-1}(y)\big] \quad\text{(because $X$ is a continuous r.v.)}\\
	& = 1 - F_X\big[g^{-1}(y)\big].
\end{align*}
\een
\end{proof}	


%----------------------------------------------------------------------
\section{Transformations of PMFs and PDFs}
%----------------------------------------------------------------------

% theorem: discrete
\begin{theorem}[Transformations of PMFs]\label{thm:transf_injective_discrete}
Let $X$ be a discrete random variable and let $f_X$ denote its PMF.  Let $g:\R\to\R$ be an injective transformation over $\supp(f_X)$, and let $Y=g(X)$. Then the PMF of $Y$ is given by
\[
f_Y(y) = f_X\big[g^{-1}(y)\big] 
	\quad\text{for all}\quad y\in \supp(f_Y).%=\big\{g(x):x\in\supp(f_X)\big\}.
\]
\end{theorem}

\begin{proof}
\[
f_Y(y)
	= \prob(Y=y)
	= \prob\big[g(X) = y\big]
	= \prob\big[X = g^{-1}(y)\big]
	= f_X\big[g^{-1}(y)\big].
\]
\end{proof}	


% theorem: continuous
\begin{theorem}[Transformations of PDFs]\label{thm:transf_injective_continuous}
Let $X$ be a continuous random variable and let $f_X$ denote its PDF. Let $g:\R\to\R$ be an injective transformation over $\supp(f_X)$, and let $Y=g(X)$. Then, if the derivative of $g^{-1}(y)$ is continuous and non-zero over $\supp(f_Y)$, the PDF of $Y$ is given by
\[
f_Y(y) = f_X\big[g^{-1}(y)\big]\left|\frac{d}{dy}g^{-1}(y)\right| \quad\text{for all}\quad y\in \supp(f_Y).
\]
%\par
%provided that $\displaystyle\frac{dg^{-1}(y)}{dy}$ is continuous and non-zero for all $y\in\supp(f_Y)$.% the PDF of $Y=g(X)$ can be written as
\end{theorem}

% remark
\begin{remark}
\bit
\it The transformation is equivalent to making a \emph{change of variable} in an integral.
\it The scale factor $\displaystyle\left|\frac{d}{dy}g^{-1}(y)\right|$ ensures that $f_Y(y)$ integrates to one.
\eit
\end{remark}

\begin{proof}
For brevity of notation, let $h(y)$ denote the inverse function $g^{-1}(y)$.
\ben
\it If $g$ is increasing, $F_Y(y) = F_X\big[g^{-1}(y)\big]$, so by the chain rule,
%\begin{align*}
%f_Y(y)
%	= \frac{d}{dy} F_Y(y)
%	& = \frac{d}{dy} F_X\big[g^{-1}(y)\big] \\
%	& = \frac{d}{dg^{-1}(y)} F_X\big[g^{-1}(y)\big]\cdot \frac{dg^{-1}(y)}{dy} \text{\qquad(by the chain rule),} \\
%	& = f_X\big[g^{-1}(y)\big]\left|\frac{dg^{-1}(y)}{dy}\right| \text{\qquad\qquad because} \frac{dg^{-1}(y)}{dy}>0\quad\text{over $\supp(f_X)$.}
%\end{align*}
%
%\it By Theorem~\ref{thm:transf_cdf}, if $g$ decreasing we have $F_Y(y) = 1 - F_X\big[g^{-1}(y)\big]$, so
%\begin{align*}
%f_Y(y)
%	= \frac{d}{dy} F_Y(y)
%	& = \frac{d}{dy} \big[1 - F_X[g^{-1}(y)]\big] \\
%	& = 0 - \frac{d}{dg^{-1}(y)} F_X\big[g^{-1}(y)\big]\cdot \frac{dg^{-1}(y)}{dy} \text{\qquad(chain rule),} \\
%	& = - f_X\big[g^{-1}(y)\big] \frac{dg^{-1}(y)}{dy} \\
%	& = f_X\big[g^{-1}(y)\big]\left|\frac{dg^{-1}(y)}{dy}\right| \text{\qquad\qquad\qquad because} \frac{dh(y)}{dy}<0.
%\end{align*}
%\een
%\end{proof}
%
%
%\begin{proof}
%Let $h(y)=g^{-1}(y)$. If $g$ is increasing, $F_Y(y) = F_X\big[g^{-1}(y)\big]$ so the PDF of $Y$ is
\begin{align*}
f_Y(y)
	= \frac{d}{dy} F_Y(y)
	& = \frac{d}{dy} F_X\big[h(y)\big] \\
	& = \frac{d}{dh(y)} F_X\big[h(y)\big]\cdot \frac{dh(y)}{dy} \\
	& = f_X\big[h(y)\big]\left|\frac{dh(y)}{dy}\right|, \text{\qquad\qquad because } \frac{dh(y)}{dy}>0\text{ over $\supp(f_Y)$.}
\end{align*}

\it If $g$ decreasing, $F_Y(y) = 1 - F_X\big[g^{-1}(y)\big]$, so by the chain rule,
%If $g$ decreasing, $F_Y(y) = 1 - F_X\big[g^{-1}(y)\big]$ so the PDF of $Y$ is
\begin{align*}
f_Y(y)
	= \frac{d}{dy} F_Y(y)
	& = \frac{d}{dy} \big[1 - F_X[h(y)]\big] \\
	& = 0 - \frac{d}{dh(y)} F_X\big[h(y)\big]\cdot \frac{dh(y)}{dy} \\
	& = - f_X\big[h(y)\big] \frac{dh(y)}{dy} \\
	& = f_X\big[h(y)\big]\left|\frac{dh(y)}{dy}\right|, \text{\qquad\qquad because} \frac{dh(y)}{dy}<0\text{ over $\supp(f_Y)$.}
\end{align*}
\een
\end{proof}



%----------------------------------------------------------------------
\section{The probability integral transform}
%----------------------------------------------------------------------
% theorem
\begin{theorem}[The Probability Integral Transform]
Let $X$ be a continuous random variable, let $F(x)$ denote its CDF, and suppose that the inverse $F^{-1}$ of the CDF exists for all $x\in\R$. Then the random variable $Y=F(X)$ has the continuous uniform distribution on $[0,1]$.
\end{theorem}

\begin{proof}
Since $F(x)=P(X\leq x)$ is a CDF, we know that $F(x)\in [0,1]$ for all $x\in\R$. In particular, $P(Y<0)=0$ and $P(Y>1)=0$. For $y\in[0,1]$, because the inverse $F^{-1}$ exists we have that
\begin{align*}
F_Y(y) = P(Y\leq y) 
	& = P\big(F(X)\leq y\big) \\
	& = P\big(X\leq F^{-1}(y)\big) \\
	& = F\big(F^{-1}(y)\big) \\
	& = y,
\end{align*}
which is the CDF of the continuous uniform distribution on $[0,1]$.
\end{proof}

% corollary
\begin{corollary}
Let $F(x)$ be a CDF whose inverse exists for all $x\in\R$, and let $Y\sim\text{Uniform}(0,1)$. Then $F$ is the CDF of the random variable $X = F^{-1}(Y)$.
\end{corollary}

\bit
\it Uniformly distributed pseudo-random numbers in $[0,1]$ can be generated using sophisticated algorithms.
\it Using the probability integral transform, we can convert uniformly distributed pseudo-random samples to pseudo-random samples from other (continuous) distributions:
\eit

\ben
\it Generate uniformly distributed pseudo-random numbers $u_1,u_2,\ldots,u_n$ in $[0,1]$.
\it Compute $x_i = F^{-1}(u_i)$ for $i=1,2,\ldots,n$.
\een
The set $\{x_1,x_2,\ldots,x_n\}$ is a pseudo-random sample from the distribution whose CDF is $F(x)$.

% example: 
\begin{example}
Given an algorithm that generates uniformly distributed pseudo-random numbers in the range $[0,1]$, show how to generate a pseudo-random sample from the exponential distribution with scale parameter $1/2$.
\end{example}
\begin{solution}
The CDF of the exponential distribution with scale parameter $1/2$ (or equivalently, with rate parameter $2$) is 
\[
F(x) = \begin{cases}
	1 - e^{-2x}	& x>0 \\
	0			& \text{otherwise.}
\end{cases}	
\]
First we invert $F$:
\begin{align*}
u = 1 - e^{-2x} 
	& \Rightarrow e^{-2x} = 1-u \\
	& \Rightarrow e^x = \frac{1}{\sqrt{1-u}} \\
	& \Rightarrow x = \log\left(\frac{1}{\sqrt{1-u}}\right)
\end{align*}
Having done this, we generate a pseudo-random sample $u_1,u_2,\ldots,u_n$ from the $\text{Uniform}(0,1)$ distribution, then compute 
\[
x_i = \log\left(\frac{1}{\sqrt{1-u}}\right)\quad\text{for each}\quad i=1,2,\ldots,n.
\]
\end{solution}

%----------------------------------------------------------------------
\section{Exercises}
\input{ex07_transformations}
%----------------------------------------------------------------------

%======================================================================
\endinput
%======================================================================
