% !TEX root = main.tex
%----------------------------------------------------------------------
\chapter{The Law of Large Numbers}\label{chap:lln}
%----------------------------------------------------------------------

%To observe the outcome of a single random experiment does not reveal the laws that hold when the experiment is repeated a large number of times. For many centuries, it was known that the sample mean of the observed values over many repetitions approached a stable value as the number of repetitions increased. At the end of the nineteenth century, Chebyshev, Markov and others not only explained the empirical stability of the sample mean, but established general conditions under which this occurs.

%To take an example from physics, a gas is composed of a huge number of individual molecules in constant motion. The position or velocity of any molecule at any given moment in time cannot be determined. However, under certain conditions we are able compute the average proportion of molecules that have a given velocity, or the average proportion of molecules that are to be found in a given volume. This is precisely what a physiscist wants to know, because the basic characteristics of a gas are its pressure, temperature, volume etc., which are determined not by the actions of a single molecule, but by the action of the entire collection of molecules. For example, the pressure of a gas is determined by the overall effect of the molecules colliding with a surface of unit area per unit time. 

%----------------------------------------------------------------------
\section{Convergence}
%----------------------------------------------------------------------
%To consider the limiting behaviour of sequences $X_1,X_2,\ldots$ of random variables, we must remember that random variables are \emph{functions}. 
%
%% definition: pointwise convergence
%\begin{definition}
%Let $X_1,X_2,\ldots$ and $X$ be functions from some domain $\Omega$ into the real numbers. We say that $X_n\to X$ \emph{pointwise} if for every $\omega\in\Omega$, the sequence $X_1(\omega),X_2(\omega),\ldots$ of real numbers is such that $X_n(\omega)\to X(\omega)$ as $n\to\infty$.
%\end{definition}
%
%
% definition
%\begin{definition}
%Let $X_1,X_2,\ldots$ and $X$ be random variables on some probability space $(\Omega, \mathcal{F},\prob)$. We say that:
%\ben
%\it % a.s.
%$X_n\to X$ \emph{almost surely}, written $X_n\xrightarrow{a.s.} X$, if 
%\[
%\prob\big(\{\omega:X_n(\omega)\to X(\omega) \text{ as } n\to\infty\}\big) = 1.
%\]
%%\it % rth mean
%%$X_n\to X$ \emph{in $r$th mean}, written $X_n\xrightarrow{L^r}X$, if $\expe(|X_n^r|)<\infty$ for all $n$ and
%%\[
%%\expe(|X_n - X|^r) \to 0 \quad\text{as}\quad n\to\infty.
%%\]
%\it % mean
%$X_n\to X$ \emph{in mean}, written $X_n\xrightarrow{L^1}X$, %if $\expe(|X_n|)<\infty$ for all $n$ and
%\[
%\expe(|X_n - X|) \to 0 \quad\text{as}\quad n\to\infty.
%\]
%\it % mean square
%$X_n\to X$ \emph{in mean square}, written $X_n\xrightarrow{L^2}X$, if %$\expe(|X_n|^2)<\infty$ for all $n$ and
%\[
%\expe(|X_n - X|^2) \to 0 \quad\text{as}\quad n\to\infty.
%\]
%\it % prob
%$X_n\to X$ \emph{in probability}, written $X_n\xrightarrow{p}X$, if for all $\epsilon > 0$,
%\[
%\prob(|X_n-X|\geq\epsilon) \to 0 \quad\text{as}\quad n\to\infty.
%\]
%\it % dist
%$X_n\to X$ \emph{in distribution}, written $X_n\xrightarrow{D}X$, if 
%\[
%\prob(X_n\leq x) \to \prob(X\leq x) \quad\text{as}\quad n\to\infty,
%\]
%for all points $x$ at which $F(x)=\prob(X\leq x)$ is continuous.
%\een
%\end{definition}
%
% definition
We define the following notions of convergence for sequences of random variables.
\begin{definition}
Let $X_1,X_2,\ldots$ and $X$ be random variables. We say that

\ben
\it % a.s.
%$X_n\to X$ \emph{almost surely} if $\prob\big(\{\omega:X_n(\omega)\to X(\omega) \text{ as } n\to\infty\}\big) = 1$.
$X_n\to X$ \emph{almost surely} if $\prob(X_n\to X \text{ as } n\to\infty) = 1$,

\it % mean square
$X_n\to X$ \emph{in mean square} if $\expe(|X_n - X|^2) \to 0$ as $n\to\infty$,

\it % mean
$X_n\to X$ \emph{in mean} if $\expe(|X_n - X|) \to 0$ as $n\to\infty$,

\it % prob
$X_n\to X$ \emph{in probability}, if for all $\epsilon > 0$, $\prob(|X_n-X|\geq\epsilon) \to 0$ as $n\to\infty$,

%\it % dist
%$X_n\to X$ \emph{in distribution} if $\prob(X_n\leq x) \to \prob(X\leq x)$ as $n\to\infty$ for all points $x$ at which $F(x)=\prob(X\leq x)$ is continuous.
%
\it % dist
$X_n\to X$ \emph{in distribution} if $F_n(x)\to F(x)$ as $n\to\infty$ for every point $x$ at which $F$ is continuous.
\een
\end{definition}

%% remark
%\begin{remark}
%%\bit
%If $X_n\xrightarrow{a.s.} X$, we also say that $X_n\to X$ \emph{almost everywhere}, or $X_n\to X$ \emph{with probability one}.
%%\it If $\expe(|X_n - X|) \to 0$, we say that $X_n\to X$ \emph{in mean}.
%%\it If $\expe(|X_n - X|^2) \to 0$, we say that $X_n\to X$ \emph{in mean square}.
%%\eit
%\end{remark}
% theorem
\begin{theorem}
\ben
\it Convergence almost surely implies convergence in probability.
\it Convergence in mean square implies convergence in mean.
\it Convergence in mean implies convergence in probability.
\it Convergence in probability implies convergence in distribution.
\een
%\ben
%\it $(X_n\xrightarrow{a.s.}	X)		\quad\Rightarrow\quad (X_n\xrightarrow{p} X)$.
%\it $(X_n\xrightarrow{L^r}  	X)		\quad\Rightarrow\quad (X_n\xrightarrow{L^s} X)$ \qquad for all\quad $r>s\geq 1$.
%\it $(X_n\xrightarrow{L^1}  	X)		\quad\Rightarrow\quad (X_n\xrightarrow{p} X)$.% \qquad for all\quad $r\geq 1$.
%\it $(X_n\xrightarrow{p}		X)		\quad\Rightarrow\quad (X_n\xrightarrow{D} X)$.
%\een
\end{theorem}
\vspace*{-2ex}
\proofomitted

%----------------------------------------------------------------------

\section{The law of large numbers}
%----------------------------------------------------------------------
% theorem: lln - convergence in probability
\begin{theorem}[The weak law of large numbers]\label{thm:wlln}
Let $X_1,X_2,\ldots$ be a sequence of i.i.d. random variables having finite mean $\mu$, and finite variance. Then
\[
\frac{1}{n}\sum_{i=1}^n X_i \to \mu \text{\quad in probability as\ } n\to\infty.
\]
\end{theorem}

% proof
\begin{proof}
\small
\bit
\it 
By the linearity of expectation,
$
\displaystyle\expe\left(\frac{1}{n}\sum_{i=1}^n X_i\right)	= \frac{1}{n}\sum_{i=1}^n \expe(X_i)= \mu.
$
\it
Because the $X_i$ are independent,
$
\displaystyle\var\left(\frac{1}{n}\sum_{i=1}^n X_i\right)	= \frac{1}{n^2}\sum_{i=1}^n \var(X_i) = \frac{\sigma^2}{n}.
$
\it
Applying Chebyshev's inequality,
$
\displaystyle\prob\left(\left|\frac{1}{n}\sum_{i=1}^n X_i - \mu\right| \geq \epsilon\right) \leq \frac{\sigma^2}{n\epsilon^2} \qquad\text{for all $\epsilon>0$.}
$
\eit
Because $\sigma^2$ is finite, it follows that $\displaystyle\frac{1}{n}\sum_{i=1}^n X_i\to\mu$ in probability as $n\to\infty$, as required.
\normalsize
\end{proof}

%\begin{remark}
%The proof of Theorem~\ref{thm:wlln} shows that the sample mean $\bar{X}_n = \displaystyle\frac{1}{n}\sum_{i=1}^n X_i$ satisfies
%\[
%\prob(|\bar{X}_n - \mu| \geq \epsilon) \leq \frac{\sigma^2}{n\epsilon^2} \qquad\forall\ \epsilon>0.
%\]
%We say that the \emph{rate} at which $\bar{X}_n\to\mu$ is of order $O(1/n)$ as $n\to\infty$.
%\end{remark}

% theorem: LLN - convergence in mean square
\begin{theorem}[The law of large numbers: convergence in mean square]\label{thm:lln-msq}
Let $X_1,X_2,\ldots$ be a sequence of i.i.d. random variables having finite mean $\mu$, and finite variance. Then
\[
\frac{1}{n}\sum_{i=1}^n X_i \to \mu \quad\text{in mean square as } n\to\infty.
\]
\end{theorem}

% proof
\begin{proof}
Let $\mu$ and $\sigma^2$ respectively denote the mean and variance of $X$.
\[
\expe\left[\left(\frac{1}{n}\sum_{i=1}^n X_i - \mu\right)^2\right] 
	= \var\left(\frac{1}{n}\sum_{i=1}^n X_i\right) = \frac{\sigma^2}{n}\to 0 \text{ as } n\to\infty.
\]
\end{proof}

% remark
\begin{remark}[Frequentist probability]
A random experiment is repeated $n$ times under the same conditions. Let $A$ be some random event, and let $X_i$ be the indicator variable of the event that $A$ occurs on the $i$th trial. Then the sample mean of the $X_i$ is the \emph{relative frequency} of event $A$ over these $n$ repetitions, and by the law of large numbers,
\[
\frac{1}{n}\sum_{i=1}^n X_i \to \prob(A) \quad\text{as $n\to\infty$.}
\]
This shows that the frequentist model, in which probability is defined to be the limit of relative frequency as the number of repetitions increases to infinity, is a reasonable one.
\end{remark}

%----------------------------------------------------------------------
\section{Bernoulli's law of large numbers*}
%----------------------------------------------------------------------
In the proof of Theorem~\ref{thm:wlln}, we used Chebyshev's inequality to show that% the sample mean $\bar{X}_n = \displaystyle\frac{1}{n}\sum_{i=1}^n X_i$ satisfies
\[
\prob(|\bar{X}_n - \mu| \geq \epsilon) \leq \frac{\sigma^2}{n\epsilon^2} \qquad\forall\ \epsilon>0.
\]
We say that the \emph{rate} at which $\bar{X}_n\to\mu$ is of order $O(1/n)$ as $n\to\infty$.
%
In the proof of the following theorem, we use Bernstein's inequality to show that the sample mean of Bernoulli random variables satisfies
\[
\prob\left(|\bar{X}_n - \mu| > \epsilon\right) \leq e^{-\frac{1}{2}n\epsilon^2}  \qquad\forall\ \epsilon>0.
\]
In this case, the rate at which $\bar{X}_n\to\mu$ as $n\to\infty$ is said to be \emph{exponentially fast}. 

% theorem
\begin{theorem}[Bernoulli's Law of Large Numbers]\label{thm:bernoulli_lln}
Let $X_1,X_2,\ldots$ be independent, with each $X_i\sim\text{Bernoulli}(p)$, and let $\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i$ be the sample mean of the first $n$ variables in the sequence.
Then for every $\epsilon > 0$, 
\[
\prob(|\bar{X}_n - p| > \epsilon) \to 0 \text{\quad as\quad} n\to\infty.
\]
\end{theorem}

% proof
\begin{proof}
Let $\epsilon>0$ and define $S_n=\sum_{i=1}^n X_i$. Then
\[
\prob\left(\bar{X}_n - p > \epsilon\right)
	= \prob\big[S_n > n(p+\epsilon)\big]
\]

%\bit
%\it Recall Bernstein's inequality: $\prob(X>a) \leq e^{-ta}\expe(e^{tX})$ for all $t>0$.
%\eit

Applying Bernstein's inequality (Theorem~\ref{thm:bernstein}) to the random variable $S_n$ with $a = n(p+\epsilon)$,
\begin{align*}
\prob\big[S_n > n(p+\epsilon)\big]
	& \leq e^{-tn(p+\epsilon)}\expe(e^{tS_n}) \\
	& = e^{-tn(p+\epsilon)}\big[1-p+pe^t\big]^n \\
	& = e^{-tn\epsilon}\big[ e^{-tp}(1-p+pe^t) \big]^n \\
	& = e^{-tn\epsilon}\big[ (1-p)e^{-tp} + pe^{t(1-p)} \big]^n
\end{align*}

Using the inequality $e^x \leq x + e^{x^2}$, which holds for all $x\in\R$, % (see exercises),
\begin{align*}
(1-p)e^{-tp} + pe^{t(1-p)}  
	& \leq (1-p)\big[-tp + e^{t^2p^2}\big] + p\big[t(1-p) + e^{t^2(1-p)^2}\big] \\
	& = (1-p)e^{t^2p^2} + pe^{t^2(1-p)^2} \\
	& \leq (1-p)e^{t^2} + pe^{t^2} \\
	& = e^{t^2}.
\end{align*}
Hence, for all $t>0$,
\[
\prob\left(\bar{X}_n -p > \epsilon\right) = \prob\big[S_n > n(p+\epsilon)\big] \leq e^{-tn\epsilon}e^{t^2n} = e^{tn(t-\epsilon)}.
\]
\bit
\it This inequality is valid for all $t>0$. 
\it We choose $t$ so that the right-hand side is made as small as possible. 
\it Because $e^x$ is an increasing function, this corresponds to the minimum value of the exponent. 
\it We differentiate the exponent $tn(t-\epsilon)$ with respect to $t$ and set this equal to zero.
\eit

This yields the value $t = \frac{1}{2}\epsilon$, so
\[
\prob\left(\bar{X}_n -p > \epsilon\right) = \prob\big[S_n > n(p+\epsilon)\big] \leq e^{-\frac{1}{4}n\epsilon^2}.
\]
A similar argument shows that 
\[
\prob\left(\bar{X}_n -p < -\epsilon\right) = \prob\big[S_n < n(p-\epsilon)\big] \leq e^{-\frac{1}{4}n\epsilon^2},
\]
Thus we have
\[
\prob\left(|\bar{X}_n - p| > \epsilon\right) \leq e^{-\frac{1}{2}n\epsilon^2} \to 0 \text{\quad as $n\to\infty$,}
\]
as required.
\end{proof}




%----------------------------------------------------------------------
\section{Exercises}
\input{ex15_law_of_large_numbers}
%----------------------------------------------------------------------

%======================================================================
\endinput
%======================================================================
