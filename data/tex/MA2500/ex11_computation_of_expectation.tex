% !TEX root = main.tex
%----------------------------------------------------------------------
\begin{exercise}
\begin{questions}
%----------------------------------------
%==========================================================================
%\question
%Let $X$ be a continuous random variable with PDF $f(x)$ and range $[-1,1]$ (so $f(x)=0$ for all $|x|>1$). Find the mean $\expe(X)$ and variance $\var(X)$ in each of the following cases:
%\ben
%\it $f(x) = (3/4)(1-x^2)$.
%\it $f(x) = (\pi/4)\cos(\pi x/2)$.
%\it $f(x) = (x+1)/2$.
%\it $f(x) = (3/8)(x + 1)^2$.
%\een
%\begin{answer}
%\ben
%\it %(a)
%$\expe(X)=0$, $\var(X)=1/5$.
%\it %(b)
%$\expe(X)=0$, $\var(X)=(\pi^2-8)/\pi^2$.
%\it %(c)
%$\expe(X)=1/3$, $\var(X)=2/9$.
%\it %(d)
%$\expe(X)=1/2$, $\var(X)=3/20$.
%\een
%
%For example, in part (a), 
%\begin{align*}
%\expe(X)	
%	& = \frac{3}{4}\int_{-1}^1 x(1-x^2)\,dx
%	= \frac{3}{4}\int_{-1}^1 x - x^3 \,dx
%	= \frac{3}{4}\left[\frac{x^2}{2}-\frac{x^4}{4}\right]_{-1}^1
%	= 0 \\
%\intertext{Note that, because the PDF $f(x)$ is an even function and the integral is defined over a symmetric interval, the mean must be equal to zero. The second moment is}
%\expe(X^2)	
%	& = \frac{3}{4}\int_{-1}^1 x^2(1-x^2)\,dx
%	= \frac{3}{4}\int_{-1}^1 x^2 - x^4 \,dx 
%	= \frac{3}{4}\left[\frac{x^3}{3}-\frac{x^5}{5}\right]_{-1}^1
%	= \frac{1}{5} \\
%\end{align*}	
%so $\var(X)=1/5$. The results for parts (ii), (iii) and (iv) follow similarly.
%\end{answer}

%==========================================================================
\question
Let $X$ be the score on a fair die, and let $g(x)=3x-x^2$. Find the expected value and variance of the random variable $Y=g(X)$.
\begin{answer}
The expectation of $Y=3X-X^2$ is determined by the distribution of $X$,
\begin{align*}
\expe(Y)
	& = \sum_{x=1}^6 y(x)f(x) = \sum_{x=1}^6 (3x-x^2)\times \frac{1}{6} \\
	& = \frac{1}{6}\left(3\sum_{x=1}^6 x - \sum_{x=1}^6 x^2\right) = \frac{-14}{3}
\end{align*}
and
\begin{align*}
\expe(Y^2)
	& = \sum_{x=1}^6 y^2(x)f(x) = \sum_{x=1}^6 (3x-x^2)^2\times \frac{1}{6} \\
	& = \frac{1}{6}\left(9\sum_{x=1}^6 x^2 - 6\sum_{x=1}^6 x^3 + \sum_{x=1}^6 x^4\right) = \frac{448}{6}
\end{align*}
Hence
\[
\var(Y) = \expe(Y^2) - \expe(Y)^2 = \frac{448}{6} - \frac{196}{9} = \frac{476}{9}
\]
\end{answer}

%==================================================================================================

% infinite expectation
\question
A long line of athletes $k=0,1,2,\ldots$ make throws of a javelin to distances $X_0,X_1,X_2,\ldots$ respectively. The distances are independent and identically distributed random variables, and the probability that any two throws are exactly the same distance is equal to zero. Let $Y$ be the index of the first athlete in the sequence who throws further than distance $X_0$. Show that the expected value of $Y$ is infinite.

\begin{answer}
$Y$ is a discrete random variable, taking values in the set $\{1,2,\ldots\}$.

\bit
\it The event $\{Y>k\}$ means that out of the first $k+1$ throws, the initial throw was the furthest.
\eit

Because the distances $X_0,X_1,\ldots,X_k$ are identically distributed, it follows that 
\[
\prob(Y>k)=\frac{1}{k+1}.
\]
Thus, 
\[
\prob(Y=k) = \prob(Y>k-1) - \prob(Y>k) = \frac{1}{k} - \frac{1}{k+1} = \frac{1}{k(k+1)}
\]
so
\[
\expe(Y)
	= \sum_{n=0}^\infty k\prob(Y=k)
	= \sum_{n=0}^\infty \frac{1}{k+1} 
	= \sum_{n=1}^\infty \frac{1}{k}
	= \infty.
\]
\end{answer}

%==========================================================================
\question
Consider the following game. A random number $X$ is chosen uniformly from $[0,1]$, then a sequence $Y_1,Y_2,\ldots$ of random numbers are chosen independently and uniformly from $[0,1]$. Let $Y_n$ be the first number in the sequence for which $Y_n > X$. When this occurs, the game ends and the player is paid $(n-1)$ pounds. Show that the expected win is infinite.
\begin{answer}
Let $Z$ be the amount won.
\begin{align*}
\prob(Z=k|X=x)
	& = \prob(Y_1\leq x,Y_2\leq x,\ldots,Y_k\leq x,Y_{k+1}>x) \\
	& = \prob(Y_1\leq x)\prob(Y_2\leq x)\ldots \prob(Y_k\leq x)\prob(Y_{k+1}>x) \qquad\text{(by independence)}\\
	& = x^k(1-x) \\
\intertext{Therefore,}
\prob(Z=k)
	& = \int_0^1 x^k(1-x)\,dx \\
	& = \left[\frac{1}{k+1}x^{k+1}-\frac{1}{k+2}x^{k+2}\right]_0^1 \\
	& = \frac{1}{k+1} - \frac{1}{k+2} \\
	& = \frac{1}{(k+1)(k+2)} \\
\intertext{Thus,}
\expe(Z)
	& = \sum_{k=0}^{\infty}k\left(\frac{1}{(k+1)(k+2)}\right) = \infty.
\end{align*}
\end{answer} 


%==========================================================================
% undefined expectation: discrete
\question
Let $X$ be a discrete random variable with PMF
\[
f(k) = \begin{cases}
	\displaystyle\frac{3}{\pi^2 k^2} 	& \text{if }\ k\in\{\pm 1,\pm 2, \ldots\} \\[2ex]
	0						& \text{otherwise.}
\end{cases}	
\]
Show that $\expe(X)$ is undefined.

% solution
\begin{answer}
Let $X = X^{+} - X^{-}$ where
\begin{align*}
X^{+}	& = \max\{X,0\} 	= \begin{cases}  X & \text{ if } X \geq 0, \\ 0 & \text{otherwise.}\end{cases} \\
X^{-}	& = \max\{-X,0\}	= \begin{cases} -X & \text{ if } X <    0, \\ 0 & \text{otherwise.}\end{cases}
\end{align*}
Then
\[\begin{array}{lll}
\expe(X^{+})	
	& = \displaystyle \sum_{k=1}^{\infty} k \left(\frac{3}{\pi^2 k^2}\right)
	& = \displaystyle \frac{3}{\pi^2} \sum_{k=1}^{\infty}\frac{1}{k} = \infty \\
\expe(X^{-})	
	& = \displaystyle \sum_{k=-\infty}^{-1} (-k) \left(\frac{3}{\pi^2 k^2}\right)
	& = \displaystyle \frac{3}{\pi^2} \sum_{k=1}^{\infty}\frac{1}{k} = \infty \\
\end{array}\]
so $\expe(X) = \expe(X^{+}) - \expe(X^{-})$ is undefined.
\end{answer}

%==========================================================================
% undefined expectation: continuous (cauchy)
\question
Let $X$ be a continuous random variable having the Cauchy distribution, defined by the PDF
\[
f(x) = \frac{1}{\pi(1+x^2)}\qquad x\in\R
\]
Show that $\expe(X)$ is undefined.
\begin{answer}
The expectation of $X$ is 
\begin{align*}
\expe(X)
	& = \expe(X_{+}) - \expe(X_{-}) \\
	& = \int_0^\infty xf(x)\,dx - \int_{-\infty}^0 (-x)f(x)\,dx \\
	& = \int_0^\infty \frac{x}{\pi(1+x^2)}\,dx - \int_0^{\infty} \frac{x}{\pi(1+x^2)}\,dx
\end{align*}

If $x>1$ then $x^2 > 1$ and therefore $2x^2 > 1+x^2$, so
\[
\frac{x}{1+x^2} > \frac{1}{2x} \qquad\text{for all } x > 1
\]
Consequently,
\[
\int_{0}^{\infty}\frac{x}{1+x^2}\,dx
	> \int_{1}^{\infty}\frac{x}{1+x^2}\,dx
	> \frac{1}{2}\int_{1}^{\infty}\frac{1}{x}\,dx
	= \infty
\]
Thus $X$ is not integrable:
\[
\expe(|X|) = \expe(X_{+}) + \expe(X_{-}) = 2\int_0^\infty \frac{x}{\pi(1+x^2)}\,dx = \infty
\]
and $\expe(X)$ is not defined.
\end{answer}


%==========================================================================
\question
A coin is tossed until the first time a head is observed. If this occurs on the $n$th toss and $n$ is odd, you win $2^n/n$ pounds, but if $n$ is even then you lose $2^n/n$ pounds. Show that the expected win is undefined.
\begin{answer}
Let $X$ represent the amount won.
$\prob(\text{First head occurs on $n$th toss}) = 1/2^n$, so
\begin{align*}
\expe(X)
	& = \sum_{n=1}^{\infty} \left(\frac{(-1)^{n+1}2^n}{n} \times \frac{1}{2^n}\right) \\
	& = \sum_{n=1}^{\infty} (-1)^{n+1} \frac{1}{n} \\
	& = 1 - \frac{1}{2} + \frac{1}{3} - \frac{1}{4} + \ldots \\
\end{align*}
This is the alternating harmonic series, which is not absolutely convergent. Hence the expected win is undefinned.

\textbf{Remark}. It is known that the alternating harmonic series is convergent:
\[
\sum_{n=0}^{\infty} \frac{(-1)^{n+1}}{n} = 1 - \frac{1}{2} + \frac{1}{3} - \frac{1}{4} + \ldots = \log 2
\]
However, the series is not absolutely convergent, because
\[
\sum_{n=0}^{\infty} \left|\frac{(-1)^{n+1}}{n}\right| 
	= \sum_{n=0}^{\infty}\frac{1}{n} = \infty 
\]
The Riemann rearrangement theorem says that if a series is convergent but not absolutely convergent, then its limit depends on the order in which its terms are added. For example
\begin{align*}
\log 2
	& = 1 - \frac{1}{2} + \frac{1}{3} - \frac{1}{4} + \frac{1}{5} - \frac{1}{6} + \frac{1}{7} - \frac{1}{8} + \ldots \\
	& = 1 - \frac{1}{2} - \frac{1}{4} + \frac{1}{3} - \frac{1}{6} - \frac{1}{8} + \frac{1}{5} - \frac{1}{10} + \ldots \\
	& = \left(1 - \frac{1}{2}\right) - \frac{1}{4} + \left(\frac{1}{3} - \frac{1}{6}\right)
			 - \frac{1}{8} + \left(\frac{1}{5} - \frac{1}{10}\right) + \ldots \\
	& = 1 - \frac{1}{4} + \frac{1}{6} - \frac{1}{8} + \frac{1}{10} - \ldots \\
	& = \frac{1}{2}\left(1 - \frac{1}{2} + \frac{1}{3} - \frac{1}{4} + \frac{1}{5} - \ldots \right)\\
	& = \frac{1}{2}\log 2	
\end{align*}
which is absurd, since $\log 2 \neq 0$.	The expectation $\expe(X)=\sum_x g(x)f(x)$ of a discrete random variable cannot be sensibly defined unless the series $\sum_x g(x)f(x)$ is absolutely convergent.
 
\end{answer}

%==========================================================================
\question
Let $X$ be a continuous random variable with uniform density on the interval $[-1,1]$,
\[
f(x) = \begin{cases}
	\frac{1}{2}	& \text{ if } x\in[-1,+1] \\
	0			& \text{ otherwise.}
\end{cases}
\]
Compute $\expe(X)$, $\expe(X^2)$, $\expe(X^3)$, $\expe(1/X)$ and $\expe(1/X^2)$.
%\begin{answer}
%\ben
%\it % (a)
%\begin{align*}
%\expe(X)	& = \frac{1}{2}\int_{-1}^{1} x   \,dx = 0 \\
%\expe(X^2)	& = \frac{1}{2}\int_{-1}^{1} x^2 \,dx = 1/3 \\
%\expe(X^3)	& = \frac{1}{2}\int_{-1}^{1} x^3 \,dx = 0 
%\end{align*}
%\it % (b)
%\begin{align*}
%\expe\left(\frac{1}{X}\right)
%	& = \frac{1}{2}\int_{0}^{1}\frac{1}{x}\,dx +  \frac{1}{2}\int_{-1}^{0}\frac{1}{x}\,dx \\
%	& = \frac{1}{2}\int_{0}^{1}\frac{1}{x}\,dx -  \frac{1}{2}\int_{ 0}^{1}\frac{1}{x}\,dx \\
%	& = \infty - \infty
%\end{align*}
%so $\expe(1/X)$ does not exist.
%\it % (c)
%\[
%\expe\left(\frac{1}{X^2}\right) 
%	= \frac{1}{2}\int_{-1}^{1}\frac{1}{x^2}\,dx
%	= \int_{0}^{1}\frac{1}{x^2}\,dx 
%	= \infty
%\]
%so $\expe(1/X^2)$ is infinite.
%\een
%\end{answer} 

%\begin{answer}
%% preliminaries
%Let $F$ be the CDF of $X$, let $g:\R\to\R$, and recall the following:
%\bit
%\it 
%If $g(X)$ is non-negative random variable, its expectation with respect to $F$ is 
%\[
%\expe\big[g(X)\big] = \int_{-\infty}^{\infty} g(x)f(x)\,dx
%\]
%(For non-negative random variables, we can accept that its expectation is infinite.)
%\it 
%If $g(X)$ is a signed random variable, its expectation with respect to $F$ is only defined if
%\[
%\int_{-\infty}^{\infty} |g(x)| f(x)\,dx < \infty.
%\]
%If this condition holds, the expectation is given by
%\[
%\expe\big[g(X)] = \int_{-\infty}^{\infty} g^{+}(x)f(x)\,dx - \int_{-\infty}^{\infty} g^{-}(x)f(x)\,dx
%\]
%where $g^{+}(x)$ and $g^{-}(x)$ are respectively is the positive and negative parts of $g(x)$:
%\[
%g^{+}(x) = \begin{cases}  g(x) & \text{ if}\quad g(x)\geq 0, \\ \ \ 0 & \text{ if}\quad g(x) < 0,\end{cases}
%\qquad\text{and}\qquad
%g^{-}(x) = \begin{cases}  \quad 0 & \text{ if}\quad g(x)\geq 0, \\ -g(x) & \text{ if}\quad g(x) < 0.\end{cases}
%\]
%\eit
%
%% solutions
%\ben
%\it % << (i)
%$g(x) = x$. In this case, $g(x)$ is a signed function. Since
%\[
%|g(x)| = \begin{cases}  \phantom{-}x & \text{ if } x\geq 0, \\ -x & \text{ if } x < 0,\end{cases}
%\]
%we see that the expectation exists:
%\[
%\int_{-\infty}^{\infty}|g(x)|f(x)\,dx 
%	= \frac{1}{2}\int_{-1}^0 (-x)\,dx + \frac{1}{2}\int_0^1 x\,dx \\
%	= \int_0^1 x\,dx
%	= \left[\frac{x^2}{2}\right]_0^1
%	= \frac{1}{2} 
%	< \infty.
%\]
%The positive and negative parts of $g$ are
%\[
%g^{+}(x)	= \begin{cases}  x & \text{ if}\quad x\geq 0, \\ 0 & \text{ if}\quad x < 0,\end{cases}
%\qquad\text{and}\qquad
%g^{-}(x)	= \begin{cases}  0 & \text{ if}\quad x\geq 0, \\ -x & \text{ if}\quad x < 0.\end{cases}
%\]
%Thus we have
%\begin{align*}
%\expe(X) = \expe\big[g(X)] 
%	& = \int_{-\infty}^{\infty} g^{+}(x)f(x)\,dx - \int_{-\infty}^{\infty} g^{-}(x) f(x)\,dx \\
%	& = \frac{1}{2}\int_0^1 x\,dx - \frac{1}{2}\int_{-1}^0 (-x)\,dx \\
%	& = \frac{1}{2}\left[\frac{x^2}{2}\right]_0^1 - \frac{1}{2}\left[\frac{-x^2}{2}\right]_{-1}^0 \\
%	& = \left(\frac{1}{4}-0\right) - \left(0 + \frac{1}{4}\right) = 0.
%\end{align*}
%Note that, if we regard an integral as the "area between a curve and the $x$-axis", the positive part gives the area above the $x$-axis (which has a positive sign), and the negative part gives the area below the $x$-axis (which has a negative sign): the integral is zero because these two areas are of equal magnitude.
%
%\it % << (ii)
%$g(x) = x^2$. In this case, $g(x)$ is a non-negative function, so
%\[
%\expe(X^2) = \expe\big[g(X)] 
%	= \int_{-\infty}^{\infty} g(x)f(x)\,dx 
%	= \frac{1}{2}\int_{-1}^{1} \frac{x^2}{2}\,dx 
%	= \int_{0}^{1} x^2\,dx 
%	= \left[\frac{x^3}{3}\right]_0^1
%	= \frac{1}{3}.
%\]
%
%\it % << (iii)
%$g(x) = x^3$. In this case, $g(x)$ is a signed function. Since
%\[
%|g(x)| = \begin{cases}  x^3 & \text{ if } x\geq 0, \\ -x^3 & \text{ if } x < 0,\end{cases}
%\]
%we see that its expectation exists:
%\[
%\int_{-\infty}^{\infty}|g(x)|f(x)\,dx 
%	= \frac{1}{2}\int_{-1}^0 (-x^3)\,dx + \frac{1}{2}\int_0^1 x^3\,dx \\
%	= \int_0^1 x^3\,dx
%	= \left[\frac{x^4}{4}\right]_0^1
%	= \frac{1}{4} 
%	< \infty.
%\]
%The positive and negative parts of $g$ are
%\[
%g^{+}(x)	= \begin{cases}  x^3 & \text{ if}\quad x^3\geq 0, \\ 0 & \text{ if}\quad x^3 < 0,\end{cases}
%\qquad\text{and}\qquad
%g^{-}(x)	= \begin{cases}  0 & \text{ if}\quad x^3\geq 0, \\ -x^3 & \text{ if}\quad x^3 < 0.\end{cases}
%\]
%Since $x^3\geq 0$ if and only if $x\geq 0$, these can be written as:
%\[
%g^{+}(x)	= \begin{cases}  x^3 & \text{ if}\quad x\geq 0, \\ 0 & \text{ if}\quad x < 0,\end{cases}
%\qquad\text{and}\qquad
%g^{-}(x)	= \begin{cases}  0 & \text{ if}\quad x\geq 0, \\ -x^3 & \text{ if}\quad x < 0.\end{cases}
%\]
%Thus we have
%\begin{align*}
%\expe(X^3) = \expe\big[g(X)] 
%	& = \int_{-\infty}^{\infty} g^{+}(x)f(x)\,dx - \int_{-\infty}^{\infty} g^{-}(x) f(x)\,dx \\
%	& = \frac{1}{2}\int_0^1 x^3\,dx - \frac{1}{2}\int_{-1}^0 (-x^3)\,dx \\
%	& = \frac{1}{2}\left[\frac{x^4}{4}\right]_0^1 - \frac{1}{2}\left[\frac{-x^4}{4}\right]_{-1}^0 \\
%	& = \left(\frac{1}{8}-0\right) - \left(0 + \frac{1}{8}\right) = 0.
%\end{align*}
%
%\it % << (iv)
%$g(x) = 1/x$. In this case, $g(x)$ is a signed function. Since
%\[
%|g(x)| = \begin{cases}  1/x & \text{ if } x\geq 0, \\ -1/x & \text{ if } x < 0,\end{cases}
%\]
%we see that its expectation does \emph{not} exist:
%\begin{align*}
%\int_{-\infty}^{\infty}|g(x)|f(x)\,dx 
%	& = \frac{1}{2}\int_{-1}^0 \frac{-1}{x}\,dx + \frac{1}{2}\int_0^1 \frac{1}{x}\,dx \\
%	& = \frac{1}{2}\int_0^1 \frac{1}{x}\,dx + \frac{1}{2}\int_0^1 \frac{1}{x}\,dx \\
%	& = \int_0^1 \frac{1}{x}\,dx \\
%	& = \infty.
%\end{align*}
%
%Another way of seeing that the expectation is undefined is to consider the positive and negative parts of $g$:
%\[
%g^{+}(x)	= \begin{cases}  1/x & \text{ if}\quad 1/x\geq 0, \\ 0 & \text{ if}\quad 1/x < 0,\end{cases}
%\qquad\text{and}\qquad
%g^{-}(x)	= \begin{cases}  0 & \text{ if}\quad 1/x\geq 0, \\ -1/x & \text{ if}\quad 1/x < 0.\end{cases}
%\]
%Since $1/x \geq 0$ if and only if $x\geq 0$, these can be written as:
%\[
%g^{+}(x)	= \begin{cases}  1/x & \text{ if}\quad x\geq 0, \\ 0 & \text{ if}\quad x < 0,\end{cases}
%\qquad\text{and}\qquad
%g^{-}(x)	= \begin{cases}  0 & \text{ if}\quad x\geq 0, \\ -1/x & \text{ if}\quad x < 0.\end{cases}
%\]
%Thus we have
%\begin{align*}
%\expe\left(\frac{1}{X}\right) = \expe\big[g(X)]  
%	& = \int_{-\infty}^{\infty} g^{+}(x)f(x)\,dx - \int_{-\infty}^{\infty} g^{-}(x) f(x)\,dx \\
%	& = \frac{1}{2}\int_0^1 \frac{1}{x}\,dx - \frac{1}{2}\int_{-1}^0 \frac{-1}{x}\,dx \\
%	& = \frac{1}{2}\int_0^1 \frac{1}{x}\,dx - \frac{1}{2}\int_{0}^{1} \frac{1}{x}\,dx \\
%	& = \infty - \infty.
%\end{align*}
%so $\expe(1/X)$ is undefined.
%
%\it % << (v)
%$g(x) = 1/x^2$. In this case, $g(x)$ is a non-negative function, so
%\begin{align*}
%\expe\left(\frac{1}{X^2}\right) = \expe_F(g)
%	= \int_{-\infty}^{\infty} g(x)f(x)\,dx 
%	= \frac{1}{2}\int_{-1}^{1} \frac{1}{x^2}\,dx
%	= \int_{0}^{1}\frac{1}{x^2}\,dx
%	= \infty.
%\end{align*}
%so $\expe(1/X^2)$ is infinite (which is acceptable because $1/X^2$ is non-negative).
%\een
%\end{answer} 


%==========================================================================
\question

Let $X$ be a random variable with the following CDF:
\[
F(x) = \begin{cases}
	0					& \text{for}\quad x\leq 1 \\
	1 - 1/x^{2}	& \text{for}\quad x\geq 1
\end{cases}
\]
Compute $\expe(X)$, $\expe(X^2)$, $\expe(1/X)$ and $\expe(1/X^2)$.
\begin{answer}
\[
f(x) = \begin{cases}
	\frac{2}{x^3}	& \text{if } x\geq 1, \\
	0 				& \text{otherwisee.}
\end{cases}
\]
\begin{align*}
\expe(X)	
	& = \int_{1}^{\infty}x\left(\frac{2}{x^3}\right)\,dx 
		= 2\int_{1}^{\infty}\frac{1}{x^2}\,dx
		= 2\left[-\frac{1}{x}\right]_{1}^{\infty}
		= 2 \\
\expe(X^2)
	& = \int_{1}^{\infty}x^2\left(\frac{2}{x^3}\right)\,dx 
		= 2\int_{1}^{\infty}\frac{1}{x}\,dx
		= \infty \\	
\expe\left(\frac{1}{X}\right)	
	& = \int_{1}^{\infty}\frac{1}{x}\left(\frac{2}{x^3}\right)\,dx 
		= 2\int_{1}^{\infty}\frac{1}{x^4}\,dx
		= 2\left[-\frac{1}{3x^3}\right]_{1}^{\infty}
		= \frac{2}{3} \\
\expe\left(\frac{1}{X^2}\right)	
	& = \int_{1}^{\infty}\frac{1}{x^2}\left(\frac{2}{x^3}\right)\,dx 
		= 2\int_{1}^{\infty}\frac{1}{x^5}\,dx
		= 2\left[-\frac{1}{4x^4}\right]_{1}^{\infty}
		= \frac{1}{2} \\
\end{align*}
\end{answer} 


%==========================================================================
\question

Let $X$ be a continuous random variable with the following PDF:
\[
f(x) = \begin{cases}
	1-|x|	& \text{ if } x\in[-1,1] \\
	0		& \text{ otherwise.}
\end{cases}
\]
Find the range of integer values $\alpha\in\Z$ for which $\expe(X^{\alpha})$ exists.

\begin{answer}
For $\alpha>0$,
\[
\expe(X^{\alpha}) = \int_{-1}^{0}x^{\alpha}(1+x)\,dx + \int_{0}^{1}x^{\alpha}(1-x)\,dx < \infty
\]
Let $\alpha < 0$. If $\alpha$ is even then $X^\alpha$ is non-negative, so
\begin{align*}
\expe(X^\alpha)
	& = \expe\big((X^{+})^{\alpha}\big) = +\infty\\
\intertext{If $\alpha$ is odd,}
\expe(X^\alpha)
	& = \expe\big((X^{+})^{\alpha}\big) - \expe\big((X^{-})^{\alpha}\big) 
		= \infty - \infty 
\end{align*}
so in this case the moment $\expe(X^\alpha)$ does not exist.
\end{answer} 


%%==========================================================================
%\question
%Let $X$ be a discrete random variable, defined on the set of non-zero integers by the PMF
%\[
%\prob(X=k) = \frac{c}{k^4} \qquad\text{for}\quad k=\pm 1,\pm 2,\ldots
%\]
%where $c = 45/\pi^4$ is a normalizing constant. 
%\ben
%\it For what values of $\alpha\in\Z$ do the moments $\expe(X^{\alpha})$ exist?  
%\it Compute the variance of $X$ and the first negative moment of $X$.
%\een
%
%\begin{answer}
%\ben
%\it % (i)
%\bit
%\it If $\alpha\leq 2$ then $\expe(X^\alpha)$ exists and is finite because 
%\[
%\expe(X^\alpha) 
%	= c\sum_{k\neq 0}\frac{k^\alpha}{k^4} 
%	= 2c\sum_{k=1}^{\infty}\frac{1}{k^{4-\alpha}}
%	< \infty.
%\]
%\it If $\alpha>2$ and $\alpha$ is even, $\expe(X^\alpha)$ is infinite because
%\[
%\expe(X^\alpha) 
%	= c\sum_{k\neq 0}\frac{k^\alpha}{k^4} 
%	= c\sum_{k=1}^{\infty}\frac{1}{k^{4-\alpha}} 
%	= \infty.
%\]
%If $\alpha>2$ and $\alpha$ is odd, $\expe(X^\alpha)$ does not exist because
%\[
%\expe(X^\alpha) 
%	= c\sum_{k\neq 0}\frac{k^\alpha}{k^4} 
%	= c\sum_{n=1}^{\infty}\frac{n^{\alpha}}{n^4} - c\sum_{n=1}^{\infty}\frac{n^{\alpha}}{n^4} 
%	= \infty - +\infty.
%\]
%\eit
%\it % (b)
%The first two moments of $X$ are	
%\begin{align*}
%\expe(X) 
%	& = c\sum_{n\neq 0}\frac{1}{n^3} 
%	  = c\sum_{n=1}^{\infty}\frac{1}{n^3} - c\sum_{n=1}^{\infty}\frac{1}{n^3} = 0 \\
%\expe(X^2)
%	& = c\sum_{n\neq 0}\frac{1}{n^2} 
%	  = 2c\sum_{n=1}^{\infty}\frac{1}{n^2}
%	  = 2\left(\frac{45}{\pi^4}\right)\left(\frac{\pi^2}{6}\right) = \frac{15}{\pi^2}\approx 1.52
%\intertext{so $\var(X)=15/\pi^2$. Similarly,}
%\expe\left(\frac{1}{X}\right) 
%	& = c\sum_{n\neq 0}^{\infty}\frac{1}{n^5} = c\sum_{n=1}^{\infty}\frac{1}{n^5} - c\sum_{n=1}^{\infty}\frac{1}{n^5} = 0
%\end{align*}
%In fact, all negative odd-integer moments are zero.
%\een
%\end{answer}


%----------------------------------------
\end{questions}
\end{exercise}
%----------------------------------------------------------------------

%======================================================================
\endinput
%======================================================================
