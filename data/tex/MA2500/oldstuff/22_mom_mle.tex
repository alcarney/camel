% !TEX root = main.tex
%----------------------------------------------------------------------
\chapter{Method of Moments and Maximum Likelihood Estimators}\label{chap:mom-mle}
\setcounter{page}{1}
\startcontents[chapters]
%----------------------------------------------------------------------
\dictum{X}{X}{X}
\chapcontents

%----------------------------------------------------------------------
\section{The method of moments estimator (MME)}
%----------------------------------------------------------------------
Let $X$ be a random variable and let $F(x;\theta)$ denote its CDF.

\bit
\it $F$ belongs to a parametric class of distributions $\{F(x;\theta): \theta\in\Theta\}$.
\it $\theta$ is an unknown parameter (or vector of parameters).
\eit

\vspace{2ex}
Let $\expe(X;\theta)$ denote the expected value of $X$:
\[
\expe(X;\theta) = \int x\,dF(x,\theta)
\]

\vspace{2ex}
Let $f(x;\theta)$ denote the PMF (discrete case) or PDF (continuous case) of $X$.
\[\begin{array}{lll}
\expe(X;\theta) & = \displaystyle\sum_x x f(x;\theta) \qquad	& \text{(discrete case),} \\[3ex]
\expe(X;\theta) & = \displaystyle\int x f(x;\theta)\,dx 				& \text{(continuous case)}.
\end{array}\]



\ben
\it % single parameter
To estimate a scalar parameter $\theta$ using the \emph{method of moments}, we equate the first theoretical moment with the first empirical moment (i.e.\ the sample mean),
\[
\expe(X;\theta) = \frac{1}{n}\sum_{i=1}^n X_i = \bar{X},
\]
then solve this with respect to $\theta$.
\it % two parameters
To estimate two parameters $\mathbf{\theta} = (\theta_1,\theta_2)$, we equate the first two theoretical moments with the first two empirical moments,
\[
\expe(X;\mathbf{\theta}) = \frac{1}{n}\sum_{i=1}^n X_i
\qquad\text{and}\qquad
\expe(X^2;\mathbf{\theta}) = \frac{1}{n}\sum_{i=1}^n X_i^2,
\]
then solve this system of two equations with respect to $\theta_1$ and $\theta_2$.
\it % k parameters
To estimate $k$ parameters $\mathbf{\theta} = (\theta_1,\theta_2,\ldots,\theta_k)$, we equate the first $k$ theoretical moments with the first $k$ empirical moments, then solve the resulting system of equations with respect to $\theta_1,\theta_2,\ldots,\theta_k$. (The number of equations must be equal to the number of unknown parameters we wish to estimate.)
\een

% remark
\begin{remark}
In fact, we can equate the theoretical and empirical expected values of \emph{any} functions of $X$:
\begin{align*}
\expe\big[g_1(X);\mathbf{\theta}\big] & = \frac{1}{n}\sum_{i=1}^n g_1(X_i), \\
\expe\big[g_2(X);\mathbf{\theta}\big] & = \frac{1}{n}\sum_{i=1}^n g_2(X_i), \quad\text{etc.}
\end{align*}
\end{remark}

% example: bernoulli
\begin{example}
Let $X_1,X_2,\ldots,X_n$ be a random sample from the $\text{Bernoulli}(\theta)$ distribution. Estimate $\theta$ using the method of moments.
\end{example}
\begin{solution}
The first moment of the $\text{Bernoulli}(\theta)$ distribution is $\theta$. 
Equating this to the first empirical moment (i.e.\ the sample mean), we obtain the MME
\[
\hat{\theta} = \bar{X}.
\]
The MME of $\theta$ is the proportion of successes in $n$ trials.
\end{solution}



% example: normal
\begin{example}
Let $X_1,X_2,\ldots,X_n$ be a random sample from the $N(\mu,\sigma^2)$ distribution. Estimate $\mu$ and $\sigma^2$ using the method of moments.
\end{example}

\begin{solution}
Let $X\sim N(\mu,\sigma^2)$. Equating the first and second moments,
\begin{align*}
\expe(X;\mu,\sigma^2)	& = \Xbar, \\
\expe(X^2;\mu,\sigma^2)	& = \frac{1}{n}\sum_{i=1}^n X_i^2.
\end{align*}
The theoretical moments are $\expe(X;\mu,\sigma^2) = \mu$ and $\expe(X^2;\mu,\sigma^2) = \sigma^2 + \mu^2$, so we obtain the following MMEs of $\mu$ and $\sigma^2$:
\begin{align*}
\hat{\mu}			& = \Xbar, \\
\hat{\sigma}^2	& = \frac{1}{n}\sum_{i=1}^n X_i^2 - \bar{X}^2 = \frac{1}{n}\sum_{i=1}^n (X_i-\bar{X})^2.
\end{align*}
\bit
\it The MME of $\mu$ is the \emph{sample mean}.
\it The MME of $\sigma^2$ is the \emph{empirical mean squared deviation from the sample mean}.
\eit
\end{solution}



% example: uniform
\begin{example}
Let $X_1,X_2,\ldots,X_n$ be a random sample from the $\text{Uniform}(0,\theta)$ distribution. Estimate $\theta$ using the method of moments.
\end{example}
\begin{solution}
The PDF of the $\text{Uniform}(0,\theta)$ distribution is
\[
f(x) = \left\{\begin{array}{ll}
%	\theta^{-1}	& 0\leq x\leq \theta, \\
	\displaystyle\frac{1}{\theta}	& \text{for } 0 < x < \theta, \\[2ex]
	0				& \text{otherwise}.
\end{array}\right.
\]
The expected value (first theoretical moment) is 
\[
\expe(X;\theta) = \frac{1}{\theta}\int_{0}^{\theta}x\,dx = \frac{1}{2}\theta. 
\]
Equating this to the sample mean (first empirical moment) $\bar{X}$, we obtain the MME
\[
\hat{\theta}= 2\bar{X}.
\] 
\end{solution}

%==================================================================================================
\section{The maximum likelihood estimator (MLE)} 
%==================================================================================================
Let $X$ be a random variable and let 
\bit
\it $F(x;\theta)$ be the CDF of $X$ and 
\it $f(x;\theta)$ be the PMF or PDF of $X$, 
\eit
where $\theta$ is an unknown parameter (or vector of parameters).

\vspace{2ex}
\bit
\it Let $\boldX = (X_1,X_2,\ldots,X_n)$ be a random sample from the distribution of $X$. 
\it Let $\boldx = (x_1,x_2,\ldots,x_n)$ be a realization of the random sample.
\eit

By independence, the joint CDF and PMF/PDF of the random sample is 
\[
F_{\boldX}(\boldx;\theta) = \prod_{i=1}^n F(x_i,\theta)
\qquad\text{and}\qquad
f_{\boldX}(\boldx;\theta) = \prod_{i=1}^n f(x_i,\theta).
\]

\bit
\it So far, we have considered these as functions of $\boldx$, (with $\theta$ fixed).
\it We now wish to consider them as functions of $\theta$ (with $\boldx$ fixed).
\eit

%==========================================================================

\subsection{The likelihood function}
%==========================================================================

% defn: likelihood function
\begin{definition}
The \emph{likelihood function} of a random sample $\boldX = (X_1,X_2,\ldots,X_n)$ is defined to be
\[
L(\theta) = L(\theta;\mathbf{x}) = \prod_{i=1}^n f(x_i;\theta).
\]
where $\boldx=(x_1,x_2,\ldots,x_n)$ is a realization of the sample.
\end{definition}

% defn: MME
\begin{definition}
A \emph{maximum likelihood estimator} (MLE) of $\theta$ is obtained by holding the data fixed, and maximizing the likelihood function over all possible values of $\theta$:
\[
\hat{\theta}(\mathbf{x}) 
	= \arg\max_{\theta} L(\theta;\mathbf{x}) 
	= \arg\max_{\theta} \prod_{i=1}^n f(x_i;\theta)
\]
\end{definition}

% remark
\begin{remark}
\bit
%\it 
\it
$L(\theta)$ is the probability that the parameter value is $\theta$, given that $\mathbf{x}$ is observed.
\it
The MLE is the value of $\theta$ for which $F(x;\theta)$ is \emph{most likely} to have generated the data.
 \eit
\end{remark}

%-----------------------------
\subsection{The log-likelihood function}
%-----------------------------

To find the value of $\theta$ that maximises $L(\theta)$, our first line of attack would be to differentiate $L(\theta)$ with respect to $\theta$, then set the resulting expression to zero and solve for $\theta$.

\bit
\it However, differentiating products such as $\prod_{i=1}^n f(x_i;\theta)$ is not always straightforward.
\it Instead, we usually work with the \emph{log-likelihood} function.
\eit

\begin{definition}
The \emph{log-likelihood function} of a random sample is
\[
\ell(\theta) = \log L(\theta) = \sum_{i=1}^n \log f(x_i;\theta)
\]
\end{definition}

\bit
\it
Because log is a strictly increasing function, the value of $\theta$ that maximizes $L(\theta)$ coincides with the value of $\theta$ that maximizes $\ell(\theta)$. 
\it
The maximum likelihood estimate of $\theta=(\theta_1,\theta_2,\ldots,\theta_k)$ can therefore be obtained by solving the equations,
\[
\frac{\partial\ell}{\partial\theta_1}=0,
\quad
\frac{\partial\ell}{\partial\theta_2}=0,
\quad\ldots\quad,
\frac{\partial\ell}{\partial\theta_k}=0.
\]
\eit

%% remark: computation
%\begin{remark}
%Using the log-likelihood also avoids the need to multiply lots of small numbers, which can lead to problems with computation.
%\end{remark}

 % <<

% example: MLE for binomial parameter
\begin{example}\label{ex:mlebinomial}
Consider the set of binomial distributions, $\mathcal{C} = \{\text{Binomial}(n,\theta):\theta\in[0,1]\}$. 
If we observe $k$ successes in $n$ trials, what is the maximum likelihood estimate of $\theta$?
\end{example}
\begin{solution}
Let $X$ be the number of successes (a random sample of size $1$). The realization of the sample is $k$, the number of successes observed. 

\bit
\it
Likelihood function:\quad
$\displaystyle L(\theta) = L(\theta; k) = f(k;\theta) = \binom{n}{k}\theta^k(1-\theta)^{n-k}$.
\it 
Log-likelihood function:\quad
$\displaystyle\ell(\theta) = \log \binom{n}{k} + k\log\theta + (n-k)\log(1-\theta)$.
\it
Derivative of $\ell(\theta)$ with respect to $\theta$:\quad
$\displaystyle\ell\,'(\theta)= \frac{k}{\theta} - \frac{n-k}{1-\theta}$.
\it
Setting this equal to zero yields:\quad $\displaystyle\theta = \frac{k}{n}$.
\it
Second derivative of $\ell(\theta)$ with respect to $\theta$:\quad $\displaystyle\ell\,''(\theta) = -\frac{k}{\theta^2} - \frac{n-k}{(1-\theta)^2}< 0.$
\it
This indicates a maximum, so the MLE is $\displaystyle\hat{\theta} = \frac{k}{n}$.
\eit
%Hence the MLE is the observed number of successes, expressed as a proportion of the total number of trials.
\end{solution}

 % <<

% example: MLE for normal
\begin{example}\label{ex:mlenormal}
Let $X_1,X_2,\ldots,X_n$ be a random sample from the $N(\mu,\sigma^2)$ distribution, where $\mu$ and $\sigma^2$ are both unknown. Find the maximum likelihood estimates of $\mu$ and $\sigma^2$.
\end{example}

\begin{solution}
Let $\theta=(\mu,\sigma)$. The common PDF of the $X_i$ is
\[
f(x;\mu,\sigma) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right).
\]

For a sequence of observations $\boldx=(x_1,x_2,\ldots,x_n)$, the likelihood function is \[
\displaystyle L(\mu,\sigma) 
	= \prod_{i=1}^n f(x_i;\mu,\sigma).
%	= \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x_i-\mu)^2}{2\sigma^2}\right).
\]

The log-likelihood function is therefore
\begin{align*}
\ell(\mu,\sigma) = \sum_{i=1}^n \log f(x_i;\mu,\sigma)
	& = \sum_{i=1}^n \left( -\log(\sqrt{2\pi}) - \frac{1}{2}\log(\sigma^2) - \frac{(x_i-\mu)^2}{2\sigma^2}\right) \\
	& = -n\log(\sqrt{2\pi}) - \frac{n}{2}\log(\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^n (x_i-\mu)^2
\end{align*}



\[
\ell(\mu,\sigma) = -n\log(\sqrt{2\pi}) - \frac{n}{2}\log(\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^n (x_i-\mu)^2
\]

The partial derivatives of $\ell(\mu,\sigma)$ with respect to $\mu$ and $\sigma$ are, respectively,
\[
\frac{\partial\ell}{\partial\mu} = \frac{1}{\sigma^2}\sum_{i=1}^n(x_i-\mu)
\qquad\text{and}\qquad
\frac{\partial\ell}{\partial\sigma} = -\frac{n}{\sigma} + \frac{1}{\sigma^3}\sum_{i=1}^n(x_i-\mu)^2.
\]

Setting these to equal zero then solving for $\mu$ and $\sigma$, we obtain
\[
\hat{\mu} = \frac{1}{n}\sum_{i=1}^n X_i = \Xbar
\qquad\text{and}\qquad
\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n (X_i-\Xbar)^2.
\]

\bit
\it The MLE of $\mu$ is the \emph{sample mean}.
\it The MLE of $\sigma^2$ is the \emph{empirical mean squared deviation from the sample mean}.
\eit
\end{solution}

% example: MLE for uniform
\begin{example}\label{ex:mleuniform}
Let $X_1,X_2,\ldots,X_n$ be a random sample from the $\text{Uniform}(0,\theta)$ distribution. Find the maximum likelihood estimate of $\theta$.
\end{example}

\begin{solution}
The PDF of the $\text{Uniform}(0,\theta)$ distribution is
\[
f(x) = \left\{\begin{array}{ll}
	\displaystyle\frac{1}{\theta}	& \text{for } 0 < x < \theta, \\[2ex]
	0				& \text{otherwise}.
\end{array}\right.
\]
The parameter space is $\Theta = \{\theta:\theta>0\}$, and the likelihood function is
\[
L(\theta;\mathbf{x}) = \left\{\begin{array}{ll}
	\displaystyle\frac{1}{\theta^n}	& \text{for } \theta > \max\{x_1,x_2,\ldots,x_n\} \\[2ex]
	0				& \text{otherwise}.
\end{array}\right.
\]
\bit
\it $L(\theta)$ is a decreasing function of $\theta$ for all $\theta>\max\{x_1,x_2,\ldots,x_n\}$.
\it Hence the MLE of $\theta$ is given by $\hat{\theta} = \max\{X_1,X_2,\ldots,X_n\}$.
\eit
\end{solution}
%======================================================================
\stopcontents[chapters]
\endinput
%======================================================================
