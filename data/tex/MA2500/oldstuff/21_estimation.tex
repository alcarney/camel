% !TEX root = main.tex
%----------------------------------------------------------------------
\chapter{Point Estimation}\label{chap:point-estimation}
\setcounter{page}{1}
\startcontents[chapters]
%----------------------------------------------------------------------
\dictum{X}{X}{X}
\chapcontents

%----------------------------------------------------------------------
\section{Statistics}
%----------------------------------------------------------------------
\begin{center}
\fbox{\strut\ Statistics deals with extracting information from data. (Wikipedia)\ }
\end{center}

\vspace{2ex}
A typical problem is to estimate the distribution of a random variable from a set of observations. 

\vspace{2ex}
First we look at \emph{parametric statistics}, in which
\bit
\it the parametric form of the distribution is known, but 
\it the actual values of the parameters are unknown.
\eit

\vspace{2ex}
For example, certain observations may be known (or assumed) to have 
\bit 
\it an exponential distribution, and we want to estimate the rate parameter ($\lambda$), or
\it a normal distribution, and we wish to estimate the mean ($\mu$) and variance ($\sigma^2$),
\it and so on.
\eit

\vspace{2ex}
Later we will look at \emph{non-parametric} statistics.%, where even the parametric form of the distribution is unknown.


%----------------------------------------------------------------------

\section{Random vectors}
%----------------------------------------------------------------------
\ben
% probability spaces
\it A \emph{probability space} is defined by a triple $(\Omega,\mathcal{F},\prob)$, where
\bit
\it $\Omega$ is the set of possible outcomes,
\it $\mathcal{F}$ is a $\sigma$-field over $\Omega$, 
\it $\prob:\mathcal{F}\to[0,1]$ is a probability measure on $(\Omega,\mathcal{F})$.
%\[\begin{array}{cccc}
%\prob: 	& \mathcal{F} 	& \to & [0,1] \\
%		& A				& \mapsto	& \prob(A)
%\end{array}\]
\eit
% random variables
\it A \emph{random variable} on $(\Omega,\mathcal{F})$ is a function $X:\Omega\to\R$ with the property that
\bit
\it $\{X\in B\}=\{\omega: X(\omega) \in B\}\in\mathcal{F}$ for every $B\in\mathcal{B}(\R)$, or equivalently
%\eit
%where $\mathcal{B}(\R)$ is the \emph{Borel $\sigma$-field over $\R$}, 
%or equivalently that
%\bit
\it $\{X\leq x\}=\{\omega: X(\omega) \leq x\}\in\mathcal{F}$ for every $x\in\R$.
\eit
% probability distribution
\it The \emph{probability distribution} of $X$ is the probability measure
\[\begin{array}{rccl}
\prob_X:	& \mathcal{B}(\R) 	& \to 		& [0,1] \\
			& B					& \mapsto	& \prob(X\in B).
\end{array}\]
This induces a probability space $\big(\R,\mathcal{B}(\R),\prob_X\big)$ on $\R$.

% cumulative distribution function
\it The probability distribution of $X$ is uniquely defined by the values it takes on events of the form $\{X\leq x\}$. Consequently, $X$ is completely described by its \emph{cumulative distribution function} (CDF),
\[\begin{array}{rccl}
F_X:	& \R 	& \to 		& [0,1] \\
		& x		& \mapsto	& \prob(X\leq x).
\end{array}\]

% random vectors
\it A \emph{random vector} on $(\Omega,\mathcal{F})$ is a vector-valued function 
\[\begin{array}{rccl}
\boldX:	& \Omega 	& \to 		& \R^n \\
		& \omega	& \mapsto	& \big(X_1(\omega),X_2(\omega),\ldots,X_n(\omega)\big)^T.
\end{array}\]
with the property that
\bit
\it $\{\boldX\in B\}=\{\omega: \boldX(\omega) \in B\}\in\mathcal{F}$ for every $B\in\mathcal{B}(\R^n)$, or equivalently
\it $\{\boldX\leq \boldx\}=\{\omega: \boldX(\omega) \leq \boldx\}\in\mathcal{F}$ for every $\boldx\in\R^n$.
\eit
Note that $\{\boldX\leq \boldx\}$ is short-hand notation for the event
\begin{align*}
\{\boldX\leq \boldx\} 
	& = \{X_1\leq x_1, X_2\leq x_2, \ldots, X_n\leq x_n\} \\
	& = \{\omega: X_1(\omega)\leq x_1, X_2(\omega)\leq x_2, \ldots, X_n(\omega)\leq x_n\}.
\end{align*}

% probability distribution
\it The \emph{joint probability distribution} of $\boldX$ is the probability measure
\[\begin{array}{rccl}
\prob_\boldX:	& \mathcal{B}(\R^n) & \to 		& [0,1] \\
			& B						& \mapsto	& \prob(\boldX\in B).
\end{array}\]
This induces a probability space $\big(\R^n,\mathcal{B}(\R^n),\prob_\boldX\big)$ on $\R^n$.

% cumulative distribution function
\it As in the univariate case, $\boldX$ is completely described by its \emph{joint CDF},
\[\begin{array}{rccl}
F_\boldX:	& \R^n 		& \to 		& [0,1] \\
			& \boldx	& \mapsto	& \prob(\boldX\leq \boldx).
\end{array}\]

% independence
\it If the component variables $X_1,X_2,\ldots,X_n$ are independent and identically distributed,
\begin{align*}
F_{\boldX}(\boldx) = \prob(\boldX\leq \boldx)
	& = \prob(X_1\leq x_1, X_2\leq x_2, \ldots, X_n\leq x_n) \\
	& = \prob(X_1\leq x_1)\prob(X_2\leq x_2)\ldots\prob(X_n\leq x_n) \\
	& = \textstyle\prod_{i=1}^n F_X(x_i)
\end{align*}
where $F_X$ is the common CDF of the component variables $X_i$.
\een

%----------------------------------------------------------------------

\section{Random samples}
%----------------------------------------------------------------------
Let $\boldX = (X_1,\ldots,X_n)$ be a random vector over the probability space $(\Omega,\mathcal{F},\prob)$:
\[\begin{array}{rccl}
\boldX:	& \Omega 	& \to 		& \R^n \\
		& \omega	& \mapsto	& \big(X_1(\omega),X_2(\omega),\ldots,X_n(\omega)\big)^T.
\end{array}\]

\begin{definition}
\ben
\it 
If the component variables $X_i$ are independent and identically distributed, we say that $\boldX = (X_1,\ldots,X_n)$ is a \emph{random sample of size $n$} from their common distribution.
%\it 
%If $X_1$ takes the value $x_1$, $X_2$ takes the value $x_2$ and so on, the vector $\boldx = (x_1,x_2\ldots,x_n)\in\R^n$ is called a \emph{realisation} of the random sample.
%If $X_i$ takes the value $x_i$ ($i=1,2,\ldots,n$), the vector $\boldx = (x_1,x_2\ldots,x_n)\in\R^n$ is called a \emph{realisation} of the random sample.
\it A \emph{realisation} of $\boldX$ is a vector $\boldx = (x_1,x_2\ldots,x_n)\in\R^n$, where $x_i$ is the value taken by the corresponding component variable $X_i$.
\it
The set of all possible realisations is called the \emph{sample space} of $\boldX$.
\it 
A function $T=T(\boldX)$ of the random sample is called a \emph{statistic}.
\een
\end{definition}



\begin{remark}
Let $T:\R^n\to\R$ be any (measurable) function.
\bit
\it 
$T(\boldX)$ is a random variable on $(\Omega,\mathcal{F},\prob)$:
\[\begin{array}{rccl}
T(\boldX):	& \Omega 	& \to 		& \R \\
			& \omega	& \mapsto	& T\big(\boldX(\omega)\big)
\end{array}\]
The distribution of $T$ is completely determined by the probability measure $\prob$.

\it 
$T$ is a random variable on $\big(\R^n,\mathcal{B}(\R^n),\prob_{\boldX}\big)$:
\[\begin{array}{rccl}
T:	& \R^n 		& \to 		& \R \\
	& \boldx	& \mapsto	& T(\boldx)
\end{array}\]
The distribution of $T$ is completely determined by the probability measure $\prob_{\boldX}$, and hence by the joint CDF $F_{\boldX}$ of the random sample.
\par
By the assumption that the $X_i$ are independent and identically distributed, it follows that the distribution of $T$ is completely determined by the common CDF of the individual observations $X_i$. 
\eit
\end{remark}

%----------------------------------------------------------------------

\section{Estimators}
%----------------------------------------------------------------------
Let $\boldX = (X_1,\ldots,X_n)$ be a random sample, and let $F$ denote the common CDF of the $X_i$.

\vspace{2ex}
\bit
\it
We assume that $F$ belongs to some parametric collection of distribution functions, 
\[
\mathcal{C} = \{F_{\mathbf{\theta}} : \mathbf{\theta}\in\Theta\}
\]
where $\mathbf{\theta}$ is a vector of parameters, and $\Theta$ is the set of all such vectors.
\it $\mathcal{C}$ is called a \emph{statistical model}.
\it $\Theta$ is called the \emph{parameter space}.
\eit

% example
\begin{example}
\bit
\it A coin has probability $\theta$ of showing heads:
\[
%\mathcal{C} = \left\{F_{\theta}(x):0\leq\theta\leq 1\right\}.
\mathcal{C} = \big\{\text{Bernoulli}(\theta):0\leq\theta\leq 1\big\}.
\]
%\it A die has probability $\theta_i$ of landing on its $i$th face:
%\[
%\mathcal{C} = \left\{F_{\mathbf{\theta}}(x) : 0\leq\theta_i\leq 1\text{ with }\sum_{i=1}^6\theta_i=1\right\}.
%\]
\it Atmospheric pressure has normal distribution with mean $\theta_1$ and variance $\theta_2$:
\[
\mathcal{C} = \big\{N(\theta_1,\theta_2) : \theta_1\in\R,\,\theta_2\in\R^{+}\big\}.
\]

\eit
\end{example}



We would like to estimate $\mathbf{\theta}$ using a random sample of observations $\boldX = (X_1,X_2,\ldots,X_n)$.

% definition
\begin{definition}
\bit
\it Computing a particular value for $\theta$ is called \emph{point estimation}.
\it Computing a range of values for $\theta$ is called \emph{interval estimation}.  
\it Asserting whether or not $\theta$ lies in some range is called \emph{hypothesis testing}.
\eit
\end{definition}


To estimate the true value of a scalar parameter $\theta$, we need to find a real-valued function 
\[\begin{array}{rccl}
\hat{\theta}:	& \R^n 		& \to 		& \R \\
				& \boldx	& \mapsto	& \hat{\theta}(\boldx)
\end{array}\]
such that $\hat{\theta}(\boldx)$ is ``\textsl{close}'' to the true value $\theta$ for ``\emph{most}'' sample realisations $\boldx$.

% definition
\begin{definition}
\bit
\it $\theta$ is called the \emph{estimand} (or \emph{true value}).
\it The statistic $\hat{\theta}=\hat{\theta}(\boldX)$ is called an \emph{estimator} of $\theta$.
\it For any particular sample realisation, $\hat{\theta}(\boldx)$ is called an \emph{estimate} of $\theta$.
\eit
\end{definition}



% example
\begin{example}
A coin has an unknown probability $\theta$ of falling on heads. Let $\boldX = (X_1,X_2,\ldots,X_n)$ represent a random sample of independent coin tosses, where
\[
X_i = \begin{cases} 1 & \text{ if a head is observed,} \\ 0 & \text{ if a tail is observed.}\end{cases}
\]
\bit
\it The sample space is the set of binary vectors of length $n$.
\it An estimator of the true probability $\theta$ is provided by the sample mean:
%\[\begin{array}{rccl}
%\hat{\theta}:	& \{0,1\}^n 		& \to 		& \R \\
%				& \boldx	& \mapsto	& \displaystyle\frac{1}{n}\sum_{i=0}^n X_i.
%\end{array}\]
\[
\hat{\theta}(\boldX) = \frac{1}{n}\sum_{i=0}^n X_i.
\]
%\it $\hat{\theta}(\boldX)$ is a random variable on $(\Omega,\mathcal{F},\prob)$:
%\it $\hat{\theta}$ is a random variable on $\big(\R^n,\mathcal{B}(\R^n),\prob_{\boldX}\big)$.
\it $\hat{\theta}$ is a random variable on the sample space.
\it The distribution of $\hat{\theta}$ is completely determined by the distribution of $\mathbf{X}$.
\it By the law of large numbers, $\hat{\theta}\to\theta$ in probability as $n\to\infty$.
\it By the central limit theorem, if the sample is sufficiently large,
\[
\hat{\theta} \sim N\left(\theta,\frac{\theta(1-\theta)}{n}\right) \quad\text{approx.}
\]
\eit
\end{example}


%======================================================================
\stopcontents[chapters]
\endinput
%======================================================================
