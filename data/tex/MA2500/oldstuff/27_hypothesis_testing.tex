% !TEX root = main.tex
%----------------------------------------------------------------------
\chapter{Hypothesis Testing}\label{chap:hypothesis-testing}
\setcounter{page}{1}
\startcontents[chapters]
%----------------------------------------------------------------------
\dictum{X}{X}{X}
\chapcontents

%----------------------------------------------------------------------
\section{Null hypothesis significance testing}
%----------------------------------------------------------------------
Let $X$ be a random variable and let $F_{\theta}$ denote its CDF, where $\theta\in\Theta$ is an unknown parameter.
\bit
\it $\{F_\theta:\theta\in\Theta\}$ is a \emph{statistical model}; $\Theta$ is the \emph{parameter space}.
\eit

\vspace*{1ex}
\bit
\it To estimate a particular value for $\theta$ is called \emph{point estimation}.
\it To assert whether or not $\theta$ lies in some set is called \emph{hypothesis testing}.
\eit

\vspace*{1ex}
Let $\Theta_0$ and $\Theta_1$ be subsets of the parameter space.
\bit
\it We wish to test the hypothesis that $\theta\in\Theta_0$ against the alternative hypothesis $\theta\in\Theta_1$.
\eit

%\begin{tabbing}
%\qquad\qquad \= The Null Hypothesis:	\qquad\qquad		\= $H_0: \theta\in\Theta_0$, \\
%		\> The Alternative Hypothesis:		\> $H_1: \theta\in\Theta_1$.
%\end{tabbing}
%
\vspace*{1ex}

\fbox{\begin{minipage}{\linewidth}%\centering
\begin{tabbing}
The Null Hypothesis:	\qquad\qquad		\= $H_0: \theta\in\Theta_0$, \\
The Alternative Hypothesis:		\> $H_1: \theta\in\Theta_1$.
\end{tabbing}
In the absence of any evidence to the contrary, we assume that $H_0$ is correct.
\end{minipage}}

\vspace*{1ex}
\bit
\it We make a decision about whether to reject $H_0$ in favour of $H_1$ based on a random sample of observations from the distribution of $X$.
\it Randomness inevitably means that the decision could be wrong.
\eit

 % << 

% simple example
\begin{example}[revision]
Suppose $X\sim N(\mu,\sigma^2)$ where the mean $\mu$ is unknown, but the variance $\sigma^2$ is known. To test the null hypothesis $H_0:\mu=\mu_0$ against the alternative $H_1:\mu\neq\mu_0$, we obtain a random sample $X_1,X_2,\ldots,X_n$ from the distribution of $X$, and compute the sample mean estimator of $\mu$,
\[
\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i
\]
Under the null hypothesis, we know that
\[
\bar{X}_n \sim N\left(\mu_0, \frac{\sigma^2}{n}\right),
\text{\quad which implies that \quad}
Z = \sqrt{n}\left(\frac{\bar{X}_n-\mu_0}{\sigma}\right) \sim N(0,1).
\]

\bit
\it $Z$ is the test statistic. If $H_0$ is correct, we know that $Z\sim N(0,1)$.
\eit

Let $z$ be the observed value of the test statistic: $z=\displaystyle\frac{1}{\sqrt{n}}\sum_{i=1}^n \left(\frac{x_i-\mu_0}{\sigma}\right)$.

\bit
\it If $z\approx 0$, we would decide to retain the null hypothesis $H_0:\mu=\mu_0$.
\it If $z\not\approx 0$, we might decide to reject $H_0$ in favour of the alternative $H_1:\mu\neq\mu_0$.
\eit

\end{example}

%----------------------------------------------------------------------
\subsection{Simple and composite hypotheses}
%----------------------------------------------------------------------

% defn: simple & composite hypotheses
\begin{definition}
\ben
\it A hypothesis that specifies a particular value is called a \emph{simple hypothesis}.
\it A hypothesis that specifies a subset of values is called a \emph{composite hypothesis}.
\een
\end{definition}

For example, 
\bit
\it $H_0:\theta = \theta_0$  is a simple hypothesis,
\it $H_1:\theta\neq\theta_0$ is a composite hypothesis.
\eit

\vspace*{2ex}
In what follows, we focus mainly on testing simple null hypotheses such as $H_0:\theta=\theta_0$ against either simple alternatives such as $H_1:\theta=\theta_1$, or composite alternatives such as $H_1:\theta\in\Theta_1$.


%----------------------------------------------------------------------

\section{Type I and Type II errors}
%----------------------------------------------------------------------
Suppose we wish to test the null hypothesis $H_0:\theta\in\Theta_0$ against the alternative $H_1:\theta\in\Theta_1$.

\bit
\it A \emph{Type I error} is when we decide that $\theta\in\Theta_1$ when $\theta\in\Theta_0$ is correct.
\it A \emph{Type II error} is when we decide that $\theta\in\Theta_0$ when $\theta\in\Theta_1$ is correct.
\eit

\vspace*{2ex}
A hypothesis test can be respresented by a \emph{decision table}:

\vspace*{2ex}
\begin{minipage}{\linewidth}\centering
\begin{tabular}{|c|c|c|} \hline
				& \multicolumn{2}{c|}{Reality} \\ 
Decision		& $H_0$ true			& $H_1$ true \\ \hline
Retain $H_0$	& Correct decision 	& Type II error \\ 
Reject $H_0$	& Type I error 		& Correct decision \\ \hline
\end{tabular}
\end{minipage}

\vspace*{2ex}
\bit
\it Type I error: $H_0$ is rejected when $H_0$ is true.
\it Type II error: $H_0$ is retained when $H_0$ is false.
\eit

%----------------------------------------

\subsection{Target detection}
%----------------------------------------
Hypothesis testing is used in \emph{target detection} systems. 

\vspace*{2ex}
A decision table for such applications might be as follows:

\vspace*{2ex}
\begin{minipage}{\linewidth}\centering
\begin{tabular}{|c|c|c|c|} \hline
				& \multicolumn{2}{c|}{Reality} & \\
Decision		& 	Target Absent	& Target Present & Action\\  \hline
Target Absent	&	Correct		& Miss 			& Stay silent \\
Target Present&	False alarm 	& Hit 				& Sound alarm \\  \hline
\end{tabular}
\end{minipage}

\vspace*{2ex}
\bit
\it The null hypothesis is the \emph{absence} of a target.
\it Rejecting the null hypothesis means asserting the \emph{presence} of a target.
\eit

%----------------------------------------

\subsection{Medical research}
%----------------------------------------
Hypothesis testing is used in medicine to detect the presence of a disease. 

\vspace*{2ex}
A decision table for medical applications might be as follows:

\vspace*{2ex}
\begin{minipage}{\linewidth}\centering
\begin{tabular}{|c|c|c|c|} \hline
					& \multicolumn{2}{c|}{Reality} &  \\
Decision			& Disease Absent	& Disease Present		& Action \\  \hline
Disease Absent	& True negative	& False negative		& Do nothing  \\
Disease Present 	& False positive 	& True positive 		& Prescribe antibiotics \\  \hline
\end{tabular}
\end{minipage}

\vspace*{2ex}
\bit
\it False positives are Type I errors
\it False negatives are Type II errors
\eit


%----------------------------------------------------------------------

\section{Sample spaces}
%----------------------------------------------------------------------
%\bit
%\it Let $X$ be a random variable, and let $F_{\theta}$ denote its CDF where $\theta\in\Theta$ is an unknown parameter.
%\it Let \eit

% defn: sample space
\begin{definition}
Let $\mathbf{X}=(X_1,\ldots,X_n)$ be a random sample. The \emph{sample space} is the set of all possible realisations of the sample. This will be denoted by $D\subseteq\R^n$.
\end{definition}

Examples:
\[
\begin{array}{ll}
X_i\sim\text{Bernoulli}(p):					& \qquad D = \{0,1\}^n, \\
X_i\sim\text{Poisson}(\lambda):			& \qquad D = \{x\in\mathbb{Z}: x\geq 0\}^n, \\
X_i\sim\text{Exponential}(\lambda):		& \qquad D = \{x\in\R:x\geq 0\}^n, \\
X_i\sim\text{Normal}(\mu,\sigma^2):		& \qquad D = \R^n.
\end{array}
\]

%----------------------------------------------------------------------

\subsection{The probability measure induced by $F_{\theta}$}
%----------------------------------------------------------------------

% probability measure induced by F on R
Let $X$ be a random variable, and let $F:\R\to[0,1]$ denote its CDF. Recall that there exists a unique probability measure $\prob_F:\mathcal{B}(\R)\to[0,1]$, called the \emph{probability measure induced by $F$ on $\R$}, with the property that
\[
\prob_F\big((a,b]\big) = F(b) - F(a)% \text{\quad for every interval $(a,b]\subset\R$.}
\]
for every interval of the form $(a,b]\subset\R$.

\vspace*{1ex}
\bit
%\it This is called the \emph{probability measure induced by $F$} on the Borel sets $\mathcal{B}(\R)$.
\it This naturally extends to a probability measure on $\R^n$.
\eit
\vspace*{1ex}

Let $X_1,X_2,\ldots,X_n$ be a random sample from the distribution of $X$. Then there exists a unique probability measure $\prob_F:\mathcal{B}(\R^n)\to[0,1]$, called the \emph{probability measure induced by $F$ on $\R^n$}, with the property that
\[
\prob_F(A) = \textstyle\prod_{i=1}^n \big[F(b_i)-F(a_i)\big]
\]
for every set of the form $A=\{(x_1,x_2,\ldots,x_n)\in\R^n:a_i<x_i\leq b_i,\ i=1,2,\ldots,n\}$.


%Let $X_1,X_2,\ldots,X_n$ be a random sample from the distribution of $X$. By independence,
%\begin{align*}
%\prob_F(A) 
%	& = \prob(a_1 \leq X_1 \leq b_1, a_2 \leq X_2 \leq b_2, \ldots, a_n < X_n \leq b_n) \\
%	& = \prob(a_1 \leq X_1 \leq b_1)\prob(a_2 \leq X_2 \leq b_2)\cdots\prob(a_n < X_n \leq b_n) \\
%%	& = \prod_{i=1}^n \prob(a_i \leq X_i \leq b_i) \\
%	& = \textstyle\prod_{i=1}^n \big[F(b_i)-F(a_i)\big].
%\end{align*}
%\bit
%\it This measure can be extended to sets formed by countable unions, intersections and complements of hyper-rectangles.
%\it These are the the Borel sets in $\R^n$, denoted by $\mathcal{B}(\R^n)$.
%\it The measure is called the \emph{probability measure induced by $F$} on $\mathcal{B}(\R^n)$.
%\eit
%
%This leads to the following definition:
\begin{definition}
Let $\prob_{\theta}$ denote the probability measure induced by $F_\theta$ on the sample space $D\subseteq \R^n$.
\end{definition}

%----------------------------------------------------------------------
\section{Critical regions}
%----------------------------------------------------------------------

% defn: critical region
\begin{definition}
Let $X\sim F_{\theta}$ where $\theta$ is an unknown parameter, let $X_1,X_2,\ldots,X_n$ be a random sample from the distribution of $X$, and consider the null hypothesis $H_0:\theta\in\Theta_0$ and the alternative $H_1:\theta\in\Theta_1$.
\ben
\it A \emph{hypothesis test}, of the null hypothesis $H_0$ against the alternative hypothesis $H_1$, is specified by a subset of the sample space, called the \emph{critical region}.
\it The \emph{decision rule} corresponding to the critical region $C\subseteq D$ is
\begin{align*}
\text{Reject $H_0$ if}\quad & (X_1,X_2,\ldots,X_n)\in C, \\
\text{Retain $H_0$ if}\quad & (X_1,X_2,\ldots,X_n)\in D\setminus C.
\end{align*}
\een
\end{definition}


%----------------------------------------------------------------------

\subsection{The size (or significance level) of a test}
%----------------------------------------------------------------------
We would like to choose a critical region $C$ that minimises the probability of both Type I and Type II errors. These are conflicting objectives:

\vspace*{2ex}
Extreme cases:
\bit
\it If $C=\emptyset$ we \emph{never} reject $H_0$, in which case $\prob(\text{Type I error})=0$ and $\prob(\text{Type II error})=1$.
\it If $C=D$ we \emph{always} reject $H_0$, in which case $\prob(\text{Type I error})=1$ and $\prob(\text{Type II error})=0$.
\eit

\vspace*{2ex}
% remark: status quo
\begin{remark}[Conservatie testing]
The null hypothesis represents the \textit{status quo}. From a conservative standpoint, rejecting the status quo incorrectly (a Type I error) is worse than retaining the status quo incorrectly (a Type II error). As a result, null hypothesis significance tests usually proceed as follows:
\ben
\it fine a set of critical regions for which $\prob(\text{Type I error})$ is less than or equal to some maximum value, then
\it choose one of these critical regions so that $\prob(\text{Type II error})$ is mimimized as far as possible.
\een
\end{remark}

 % <<

% remark: test statistics
\begin{remark}[Test statistics]
Critical regions are often specified in terms of a \emph{test statistic}, say $T:D\to\R$. For example
\[
C = \{\mathbf{x}\in D : T(\mathbf{x}) \leq c\}.
\]
The critical region $C$ depends on the \emph{critical value} $c$, and on the distribution of $T(\mathbf{X})$.
\end{remark}

% defn: size
\begin{definition}
The \emph{size} of a critical region is the maximum probability of making a Type I error, and denoted by
\[
\alpha = \max_{\theta\in\Theta_0} \prob_{\theta}(\mathbf{X}\in C).
\]
The size of a critical region is also called the \emph{significance level} of the corresponding test.
\end{definition}

\begin{remark}
\ben
\it $\alpha$ is the \emph{maximum} probability of making a Type I error, taken over all $\theta\in\Theta_0$.
\it If $H_0$ is a simple hypothesis, say $H_0:\theta=\theta_0$, then
\[
\alpha = \prob_{\theta_0}(\mathbf{X}\in C).
\]
This is the probability that the random sample falls into the critical region when $H_0$ is correct. This leads us to reject $H_0$ incorrectly
\een
\end{remark}

%==========================================================================

\subsection{Observed significance levels ($p$-values)}
%==========================================================================

Let $T:D\to\R$ be a test statistic for evaluating the null hypothesis $H_0$ against some alternative hypothesis $H_1$. A hypothesis test is specified in terms of a critical region, say
\[
C = \{\mathbf{x} : T(\mathbf{x}) \leq c\}.
\]

\bit
\it Let $\mathbf{x}$ be a realization of the sample.
\it Let $c_{\text{obs}} = T(\mathbf{x})$ be the observed value of the test statistic.
\eit

For this realization of the sample, the \emph{observed significance level} or \emph{$p$-value} is
\[
p = \max_{\theta\in\Theta_0} \prob_{\theta}\big[ T(\mathbf{X})\leq c_{\text{obs}}\big].
\]

If $H_0$ is a simple hypothesis, e.g.\ $H_0:\theta=\theta_0$, this simplifies to
\[
p = \prob_{\theta_0}\big[ T(\mathbf{X})\leq c_{\text{obs}}\big].
\]


\begin{remark}
For a test of size $\alpha$, let $c$ be the corresponding critical value for the test statistic:
\[
\prob_{\theta_0}\big[ T(\mathbf{X})\leq c \big] = \alpha,
\]
If we observe that $c_{\text{obs}}\leq c$, we reject $H_0$ in favour of $H_1$.
\end{remark}


%==========================================================================
\section{Statistical power} 
%==========================================================================
Among all critical regions of size $\alpha$, we want to chose one where the probability of making a Type II error is as small as possible.

\ben
\it A Type II error occurs when we incorrectly retain the null hypothesis $H_0:\theta\in\Theta_0$.
\it If the alternative hypothesis $H_1:\theta\in\Theta_1$ is correct, we want to maximize $\prob_{\theta}(\mathbf{X}\in C)$
%\it The probability of Type II error is often denoted by $\beta$:
%\[
%\beta(\theta) = 1 - \prob_{\theta}(\mathbf{X}\in C)
%\]
%\it For every $\theta\in\Theta_1$, we want to maximize $1-\beta$:
%\[
%1 - \prob_{\theta}(\text{Type II error}) = \prob_{\theta}(\mathbf{X}\in C)
%\]
\een

% defn: statistical power
\begin{definition}
Let $C$ be a critical region for testing $H_0:\theta\in\Theta_0$ against $H_1:\theta\in\Theta_1$. The \emph{power function} of the test is
\[
\gamma(\theta) = \prob_{\theta}(\mathbf{X}\in C), \text{\quad defined for all $\theta\in\Theta_1$}.
\]
\end{definition}

%%-----------------------------
%% example: statistical power
%\begin{example}[Statistical Power]
%Let $X_1,X_2,\ldots,X_n$ be an i.i.d. random sample from the $N(\mu,\sigma^2)$ distribution with standard deviation $\sigma=120$ and unknown mean $\mu$. Suppose we want to test the null hypothesis $H_0:\mu=1000$ against the alternative $H_1:\mu>1000$, with $\alpha=0.05$. Compute the power of the test to detect the simple alternative hypothesis $H_1:\mu=1020$ from a sample of size $n=36$.
%\end{example}
%
%\begin{solution}
%Let $\mu_0=1000$ and $\mu_1=1020$. 
%A suitable test statistic is the sample mean $\bar{X}=\frac{1}{n}\sum_{i=1}^n X_i$, which under $H_0$ has distribution $\Nor(\mu_0,\sigma^2/n)$, i.e.\
%\[
%\bar{X} \sim \Nor(1000, 400)
%\]
%The value of $\Xbar$ separating the critical and non-critical regions is
%\begin{align*}
%\Xbar_0 	
%	& = \mu_0 + z_{1-\alpha}\left(\frac{\sigma}{\sqrt{n}}\right) \\
%	& = 1000 + 1.645\times 20\\
%	& = 1032.9
%\end{align*}
%The $z$-value corresponding to this point under the alternative hypothesis $H_1:\mu=1020$ is
%\[
%z = \frac{\bar{X}_0-\mu_1}{\sigma/\sqrt{n}} = \frac{1032.9-1020}{20} = 0.6458 
%\]
%Using tables we find that $\prob(Z > z) = 0.2578$. This is power of the test for detecting $H_1:\mu=1020$ against $H_0:\mu=1000$ with a sample of size $n=36$.
%\end{solution}


% defn: power function
\bit
\it The value $\gamma(\theta)$ is called the \emph{power of the test at $\theta$}.
\it If $\Theta_1$ is a composite hypothesis, we can compute the power at each parameter value $\theta\in\Theta_1$.
\eit
%\begin{definition}
% The \emph{power function} of a critical region $C\subset D$ is defined by
% \[
%\gamma(\theta) = \gamma_C(\theta) = \prob_{\theta}(\mathbf{X}\in C) \qquad\text{for}\quad \theta\in\Theta_1.
%\]
%\end{definition}

\vspace*{2ex}
\bit
\it Suppose we have two critical regions $C_1$ and $C_2$, each of size $\alpha$. 
\it If $\gamma_{C_1}(\theta) \geq \gamma_{C_2}(\theta)$ for all $\theta\in\Theta_1$ then $C_1$ provides a better test than $C_2$.
\eit



%==========================================================================
\section{Example}
%==========================================================================
\begin{example}
Let $X_1,X_2,\ldots,X_8$ be a random sample from the $\text{Poisson}(\theta)$ distribution, where $\theta>0$ is unknown. We reject the simple null hypothesis $H_0:\theta=0.5$ in favour of the alternative $H_1:\theta>0.5$ whenever the observed sum satisfies $\sum_{i=1}^8 X_i \geq 8$.
\ben
\it Compute the significance level of the test.
\it Compute the power of the test at $\theta = 0.75$, $\theta = 1.0$ and $\theta = 1.25$.
\een
\end{example}

\textbf{Hint}: Use the fact that if $U\sim\text{Poisson}(\theta_1)$ and $V\sim\text{Poisson}(\theta_2)$, then $U+V\sim\text{Poisson}(\theta_1+\theta_2)$.

\begin{solution}
The critical region of the test is 
\[
C = \big\{\mathbf{x} | S(\mathbf{x}) \geq 8\big\}
\text{\quad where\quad}
S(\mathbf{X}) = \sum_{i=1}^8 X_i
\]
By the hint, if $X_i\sim\text{Poisson}(\theta)$, then $S\sim\text{Poisson}(8\theta)$.
\ben
\it % << (i)
Under $H_0:\theta=0.5$, we have $S\sim\text{Poisson}(4)$ so
\begin{align*}
%\alpha	& = \prob_{H_0}\big(S\geq 8\big) \\
\alpha	& = \prob_{H_0}\big(S\geq 8\big) \\
		& = \prob\big(S\geq 8\text{ when } S\sim\text{Poisson}(4)\big) \\
		& = 1 - \prob\big(S\leq 7\text{ when } S\sim\text{Poisson}(4)\big)	\\
		& = 1 - 0.9489 = 0.0511 \quad\text{(from tables).}
\end{align*}
\it % << (ii)
Under $H_1:\theta=0.75$, we have $S\sim\text{Poisson}(6)$ so
\begin{align*}
\gamma(0.75)	& = \prob_{H_1}\big(S\geq 8) \\
			& = \prob\big(S\geq 8\text{ when }S\sim\text{Poisson}(6)\big) \\
			& = 1 - \prob\big(S\leq 7\text{ when }S\sim\text{Poisson}(6)\big) \\
			& = 1 - 0.7440 = 0.2560 \quad\text{(from tables).}\\ 
\end{align*}
Under $H_1:\theta=1$, we have $S\sim\text{Poisson}(8)$ so
\begin{align*}
\gamma(1)	& = \prob_{H_1}\big(S\geq 8) \\
			& = \prob\big(S\geq 8\text{ when }S\sim\text{Poisson}(8)\big) \\
			& = 1 - \prob\big(S\leq 7\text{ when } S\sim\text{Poisson}(8)\big) \\
			& = 1 - 0.4530 = 0.4570 \quad\text{(from tables).}\\ 
\end{align*}
Under $H_1:\theta=1.25$, we have $S\sim\text{Poisson}(10)$ so
\begin{align*}
\gamma(1.25)	& = \prob_{H_1}\big(S\geq 8\big) \\
			& = \prob\big(S\geq 8\text{ when } S\sim\text{Poisson}(10)\big) \\
			& = 1 - \prob\big(S\leq 7\text{ when } S\sim\text{Poisson}(10)\big) \\
			& = 1 - 0.2202 = 0.7798 \quad\text{(from tables).}\\ 
\end{align*}
Notice that the power of the test to detect the alternative hypothesis \emph{increases} as $\theta$ moves away from the value $\theta_0=0.5$ specified by the null hypothesis.
\een

\end{solution}

%======================================================================
\stopcontents[chapters]
\endinput
%======================================================================
