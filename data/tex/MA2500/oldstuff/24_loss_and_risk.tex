% !TEX root = main.tex
%----------------------------------------------------------------------
\chapter{Loss and Risk}\label{chap:loss-and-risk}
\setcounter{page}{1}
\startcontents[chapters]
%----------------------------------------------------------------------
\dictum{X}{X}{X}
\chapcontents

%----------------------------------------------------------------------
\section{Variance}
%----------------------------------------------------------------------
%Intuitively, a good estimator is one that is \emph{close} to the true value for \emph{most} sample realisations.
%
\bit
\it Let $X=(X_1,X_2,\ldots,X_n)$ be a random sample where each $X_i$ has density $f(x;\theta)$.
\it Let $\hat{\theta}(X) = \hat{\theta}(X_1,X_2,\ldots,X_n)$ be an estimator of $\theta$.
%\it Let $T(X) = T(X_1,X_2,\ldots,X_n)$ be an estimator of $\theta$.
\eit


Recall that $\hat{\theta}(X)$ is called \emph{unbiased} if
\[
\bias(\hat{\theta},\theta) = \expe_\theta\big[\hat{\theta}(X)-\theta\big] = 0 \text{\quad for all values of $\theta\in\Theta$.}
\]

The errors $\hat{\theta}(X)-\theta$ can be quantified by the \emph{variance} of $\hat{\theta}$.
% defn: variance and standard error
\begin{definition}
\ben
\it The \emph{variance} of an estimator is simply 
$\displaystyle \var(\hat{\theta}) = \expe_\theta\big([\hat{\theta}-\expe(\hat{\theta})]^2\big)$.
\it The standard deviation of an estimator is called its \emph{standard error}.
%$\displaystyle \text{se}(\hat{\theta}) = \sqrt{\var(\hat{\theta})}$.
\een
%\ben
%\it The \emph{variance} of an estimator is simply 
%$\displaystyle \var_\theta\big[T(X)\big] = \expe_\theta\big(\big[\,T(X)-\expe[T(X)]\,\big]^2\Big)$.
%\it The \emph{variance} of an estimator is simply 
%$\displaystyle \var_\theta\big[T(X)\big] = \expe_\theta\big[\big(T(X)-\expe[T(X)]\,\big)^2\Big]$.
%\it The standard deviation of an estimator is called its \emph{standard error}.
%%$\displaystyle \text{se}(\hat{\theta}) = \sqrt{\var(\hat{\theta})}$.
%\een
\end{definition}

% example
\begin{example}
Let $X_1,X_2,\ldots,X_n$ be a random sample from a distribution with mean $\mu$ and variance $\sigma^2$. The variance and standard error of the sample mean $\bar{X}$ are 
%$\displaystyle\bar{X}=\frac{1}{n}\sum_{i=1}^n X_i$ are
\[
\var(\bar{X}) = \frac{\sigma^2}{n} \text{\quad and\quad} \text{StdErr}(\bar{X}) = \frac{\sigma}{\sqrt{n}} \text{\quad respectively.}
\]
\end{example}

%----------------------------------------------------------------------

\section{Mean squared error}
%----------------------------------------------------------------------
\bit
\it If an estimator is \emph{unbiased}, its accuracy can be quantified by its variance.
\it If an estimator is \emph{biased}, we should also take the bias into account.
\eit

% definition
\begin{definition}
The \emph{mean squared error} of an estimator is the expected squared difference between the estimator and the true parameter:
\[
\mse(\hat{\theta},\theta) = \expe\big[(\hat{\theta}-\theta)^2\big].
\]
\end{definition}

% theorem
\begin{theorem}
$\mse(\hat{\theta},\theta) = \var(\hat{\theta}) + \bias(\hat{\theta},\theta)^2$.
\end{theorem}
%\small
\begin{proof}
\vspace*{-2ex}
\begin{align*}
\mse(\hat{\theta},\theta) = \expe\big[(\hat{\theta}-\theta)^2\big]
	& = \expe\big[(\hat{\theta}-\expe\hat{\theta}    +  \expe\hat{\theta}-\theta)^2\big] \\
	& = \expe\big[(\hat{\theta}-\expe\hat{\theta})^2 + 2(\hat{\theta}-\expe\hat{\theta})(\expe\hat{\theta}-\theta)+ (\expe\hat{\theta}-\theta)^2 \big] \\
	& = \expe\big[(\hat{\theta}-\expe\hat{\theta})^2\big] + 2\expe(\hat{\theta}-\expe\hat{\theta})(\expe\hat{\theta}-\theta)+ (\expe\hat{\theta} - \theta)^2  \\
%	& = \expe\big[(\hat{\theta}-\expe\hat{\theta})^2\big] + \big[\expe\hat{\theta} - \theta\big]^2 \\
	& = \var(\hat{\theta}) + \bias(\hat{\theta},\theta)^2.				
\end{align*}
\vspace*{-3ex}
\end{proof}

%\normalsize
%
%% corollary
%\begin{corollary}
%If $\hat{\theta}$ is unbiased, $\mse(\hat{\theta}) = \var(\hat{\theta})$.
%\end{corollary}

%----------------------------------------------------------------------

\section{Loss and risk}
%----------------------------------------------------------------------

% loss function
More generally, to quantify estimation accuracy we define a \emph{loss function} $\mathcal{L}(\hat{\theta},\theta)\geq 0$, which has the property that 
\[
\mathcal{L}\big[\hat{\theta}(X),\theta\big]=0 \text{\quad if and only if\quad} \hat{\theta}(X)=\theta.
\]

Many different loss functions are possible. For example,
\[\begin{array}{lll}
\mathcal{L}(\hat{\theta},\theta)	& =\ (\hat{\theta}-\theta)^2		& \text{The quadratic loss function.} \\ 
\mathcal{L}(\hat{\theta},\theta)	& =\ |\hat{\theta}-\theta|			& \text{The absolute error loss function.} \\ 
\mathcal{L}(\hat{\theta},\theta)	& =\ I(\hat{\theta}\neq\theta)		& \text{The zero-one loss function.} \\ 
\mathcal{L}(\hat{\theta},\theta)	& =\ I(|\hat{\theta}-\theta|>c)\qquad\mbox{}	& \text{The large deviation loss function.} 
\end{array}\]

% risk function
For a given loss function $\mathcal{L}$, an estimator $\hat{\theta}$ can be evaluated using the associated \emph{risk function},
\[
\mathcal{R}(\hat{\theta},\theta) = \expe_{\theta}\big[\mathcal{L}(\hat{\theta},\theta)\big].
\]

\vspace*{2ex}
Risk is the \emph{expected loss}, taken over the distribution of $\hat{\theta}$.

% example: mean squared error
\begin{example}
For the quadratic loss function $\mathcal{L}(\hat{\theta},\theta) = (\hat{\theta}-\theta)^2$, the risk function is the mean squared error:
\[
\mathcal{R}(\hat{\theta},\theta) 
	= \expe\big[(\hat{\theta}-\theta)^2\big]
	= \var(\hat{\theta}) + \bias(\hat{\theta},\theta)^2.
\]
\end{example}

It is possible to reduce the overall risk by finding a biased estimator with small variance.
% example
\begin{example}
%Let $\psi(\theta) = \expe_\theta\big[\hat{\theta}(X)\big]$, where $\hat{\theta}$ is a biased estimator for $\theta$, so that $\psi(\theta)\neq \theta$ for some or all of $\theta\in\Theta$.
%
%Denote the error variable by $S=T-\theta$. Then $\expe_\theta(S) = \psi(\theta)-\theta$.
%
%Then $\mathcal{R}
Let $X_1,X_2,\ldots,X_n$ be a normally distributied $N(\mu,\sigma^2)$ random sample. 

The maximum likelihood estimate of $\theta$ is the sample mean $\bar{X}$. The associated risk is
\[
R(\bar{X},\mu)  = \expe\big[(\bar{X}-\mu)^2\big] = \var(\bar{X}) = \frac{\sigma^2}{n}.
\]

Now consider the following biased estimator of $\mu$:
\[
T(X_1,X_2,\ldots,X_n) = \frac{X_1+X_2+\ldots+X_n}{n+1}.
\]

The associated risk function is
\begin{align*}
\mathcal{R}(T,\mu)
	= \expe\big[(T-\mu)^2\big] 
	& = \var(T-\mu) + \expe(T-\mu)^2 \\
	& = \var\left(\frac{X_1+X_2+\ldots+X_n}{n+1}\right) + \left(\frac{n}{n+1}\mu - \mu\right)^2 \\
	& = \frac{n\sigma^2}{(n+1)^2} + \frac{\mu^2}{(n+1)^2}
\end{align*}

Comparing $\mathcal{R}(T,\mu)$ with $\mathcal{R}(\bar{X},\mu)$:
\begin{align*}
\mathcal{R}(T,\mu) - \mathcal{R}(\bar{X},\mu)
	& = \left(\frac{n\sigma^2}{(n+1)^2} + \frac{\mu^2}{(n+1)^2}\right) - \frac{\sigma^2}{n} \\
	& = \frac{\mu^2}{(n+1)^2} - \left(\frac{1}{n} - \frac{n}{(n+1)^2}\right)\sigma^2 \\
	& = \frac{1}{(n+1)^2}\left[\mu^2 - \left(\frac{2n+1}{n}\right)\sigma^2\right].
\end{align*}

This shows that 
\[
\mathcal{R}(T,\mu) < \mathcal{R}(\bar{X},\mu) \text{\quad whenever\quad} \mu^2 < \left(\frac{2n+1}{n}\right)\sigma^2,
\]
\vspace{-2ex}
even thoughh $T$ is a biased estimator of $\mu$.
\end{example}


%%----------------------------------------------------------------------
%
%\section{Minimax estimators}
%%----------------------------------------------------------------------
%
%% example: minimax estimator
%\begin{example}
%A \emph{minimax estimator} is one for which the maximum risk (worst case error) is minimum:
%\[
%\hat{\theta}_{\text{minimax}} = \arg \min_{\hat{\theta}}\left\{\max_{\theta\in\Theta} \mathcal{R}(\theta,\hat{\theta})\right\}.
%\]
%\end{example}
%
%======================================================================
\stopcontents[chapters]
\endinput
%======================================================================
