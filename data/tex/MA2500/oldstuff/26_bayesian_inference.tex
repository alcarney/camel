% !TEX root = main.tex
%----------------------------------------------------------------------
\chapter{Bayesian Inference}\label{chap:bayesian}
\setcounter{page}{1}
\startcontents[chapters]
%----------------------------------------------------------------------
\dictum{X}{X}{X}
\chapcontents

%----------------------------------------------------------------------
\section{The Bayesian approach}
%----------------------------------------------------------------------
\bit
\it Let $X=(X_1,X_2,\ldots,X_n)$ be a random sample of observations.
\eit

\vspace*{2ex}
Until now, we have assumed that an unknown parameter $\theta$ has a fixed value:
\bit
\it The joint PMF/PDF of the sample is written as $f(\mathbf{x};\theta)$.
\it The likelihood function of the sample is written as $L(\theta;\mathbf{x})$.
\it Point estimators such as the MME and MLE claim to estimate the `true' value of $\theta$
\eit

\vspace*{2ex}
In Bayesian inference, we think of an unknown parameter $\theta$ as a \emph{random variable}.
\bit
\it The joint PMF/PDF of the sample is written as $f(\mathbf{x}|\theta)$.
\it The likelihood function of the sample is written as $L(\theta|\mathbf{x})$.
\eit

\vspace*{2ex}
\textbf{Notation}
\bit
\it The PMF/PDF of $\mathbf{x}$ is denoted by $f(\mathbf{x})$.
\it The PMF/PDF of $\theta$ will be denoted by $\pi(\theta)$.
\eit
%----------------------------------------------------------------------

\subsection{The prior distribution}
%----------------------------------------------------------------------
% prior
Suppose we have some prior knowledge regarding the distribution of $\theta$. 
\bit
\it This is called the \emph{prior distribution} of $\theta$, and denoted by $\pi_0(\theta)$.
\eit

\vspace*{2ex}
An initial point estimate of $\theta$ can be computed from the prior distribution. For example,
\bit
\it The mean of the prior distribution: $\hat{\theta} = \expe\big[\pi_0(\theta)\big]$, or
\it The mode of the prior distribution: $\hat{\theta} = \arg\max_{\theta}\big[\pi_0(\theta)\big]$.
\eit

\begin{remark}
Without any prior information, we should initially consider every value of $\theta$ to be equally likely. 

\vspace*{2ex}
For example, if $\theta$ is continuous and all we know is that $\theta$ belongs to some interval $[a,b]$, we should adopt the \emph{uniform} prior distribution over $[a,b]$:
\[
\pi_0(\theta) = \left\{\begin{array}{ll}
	1/(b-a)	& \text{if } a\leq\theta\leq b, \\
	0	& \text{otherwise.}
\end{array}\right.
\]
This is often called the \emph{non-informative} or \emph{na\"{\i}ve} prior.
\end{remark}

%----------------------------------------------------------------------

\subsection{The posterior distribution}
%----------------------------------------------------------------------
% posterior
Suppose we now obtain some sample data $\mathbf{x}=(x_1,\ldots,x_n)$. 
\bit
\it Bayes' theorem can  be used to combine the prior distribution with the data.
\it We obtain an updated PMF/PDF $\pi_1(\theta)$, called the \emph{posterior distribution} of $\theta$.
\eit

By Bayes' theorem, 
\[
\pi_1(\theta|\mathbf{x}) = \left\{\begin{array}{ll}
	\displaystyle\frac{f(\mathbf{x}|\theta)\pi_0(\theta)}{\sum_{\theta}f(\mathbf{x}|\theta)\pi_0(\theta)} 			& \text{\quad ($\theta$ discrete)}, \\[3ex]
	\displaystyle\frac{f(\mathbf{x}|\theta)\pi_0(\theta)}{\int_{\theta}f(\mathbf{x}|\theta)\pi_0(\theta)\,d\theta} 	& \text{\quad ($\theta$ continuous)}.
\end{array}\right.
\]

\vspace*{2ex}
This can also be expressed in terms of likelihood functions,
\[
\pi_1(\theta|\mathbf{x}) = \left\{\begin{array}{ll}
	\displaystyle\frac{L(\theta|\mathbf{x})\pi_0(\theta)}{\sum_{\theta}L(\theta|\mathbf{x})\pi_0(\theta)} 			& \text{\quad ($\theta$ discrete)}, \\[3ex]
	\displaystyle\frac{L(\theta|\mathbf{x})\pi_0(\theta)}{\int_{\theta}L(\theta|\mathbf{x})\pi_0(\theta)\,d\theta} 	& \text{\quad ($\theta$ continuous)}.
\end{array}\right.
\]

 % <<

Having obtained the posterior distribution, we can compute an updated point estimate of $\theta$.

For example,
\bit
\it The mean of the posterior distribution: $\hat{\theta} = \expe\big[\pi_1(\theta)\big]$, or
\it The mode of the posterior distribution: $\hat{\theta} = \arg\max_{\theta}\big[\pi_1(\theta)\big]$.
\eit

% defn: MAP estimator
\begin{definition}
The mode of the posterior distribution is called the \emph{maximum a-posteriori} or \emph{MAP} estimator.
\end{definition}

% remark
\begin{remark}
The posterior distribution is
\[
\pi_1(\theta|\mathbf{x}) = 
\displaystyle\frac{L(\theta|\mathbf{x})\pi_0(\theta)}{\sum_{\theta}L(\theta|\mathbf{x})\pi_0(\theta)}
\text{\quad or\quad}
\pi_1(\theta|\mathbf{x}) =
\displaystyle\frac{L(\theta|\mathbf{x})\pi_0(\theta)}{\int_{\theta}L(\theta|\mathbf{x})\pi_0(\theta)\,d\theta}
\]
\bit
\it 
The MAP estimator is the value of $\theta$ that maximizes the numerator $L(\theta|\mathbf{x})\pi_0(\theta)$.
\it
When $\pi_0$ is the uniform distribution, the MAP estimator is just the MLE.
\it
To compute the mean of $\pi_1(\theta|\mathbf{x})$, we also need to compute the denominator.
\eit
\end{remark}

%----------------------------------------------------------------------

\section{The binomial model}
%----------------------------------------------------------------------
The \emph{beta function} is defined by
\[
B(\alpha,\beta) = \int_0^1 t^{\alpha-1}(1-t)^{\beta-1}\,dt \text{\quad for all $\alpha,\beta>0$.}
\]
The \emph{beta distribution} with (shape) parameters $\alpha,\beta>0$ is defined by its PDF:
\[
f(x;\alpha,\beta) = \left\{\begin{array}{ll}
	\displaystyle\frac{ x^{\alpha-1} (1-x)^{\beta-1} }{B(\alpha,\beta)} & \text{if $0\leq x\leq 1$}, \\
	0												& \text{otherwise.}
\end{array}\right.
\]
\vspace*{-1ex}
%--------------------
\begin{exercise}
For $X\sim\text{Beta}(\alpha,\beta)$, show that
\[
\expe(X) = \frac{\alpha}{\alpha+\beta} \text{\quad and\quad} \var(Y) = \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}. 
\]
Furthermore, if $\alpha,\beta > 1$ show that
\[
\text{Mode}(X) = \frac{\alpha-1}{\alpha+\beta-2}.
\]
\end{exercise}
%--------------------

 % << 

% example
\begin{example}
Consider the binomial model $\{\text{Binomial}(n,\theta):\theta\in[0,1]\}$. 
\ben
\it Suppose we conduct a sequence of $n$ independent trials, and observe $k$ successes. Find the MAP estimate of $\theta$.
\it Suppose we conduct a further sequence of $n$ independent trials, this time observing $k'$ successes. Find an updated MAP estimate of $\theta$.
\een
\end{example}

\begin{solution}
Let $f(x|\theta)$ be the PMF of the $\text{Binomial}(n,\theta)$ distribution:
\[
f(x|\theta) = \binom{n}{k}\theta^x(1-\theta)^{n-x}
\]

Initially we should consider every value of $\theta$ to be equally likely. Thus we adopt the uniform prior distribution for $\theta$.
\[
\pi_0(\theta) = \left\{\begin{array}{ll}
	1	& \text{if } \theta\in[0,1] \\
	0	& \text{otherwise.}
\end{array}\right.
\]

 % << 

\ben
\it % (i)
Given $k$ successes in $n$ trials, the likelihood function is
\[
L(\theta|k) = f(k|\theta) = \binom{n}{k}\theta^k(1-\theta)^{n-k}
\]

The posterior distribution combines the observation with the prior distribution:
\[
\pi_1(\theta) = \pi_1(\theta|k)	
	= \frac{L(\theta|k)\pi_0(\theta)}{\int L(\theta|k)\pi_0(\theta)\,d\theta}
	= \frac{\theta^k(1-\theta)^{n-k}}{\int_0^1\theta^k(1-\theta)^{n-k}\,d\theta}.
\]
\vspace*{2ex}
We recognise $\pi_1(\theta)$ as the PDF of the $\text{Beta}(\alpha,\beta)$ distribution, with parameters 
\[
\alpha=k+1 \text{\quad and\quad}\beta=n-k+1.
\]
\vspace*{-1ex}
\bit
\it The mode of $\pi_1(\theta)$ is $k/n$. This is the MAP estimator of $\theta$.
\it Note that this co-incides with the MLE of $\theta$.
\eit
\bit
\it The expected value of $\pi_1(\theta)$ is $(k+1)/(n+2)$.
\it This is approximately equal to the MAP estimator when $k$ and $n$ are both large.
\eit

 % << 

\it % (ii)
Given $k'$ successes in $n$ trials, the likelihood function is
\[
L(\theta|k') = f(k'|\theta) = \binom{n}{k'}\theta^{k'}(1-\theta)^{n-k'}.
\]

Using $\pi_1$ as a new prior distribution for $\theta$, the new posterior distribution $\pi_2$ is
\[
\pi_2(\theta) = 
\pi_2(\theta|k,k')	
	= \frac{L(\theta|k')\pi_1(\theta)}{\int_0^1 L(\theta|k')\pi_1(\theta)\,d\theta} 
	= \frac{\theta^{k+k'}(1-\theta)^{2n-(k+k')}}{\int_0^1\theta^{k+k'}(1-\theta)^{2n-(k+k')}\,d\theta}.
\]

\vspace*{1ex}
We recognise $\pi_2(\theta)$ as the PDF of the $\text{Beta}(\alpha,\beta)$ distribution, with parameters 
\[
\alpha=k+k'+1 \text{\quad and\quad}\beta=2n-(k+k')+1.
\]

Hence our updated MAP estimate of $\theta$ is
\[
\hat{\theta}_{MAP} = \frac{k+k'}{2n}.
\]

\vspace*{-1.5ex}
\bit
\it If $k'<k$, the mode shifts to the left (adjusted down).
\it If $k'>k$, the mode shifts to the right (adjusted up).
\eit
\een
\end{solution}

%----------------------------------------------------------------------

\section{The exponential model}
%----------------------------------------------------------------------
The \emph{gamma function} is defined by
\[
\Gamma(\alpha) = \int_0^{\infty} t^{\alpha-1}e^{-t}\,dt \text{\qquad for all $\alpha\in\R$.}
\]

The PDF of the \emph{gamma distribution} with shape parameter $\alpha>0$ and rate parameter $\beta>0$ is
\[
f(x;\alpha,\beta) = \left\{\begin{array}{ll}
	\displaystyle\frac{ \beta^{\alpha}}{\Gamma(\alpha)}\, x^{\alpha-1} e^{-\beta x} & \text{for $x>0$}, \\
	0												& \text{otherwise.}
\end{array}\right.
\]
Note that the exponential distribution is a special case of the gamma distribution ($\alpha=1$).

%--------------------
\begin{exercise}
For $X\sim\text{Gamma}(\alpha,\beta)$, show that
\[
\expe(X) = \frac{\alpha}{\beta} \text{\quad and\quad} \var(Y) = \frac{\alpha}{\beta^2},
\]
and if $\alpha > 1$ show that
\[
\text{Mode}(X) = \frac{\alpha-1}{\beta}.
\]
\end{exercise}
%--------------------

 % << 

% example: exponential
\begin{example}
Let $X\sim\text{Exponential}(\lambda)$ where $\lambda>0$ is a rate parameter. We adopt the $\text{Gamma}(\alpha,\beta)$ distribution as a prior distribution for $\lambda$, where $\alpha,\beta>0$ are fixed values (perhaps estimated in some preliminary experiments). Let $\mathbf{x}=(x_1,x_2,\ldots,x_n)$ be a random sample of observations from the distribution of $X$.
\ben
\it Find the mean and mode of the prior distribution.
\it Show that the posterior distribution of $\lambda$ is $\text{Gamma}(\alpha+n,\beta+\sum_{i=1}^n x_i)$.
\it Find the mean and mode of the posterior distribution.
\een
\end{example}

\begin{solution}
Let $f(x|\lambda)$ be the PDF of the $\text{Exponential}(\lambda)$ distribution:
\[
f(x|\lambda) = \left\{\begin{array}{ll}
	\lambda\exp(-\lambda x) & \text{for $x>0$}, \\
	0							& \text{otherwise.}
\end{array}\right.
\]

The PDF of the prior distribution is
\[
\pi_0(\lambda) = 	\frac{\beta^{\alpha}}{\Gamma(\alpha)}\,\lambda^{\alpha-1}\exp(-\beta\lambda).
\]
which has mean $\alpha/\beta$ and mode $(\alpha-1)/\beta$.

 % << 

The likelihood function is
\[
L(\lambda|\mathbf{x}) 
	= \prod_{i=1}^n f(x_i|\lambda)
	= \prod_{i=1}^n \lambda\exp(-\lambda x_i)
	= \lambda^n \exp\Big(-\lambda\textstyle\sum_{i=1}^n  x_i\Big).
\]

The PDF of the posterior distribution is
\[
\pi_1(\lambda|\mathbf{x})	
	= \frac{L(\lambda|\mathbf{x})\pi_0(\lambda)}{\displaystyle\int_0^{\infty} L(\lambda|\mathbf{x})\pi_0(\lambda)\,d\lambda}.
\]

The numerator is the product of the likelihood $L(\lambda|\mathbf{x})$ and the prior $\pi_0(\lambda)$:
\begin{align*}
L(\lambda|\mathbf{x})\pi_0(\lambda)
	& = \Big[\lambda^n \exp\big(-\lambda\textstyle\sum_{i=1}^n  x_i\big)\Big]\left[\displaystyle\frac{\beta^{\alpha}}{\Gamma(\alpha)}\lambda^{\alpha-1}\exp(-\beta\lambda)\right] \\[1ex]
	& = \frac{\beta^{\alpha}}{\Gamma(\alpha)} \lambda^{\alpha+n-1}\exp\Big[-\big(\beta+\textstyle\sum_{i=1}^n x_i\big)\lambda\Big]
\end{align*}

Notice that this resembles the PDF of the $\text{Gamma}\Big(\alpha+n,\beta+\sum_{i=1}^n x_i\Big)$ distribution.

 % << 

The PDF of the posterior distribution becomes
\[
\pi_1(\lambda|\mathbf{x})	
	= \frac{\lambda^{\alpha+n-1}\exp\Big[-\big(\beta+\sum_{i=1}^n x_i\big)\lambda\Big]}
		{\displaystyle\int_0^{\infty} \lambda^{\alpha+n-1}\exp\Big[-\big(\beta+\sum_{i=1}^n x_i\big)\lambda\Big]\,d\lambda}
\]

To compute the denominator, change the variable of integration to $t = -\big(\beta+\sum_{i=1}^n x_i\big)\lambda$. This yields
\[
\int_0^{\infty} \lambda^{\alpha+n-1}\exp\Big[-\big(\beta+\textstyle\sum_{i=1}^n x_i\big)\lambda\Big]\,d\lambda 
	= \displaystyle\frac{\Gamma(\alpha+n)}{\big(\beta+\sum_{i=1}^n x_i\big)^{\alpha+n}}
\]

Thus the PDF of the posterior distribution is 
\[
\pi_1(\lambda|\mathbf{x})	
	= \frac{\big(\beta+\sum_{i=1}^n x_i\big)^{\alpha+n}}{\Gamma(\alpha+n)}
	\lambda^{\alpha+n-1}\exp\Big[-\big(\beta+\sum_{i=1}^n x_i\big)\lambda\Big]
\]
which is the PDF of the $\text{Gamma}\Big(\alpha+n,\beta+\sum_{i=1}^n x_i\Big)$ distribution.

 % << 

The mean and mode of $\lambda\sim\text{Gamma}\Big(\alpha+n,\beta+\sum_{i=1}^n x_i\Big)$ are
\[
\expe(\lambda) = \frac{\alpha+n}{\beta+\sum_{i=1}^n x_i}
\text{\quad and\quad}
\text{Mode}(\lambda) = \frac{\alpha+n-1}{\beta+\sum_{i=1}^n x_i}
\text{\quad respectively.}
\]
Hence the MAP estimator of $\lambda$ is
\[
\hat{\lambda}_{MAP} 
	\ =\ \frac{\alpha+n-1}{\beta+\sum_{i=1}^n x_i}
	\ =\ \frac{1 + \left(\frac{\alpha-1}{n}\right)}{\frac{1}{n}\sum_{i=1}^n x_i + \left(\frac{\beta}{n}\right)}.
\]

\bit
\it When $n=0$, this is simply the mode of the prior distribution, $\text{Gamma}(\alpha,\beta)$.
\it As $n$ increases, the influence of the prior distribution decreases.
\it Indeed, it can be seen that $\hat{\lambda}_{MAP}=\bar{X}^{-1}$ as $n\to\infty$.
\it This is the method-of-moments estimator (MME) of $\lambda$, which is based entirely on the data and takes no account of the prior distribution.
\eit
\end{solution}

%----------------------------------------------------------------------

\section{The scientific method}
%----------------------------------------------------------------------

%% example
%\begin{example}\label{ex:biscuits}
%We have three tins of biscuits. The first tin contains $30$ chocolate and $10$ plain biscuits, the second tin contains $20$ chocolate and $20$ plain biscuits, and the third tin contains $10$ chocolate and $30$ plain biscuits. A tin is chosen at random, and a biscuit is chosen at random from the tin.
%\ben
%\it If a chocolate biscuit is observed, estimate which tin was chosen.
%\een
%The experiment is repeated, but this time two biscuits are chosen at random from the tin.
%\ben\stepcounter{enumi}
%\it If two chocolate biscuits are observed, estimate which tin was chosen.
%\it If one chocolate biscuit and one plain biscuit are observed, estimate which tin was chosen.
%\een
%\end{example}
%
%\begin{solution}
%\ben
%\it % one chocolate
%Let $\Theta = \{\theta_1,\theta_2,\theta_3\}$ where the value $\theta_k$ indicates that tin $k$ was chosen ($k=1,2,3$). Before observing the biscuit, it is reasonable to suppose that each tin is equally likely to be chosen. Thus we adopt the \emph{uniform} prior distribution for $\theta$:
%\[
%\pi_0(\theta) = \frac{1}{3} \quad\text{for all}\quad \theta\in\Theta.
%\]
%Let $A$ be the event that a chocolate biscuit was observed. Then
%\[
%\prob(A|\theta_1)=3/4,\qquad \prob(A|\theta_2)=1/2,\qquad \prob(A|\theta_3)=1/4,
%\]
%or equivalently
%\[
%L(\theta_1| A)=3/4,\qquad L(\theta_2| A)=1/2,\qquad L(\theta_3| A)=1/4.
%\]
%By Bayes' theorem, the posterior distribution is
%\[
%\pi_1(\theta_k|A) = \frac{L(\theta_k| A)\pi_0(\theta_k)}{\sum_{k=1}^3 L(\theta_k| A)\pi_0(\theta_k)}
%\text{\quad\qquad ($k=1,2,3$).}
%\]
%For the first tin,
%\[
%\prob(\theta_1|A) = \frac{3/4\times 1/3}{(3/4\times 1/2) + (1/2\times 1/3) + (1/4\times 1/3)} = \frac{1}{2}.
%\]
%Similar calculations for the second and third tins yield the following posterior:
%\[
%\pi_1(\theta_1) = 1/2,\qquad \pi_1(\theta_2) = 1/3, \qquad \pi_1(\theta_3) = 1/6.
%\]
%The MAP estimate is $\hat{\theta}_{MAP} = \theta_1$, so our best guess is that the first tin was chosen.
%
% % <<
%
%\it % two chocolate
%Let $B$ be the event that two chocolate biscuits are observed, 
%\[
%\prob(B|\theta_1)=87/156,\qquad \prob(B|\theta_2)=38/156,\qquad \prob(B|\theta_3)=9/156.
%\]
%If we assume the uniform prior $\pi_0$, we obtain the posterior distribution
%\[
%\pi_1(\theta_1|B) = 87/134,\qquad \pi_1(\theta_2|B) = 38/134, \qquad \pi_1(\theta_3|B) = 9/134.
%\]
%The MAP estimator again suggests that the first tin was chosen.
%
%\it % one chocolate, one plain
%Let $C$ be the event that one chocolate and one plain biscuit are observed, 
%\[
%\prob(C|\theta_1)=60/156,\qquad \prob(C|\theta_2)=80/156,\qquad \prob(C|\theta_3)=60/156.
%\]
%If we assume the uniform prior $\pi_0$,
%\[
%\pi_1(\theta_1|C) = 3/10,\qquad \pi_1(\theta_2|C) = 4/10, \qquad \pi_1(\theta_3|C) = 3/10.
%\]
%This time, the MAP estimator leads us to assert that the second tin was chosen.
%\een
%\end{solution}


% example
\begin{example}\label{ex:biscuits}
We have three tins of biscuits. The first tin contains $30$ chocolate and $10$ plain biscuits, the second tin contains $20$ chocolate and $20$ plain biscuits, and the third tin contains $10$ chocolate and $30$ plain biscuits. A tin is chosen at random, and a biscuit is chosen at random from the tin.
\ben
\it If a chocolate biscuit is observed, estimate which tin was chosen.
\een
The biscuit is replaced, and another biscuit is then chosen from the tin.
\ben\stepcounter{enumi}
\it If a chocolate biscuit is observed, update your estimate regarding which tin was chosen.
\it If a plain biscuit is observed, update your estimate regarding which tin was chosen.
\een
\end{example}

\begin{solution}
Let $\Theta = \{\theta_1,\theta_2,\theta_3\}$ where the value $\theta_k$ indicates that tin $k$ was chosen ($k=1,2,3$). Before observing the biscuit, it is reasonable to suppose that each tin is equally likely to be chosen. Thus we adopt the \emph{uniform} prior distribution for $\theta$:
\[
\pi_0(\theta) = \frac{1}{3} \quad\text{for all}\quad \theta\in\Theta.
\]

\ben
\it % one chocolate
Let $A$ be the event that a chocolate biscuit was observed. Then
\[
\prob(A|\theta_1)=3/4,\qquad \prob(A|\theta_2)=1/2,\qquad \prob(A|\theta_3)=1/4,
\]
or equivalently
\[
L(\theta_1| A)=3/4,\qquad L(\theta_2| A)=1/2,\qquad L(\theta_3| A)=1/4.
\]
Using Bayes' theorem, the posterior distribution is
\[
\pi_1(\theta_k|A) = \frac{L(\theta_k| A)\pi_0(\theta_k)}{\sum_{k=1}^3 L(\theta_k| A)\pi_0(\theta_k)}
\text{\quad\qquad ($k=1,2,3$).}
\]
For the first tin,
\[
\pi_1(\theta_1|A) = \frac{3/4\times 1/3}{(3/4\times 1/2) + (1/2\times 1/3) + (1/4\times 1/3)} = \frac{1}{2}.
\]
Similar calculations for the second and third tins yield the following posterior distribution:
\[
\pi_1(\theta_1) = 1/2,\qquad \pi_1(\theta_2) = 1/3, \qquad \pi_1(\theta_3) = 1/6.
\]
The MAP estimate is $\hat{\theta}_{MAP} = \theta_1$, so our best guess is that the first tin was chosen.

 % <<

\it % two chocolate
Let $B$ be the event that the a chocolate biscuit was chosen the second time:
\[
L(\theta_1|B)=3/4,\qquad L(\theta_2|B)=1/2,\qquad L(\theta_3|B)=1/4.
\]
Using $\pi_1$ as our prior distribution, we obtain an updated posterior:
\[
\pi_2(\theta_k|B) = \frac{L(\theta_k|B)\pi_1(\theta_k)}{\sum_{k=1}^3 L(\theta_k| B)\pi_1(\theta_k)}
\text{\quad\qquad ($k=1,2,3$).}
\]
For the first tin,
\[
\pi_2(\theta_1|B) = \frac{3/4\times 1/2}{(3/4\times 1/2) + (1/2\times 1/3) + (1/4\times 1/6)} = \frac{9}{14}.
\]
Similar calculations for the second and third tins yield the following posterior:
\[
\pi_2(\theta_1) = 9/14,\qquad \pi_1(\theta_2) = 4/14, \qquad \pi_1(\theta_3) = 1/14.
\]
The MAP estimator again leads us to assert that the first tin was chosen.

\it % one chocolate, one plain
Let $C$ be the event that a plain biscuit was chosen the second time:
\[
L(\theta_1|C)=1/4,\qquad L(\theta_2|C)=1/2,\qquad L(\theta_3|C)=3/4.
\]
Again using $\pi_1$ as our prior distribution, we obtain an updated posterior:
\[
\pi_2(\theta_k|C) = \frac{L(\theta_k|C)\pi_1(\theta_k)}{\sum_{k=1}^3 L(\theta_k|C)\pi_1(\theta_k)}
\text{\quad\qquad ($k=1,2,3$).}
\]
For the first tin,
\[
\pi_2(\theta_1|C) = \frac{1/4\times 1/2}{(1/4\times 1/2) + (1/2\times 1/3) + (3/4\times 1/6)} = \frac{3}{10}.
\]
Similar calculations for the second and third tins yield the following posterior distribution:
\[
\pi_2(\theta_1) = 3/10,\qquad \pi_1(\theta_2) = 4/10, \qquad \pi_1(\theta_3) = 3/10.
\]
This time, the MAP estimator leads us to assert that the second tin was chosen.
\een
\end{solution}


% remark
\begin{remark}[The Scientific Method]
\mbox{}\par
In the above example, we can think of the biscuit tins as competing scientific theories. 
\bit
\it The probability assigned to each theory indicates its \emph{relative plausibility}.
\it We update the relative plausibility of each competing theory based on \emph{observation}.
\eit

\vspace*{2ex}
Bayesian inference implements scientific method:
\ben
\it Start with an initial set of beliefs about the relative plausibility of various theories.
\it Collect new data, for example by conducting an experiment.
\it Adjust the original set of beliefs in the light of the new data, to produce a more refined set of beliefs regarding the relative plausibility of the different theories. 
\een
\end{remark}

%======================================================================
\stopcontents[chapters]
\endinput
%======================================================================
