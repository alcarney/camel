% !TEX root = main.tex
%======================================================================
\chapter{Continuous Probability}\label{chap:cts}
%======================================================================

%%----------------------------------------------------------------------
%\section{Countably infinite sample spaces}
%%----------------------------------------------------------------------

%%----------------------------------------------------------------------
%\section{Countable families of sets}
%%----------------------------------------------------------------------
%----------------------------------------------------------------------
\section{General probability spaces}
%----------------------------------------------------------------------

Let $\Omega = \{\omega_1,\omega_2,\ldots\}$ be a countably infinite sample space.
\bit
\it There are infinitely many possible subsets of $\Omega$.
\it We may wish to consider an infinite number of events.
\eit
For example, suppose that a coin is tossed repeatedly until the first head occurs. For every prime number $p$, let $A_p$ be the event that the number of times the coin is tossed is divisible by $p$. There are infinitely many prime numbers, so we have an infinite number of events to consider: $A_2, A_3, A_5, A_7,\ldots$. In such situations, to define probability measures properly, we must consider families of events that are not only closed under finite unions (Lecture~\ref{chap:events}), but which are also closed under \emph{countable} unions.

% definition: countable unions and intersections
\begin{definition}
Let $\{A_1,A_2,\ldots\}$ be a countable family of sets over $\Omega$.
\ben
\it The \emph{union} of the sets $A_1,A_2,\ldots$ is the set
\[
\textstyle\bigcup_{i=1}^\infty A_i = \{\omega:\omega\in A_i \text{ for some }A_i\}.
\]
\it The \emph{intersection} of the sets $A_1,A_2,\ldots$ is the set
\[
\textstyle\bigcap_{i=1}^\infty A_i = \{\omega:\omega\in A_i \text{ for all } A_i\}.
\]
\een
\end{definition}

% definition: sigma-field
\begin{definition}
Let $\Omega$ be any set. A family of sets $\mathcal{F}$ is called a \emph{$\sigma$-field} over $\Omega$ if 
\ben
\it $\Omega\in\mathcal{F}$,
\it $\mathcal{F}$ is closed under complementation: if $A\in\mathcal{F}$ then $A^c\in\mathcal{F}$, and
\it $\mathcal{F}$ is closed under countable unions: if $A_1,A_2,\ldots\in\mathcal{F}$ then $\bigcup_{i=1}^{\infty}A_i \in\mathcal{F}$.
\een
\end{definition}

In general, a probability space consists of a triple $(\Omega,\mathcal{F},\prob)$ where
\ben
\it $\Omega$ is a sample space,
\it $\mathcal{F}$ is a $\sigma$-field, and 
\it $\prob$ a probability measure.%on $(\Omega,\mathcal{F})$ (see Lecture~\ref{chap:prob}).
\een

%For non-finite sample spaces, a probability space $(\Omega,\mathcal{F},\prob)$ consists of 
%\ben
%\it a sample space $\Omega$, 
%\it a $\sigma$-field $\mathcal{F}$,
%\it a probability measure $\prob$.%on $(\Omega,\mathcal{F})$ (see Lecture~\ref{chap:prob}).
%$\prob:\mathcal{F}\to[0,1]$ 
%\[
%\begin{array}{rccl}
%	\prob:	& \mathcal{F}	& \to	& [0,1] \\[1ex]
%			& A				& \mapsto	& \prob(A)
%\end{array}
%\]
%which is countably additive with $\prob(\Omega)=1$ (see Lecture~\ref{chap:prob}) .
%\een

% remkar: cardinality
\begin{remark}[Cardinality]
%Georg Cantor (1845--1918) proved the following fundamental results:
\mbox{}\par\vspace{-3ex}
\bit
%\it If $\Omega$ is finite, its power set is also finite.
\it If $\Omega$ is countably infinite, its power set is uncountable (Cantor's theorem).
	\bit
	\it In this case, we can still use the power set of $\Omega$ as a $\sigma$-field of events.
	\eit
\it If $\Omega$ is uncountable, its power set is ``very uncountable''.
	\bit
	\it In this case, the power set of $\Omega$ is simply too big, and we must define an explicit $\sigma$-field of events.% to work with.
	\eit
\eit

%\bit
%\it If $\Omega$ finite or countably infinite, we can use its power set as a $\sigma$-field of events.
%\it If $\Omega$ uncountable, its power set is simply too big, and we must define an explicit $\sigma$-field of events to work with.
%\eit

\end{remark}




%----------------------------------------------------------------------
\section{Continuous random variables}
%----------------------------------------------------------------------
%A continuous function $f(x)$ is one for which small changes in $x$ result in small changes in $f(x)$:

\bit
\it Discrete random variables take values in a finite or countably infinite set.
\it Continuous random variables take arbitrary values in an interval or collection of intervals.
\eit

%Recall that the CDF of a random variable is a function $F(x)=\prob(X\leq x)$.
% definition: continuous
\begin{definition}
A random variable $X$ is called \emph{continuous} if its CDF can be written as 
\[
F(x) = \int_{-\infty}^x f(t)\,dt\qquad x\in\R
\]
for some integrable function $f:\R\to[0,\infty)$ called the \emph{probability density function} (PDF) of $X$.
\end{definition}

% remark
\begin{remark}
%If $X$ is a continuous random variable, then
\ben
%\it The term \emph{continuous} in fact refers to the CDF of $X$
\it $f(x) = F'(x)$ for all $x\in\R$.
\it $\prob(X=x)=0$ for all $x\in\R$.
\it Probabilities correspond to areas under the PDF curve: 
$
\prob(a< X\leq b) = \displaystyle\int_a^b f(x)\,dx = F(b) - F(a).
$
\it $\displaystyle\int_{-\infty}^{\infty} f(x)\,dx = 1.$
\een
\end{remark}

% example: simple continuous
\begin{example}\label{ex:cts_exp}
Let $X$ be a random variable with the following CDF:
\[
F(x) = \left\{\begin{array}{ll}
	1-e^{-\lambda x} & \text{ for } x>0, \\
	0				& \text{ otherwise.}
\end{array}\right.
\]
where $\lambda>0$ is a constant. Show that $X$ is a continuous random variable.
\end{example}
\begin{solution}
$X$ is a continuous random variable, because there exists a function $f(x) = \lambda e^{-\lambda x}$ such that
\begin{align*}
\int_{-\infty}^x f(t)\,dt 
	& = \lambda \int_{-\infty}^x e^{-\lambda t}\,dt \\
	& = \lambda\left[\frac{-e^{-\lambda t}}{\lambda}\right]_{-\infty}^x \\
	& = 1 - e^{-\lambda x} \\
	& = F(x).
\end{align*}
This is the \emph{negative exponential distribution} with (rate) parameter $\lambda$ (see next lecture).
\end{solution}

%% problem
%\begin{example}\label{ex:continuous_rv}
%A straight rod is thrown down at random onto a horizontal plane, and the angle between the rod and a certain fixed orientation is measured. The result is a number $\omega\in [0,2\pi)=\Omega$. Let us suppose that $\mathcal{F}$ has been constructed so that it contains all open subintervals of $[0,2\pi)$, i.e.\ intervals of the form $(a,b)$ with $0\leq a<b< 2\pi$. By the principle of indifference, a suitable probability measure on $(\Omega,\mathcal{F})$ should assign a probability that the angle lies in a given interval $(a,b)$ to be directly proportional to the length of the interval:
%\[
%\prob\big(\big\{\omega:\omega\in(a,b)\big\}\big)=\frac{b-a}{2\pi}.
%\]
%
%Let $X(\omega)=\omega$ and $Y(\omega)=\omega^2$ be two random variables defined on $(\Omega,\mathcal{F},\prob)$. (Note that $Y$ is a function of $X$.) Find the distribution functions of $X$ and $Y$, and show that they are both continuous random variables.
%\end{example}
%\begin{solution}
%To compute the distribution of $X$, let $0\leq x <2\pi$:
%\begin{align*}
%F_X(x)	& = \prob\big(\{\omega:0\leq X(\omega)\leq x\}\big) \\
%		& = \prob\big(\{\omega:0\leq \omega \leq x\}\big) 
%		= \frac{x}{2\pi}
%\end{align*}
%so
%\[
%F_X(x) = \begin{cases}
%	0				& x \leq 0 \\
%	\displaystyle\frac{x}{2\pi}	& 0 \leq x < 2\pi \\
%	1				& x \geq 2\pi
%\end{cases}
%\]
%$X$ is continuous because
%\[
%F_X(x) = \int_{-\infty}^x f_X(u)\,du \qquad\text{where}\qquad 
%f_X(u)=\begin{cases}
%\displaystyle\frac{1}{2\pi}	& 0\leq u\leq 2\pi \\
%0				& \text{otherwise}.
%\end{cases}
%\]
%
%To compute the distribution of $Y$, let $0\leq y <4\pi^2$:
%\begin{align*}
%F_Y(y)	& = \prob\big(\{\omega: Y(\omega)\leq y\}\big) 
%		= \prob\big(\{\omega: \omega^2 \leq y\}\big) \\
%		& = \prob\big(\{\omega: 0\leq \omega \leq \sqrt{y}\}\big) \\
%		& = \prob\big(X\leq\sqrt{y}\big) 
%		= \frac{\sqrt{y}}{2\pi}
%\end{align*}
%so
%\[
%F_Y(y) = \begin{cases}
%	0						& y \leq 0 \\
%	\displaystyle\frac{\sqrt{y}}{2\pi}	& 0 \leq y < 2\pi \\
%	1						& x \geq 2\pi
%\end{cases}
%\]
%$Y$ is continuous because
%\[
%F_Y(y) = \int_{-\infty}^x f_Y(u)\,du \qquad\text{where}\qquad 
%f_Y(y)=\begin{cases}
%\displaystyle\frac{1}{4\pi\sqrt{y}}	& 0\leq y\leq 4\pi^2 \\
%0			& \text{otherwise}.
%\end{cases}
%\]
%\end{solution}
%
%Recall that $X$ is called a \emph{continuous} random variable if its distribution function can be written as
%\[
%F(x) = \int_{-\infty}^x f(u)\,du
%\]
%for some integrable function $f:\R\to[0,\infty)$, called the \emph{probability density function} of $X$.
%
%%\bit
%%\it Note that $f(x) = \displaystyle\frac{d}{dx}F(x)$.
%%\eit

% problem
\begin{example}
A straight rod is thrown down at random onto a horizontal plane, and the angle between the rod and a certain fixed orientation is measured. In the absence of any further information, a natural probability measure on the sample space $\Omega = [0,2\pi)$ is
\[
\prob\big(\big\{\omega\in(a,b)\big\}\big)=\frac{b-a}{2\pi} \quad\text{for}\quad 0\leq a < b < 2\pi.
\]
Let $F_X$ and $F_Y$ be the CDFs of the random variables $X(\omega)=\omega$ and $Y(\omega)=\omega^2$ respectively:
\[
F_X(x) = \begin{cases}
	0				& x \leq 0, \\[2ex]
	\displaystyle\frac{x}{2\pi}	& 0 \leq x < 2\pi, \\[2ex]
	1				& x \geq 2\pi,
\end{cases}
\quad\text{and}\quad
F_Y(y) = \begin{cases}
	0						& y \leq 0, \\[2ex]
	\displaystyle\frac{\sqrt{y}}{2\pi}	& 0 \leq y < 4\pi^2, \\[2ex]
	1						& y \geq 4\pi^2.
\end{cases}
\]
The PDFs of $X$ and $Y$ are, respectively,
\[
f_X(x)=\begin{cases}
\displaystyle\frac{1}{2\pi}	& 0\leq x\leq 2\pi, \\[2ex]
0							& \text{otherwise},
\end{cases}
\quad\text{and}\quad
f_Y(y)=\begin{cases}
\displaystyle\frac{1}{4\pi\sqrt{y}}	& 0\leq y\leq 4\pi^2, \\[2ex]
0									& \text{otherwise}.
\end{cases}
\]
\end{example}



% lemma: properties of pdf
%\begin{lemma}[Properties of density functions]
%Let $X$ be a continuous random variable with probability density function $f(x)$. Then
%\ben
%\it $\displaystyle\int_{-\infty}^{\infty}f(x)\,dx = 1$.
%\it $\prob(X = x) = 0$ for all $x\in\R$.
%\it $\prob(a\leq X\leq b) = \displaystyle\int_a^b f(x)\,dx$.
%\een
%\end{lemma}
%
%% proof
%\begin{proof}
%Let $F$ be the distribution function of $X$. %By Theorem~\ref{thm:properties_cdf},
%\ben
%\it % (i)
%By the properties of CDFs, $F(x)\to 1$ as $x\to\infty$, so
%\[
%\int_{-\infty}^{\infty}f(x)\,dx = \lim_{x\to\infty} \int_{-\infty}^{x}f(x)\,dx = \lim_{x\to\infty} F(x)=1.
%\]
%\it % (ii)
%By the properties of CDFs, $F(x+h)\to F(x)$ as $h\downarrow 0$ so
%\[
%\prob(X=x) 
%	= \displaystyle\lim_{h\downarrow 0} \prob(X\leq x+h)-\prob(X\leq x)
%	= \displaystyle\lim_{h\downarrow 0} F(x+h)-F(x)
%	= 0.
%\]	
%\it % (iii)
%$\prob(a\leq X\leq b) = F(b) - F(a) + \prob(X=a) = F(b) - F(a) = \displaystyle\int_a^b f(x)\,dx$.
%\een
%\end{proof}
%

% remarks
\begin{remark}
Let $X$ be a continuous random variable, and let $f(x)$ denote its PDF. 
\bit
\it The numerical value $f(x)$ is \emph{not} a probability. 
\eit
However, the following heuristic interpretation is often useful.
%
%\vspace*{2ex}
Let $\delta x >0$ be a small positive number. If $f(x)$ is a continuous function, then
\begin{align*}
\prob(x < X \leq x+\delta x) 	
	& = F(x+\delta x) - F(x) \\
	& = \int_{-\infty}^{x+\delta x} f(x)\,dx - \int_{-\infty}^{x} f(x)\,dx \\
	& = \int_{x}^{x+\delta x} f(x)\,dx \\
	& \approx f(x)\delta x.
\end{align*}
Thus we can think of $f(x)\delta x$ as the `amount' of probability in the interval $[x,x+\delta x]$. Note that
\[
f(x) = \lim_{\delta x \to 0} \frac{F(x+\delta x) - F(x)}{\delta x} = F'(x).
\]
\end{remark}

%----------------------------------------------------------------------
\section{Independence}
%----------------------------------------------------------------------
Recall that the \emph{joint CDF} of two random variables $X$ and $Y$ is defined by
\[
F_{X,Y}(x,y) = \prob(X\leq x, Y\leq y).
\]
and that the joint PMF of two discrete random variables $X$ and $Y$ is
%\[
%f_{X,Y}(x,y) = \prob(X=x,Y=y).
%\]

%% definition: jointly continuous, joint density and marginal densities
%\begin{definition}
%\ben
%\it Two random variables $X$ and $Y$ are said to be \emph{jointly continuous} if their joint CDF can be written as
%\[
%F_{X,Y}(x,y) = \int_{-\infty}^y\int_{-\infty}^x f_{X,Y}(s,t)\,ds\,dt\qquad\text{for every }x,y\in\R,
%\]
%where $f_{X,Y}:\R^2\to[0,\infty)$ is an integrable function called the \emph{joint PDF} of $X$ and $Y$.
%\it The PDFs of $X$ and $Y$ are called the \emph{marginal} PDFs, denoted by $f_X(x)$ and $f_Y(y)$ respectively.
%\een
%\end{definition}

% independence
%Recall that two discrete random variables $X$ and $Y$ are said to be \emph{independent} if the events $\{X=x\}$ and $\{Y=y\}$ are independent for all $x$ and $y$:
%\[
%\prob(X=x,Y=y) = \prob(X=x)\prob(Y=y) \text{\quad for all }x,y\in\R.
%\]
%This definition cannot be applied to continuous random variables, because these events  $\{X=x\}$ and $\{Y=y\}$ have zero probability (and are therefore trivially independent).

% definition
\begin{definition}
Two arbitrary random variables $X$ and $Y$ are called \emph{independent} if the events
%\begin{align*}
%\{X \leq x\} & \equiv \{\omega\,:\, X(\omega) \leq x\} \\
%\{Y \leq y\} & \equiv \{\omega\,:\, Y(\omega) \leq y\}
%\end{align*}
$\{X\leq x\}$ and $\{Y\leq y\}$ are independent for all $x,y\in\R$. 
\end{definition}

\bit
\it If $X$ and $Y$ are independent, then $F_{X,Y}(x,y) = F_X(x)F_Y(y)$ for all $x,y\in\R$.
%\it If $X$ and $Y$ are independent discrete variables, then $f_{X,Y}(x,y)=p_X(x)p_Y(y)$ for all $x,y\in\R$.
%\it An analogous result applies to independent continuous random variables.
\eit

%We have the following analogue of Theorem~\ref{thm:product_marginal_pmfs}:

%% lemma: continuous
%\begin{lemma}\label{thm:product_marginal_pdfs}
%Let $X$ and $Y$ be jointly continuous random variables, with joint density function $f_{X,Y}(x,y)$ and marginal density functions $f_X(x)$ and $f_Y(x)$ respectively. If $X$ and $Y$ are independent, then
%\[
%f_{X,Y}(x,y) = f_X(x)f_Y(y) \quad\text{for all}\quad x,y\in\R
%\]
%\end{lemma}
%\proofomitted
%
%\begin{remark}
%For discrete random variables, the \emph{conditional PMF} of $Y$ given $X=x$ is defined by
%%\[
%%p_{Y|X}(y|x) = \prob(Y=y|X=x) = \frac{\prob(X=x,Y=y)}{\prob(X=x)} = \frac{p_{X,Y}(x,y)}{p_X(x)}.
%%\]
%\[
%p_{Y|X}(y|x) = \frac{p_{X,Y}(x,y)}{p_X(x)}.
%\]
%For continuous random variables, we cannot condition on $\{X=x\}$ because $\prob(X=x)=0$. However, by analogy we can define the \emph{conditional PDF} of $Y$ given $X=x$ to be
%\[
%f_{Y|X}(y|x) = \frac{f_{X,Y}(x,y)}{f_X(x)}.
%\]
%This allows us to define the conditional expectation of $Y$ given $X=x$, and prove a law of total expectation for continuous random variables.
%\end{remark}
%
%%We also have the following analogue of Theorem~\ref{thm:independence_of_functions_of_rvs}:
%%
%%% theorem: unconscious
%%\begin{theorem}\label{thm:independence_of_functions_of_rvs_cts}
%%Let $X$ and $Y$ be continuous random variables on $(\Omega,\mathcal{F})$, and let $g,h:\R\to\R$ be such that $g(X)$ and $h(Y)$ are random variables on $(\Omega,\mathcal{F})$. If $X$ and $Y$ are independent, then $g(X)$ and $h(Y)$ are also independent.
%%\end{theorem}
%%\proofomitted
%

%----------------------------------------------------------------------
\section{Expectation}
%----------------------------------------------------------------------
Let $X$ be a discrete random variable, let $\{x_1,x_2,\ldots\}$ be its range, and let $f(x)$ be its PMF. The expectation of $X$ is defined by a \emph{sum}:
\[
\expe(X) = \sum_{i=1}^{\infty} x_i f(x_i)
\]

In contrast, the expectation of a continuous random variable is defined by an \emph{integral}:

% definition
\begin{definition}
Let $X$ be a continuous random variable, and let $f(x)$ be its PDF. The expectation of $X$ is  
\[
\expe(X) = \int_{-\infty}^{\infty} x f(x)\,dx,
\]
provided this integral exists.
\end{definition}



%We have the following analogue of Theorem~\ref{lem:law_unconscious_statistician}:

% theorem: law of unconscious statistician
\begin{theorem}\label{lem:law_unconscious_statistician_cts}
Let $X$ be a continuous random variable, let $f(x)$ be its PDF, and let $g:\R\to\R$ be any well-behaved function. Then
\[
\expe\big[g(X)\big] = \int_{-\infty}^{\infty} g(x)f(x)\,dx
\]
\end{theorem}
\proofomitted

\begin{definition}
Variance, covariance and the correlation coefficient are again defined in terms of expectation:
\ben
\it $\var(X) = \expe\big[\big(X-\expe(X)\big)^2\big] = \expe(X^2) - \expe(X)^2$.
\it $\cov(X,Y) = \expe(XY) - \expe(X)\expe(Y)$.
\it $\rho(X,Y) = \displaystyle\frac{\cov(X,Y)}{\sqrt{\var(X)\cdot\var(Y)}}$.
\een
\end{definition}

The following properties of expectation and variance also hold for continuous variables:
\begin{theorem}
Let $X$ and $Y$ be continuous random variables, and let $a,b\in\R$.
\ben
\it $\expe(aX+b) = a\expe(X) + b$.
\it $\var(aX+b) = a^2\var(X)$.
\it If $X$ and $Y$ are uncorrelated, then $\var(X+Y)=\var(X)+\var(Y)$.
\een
\end{theorem}

\begin{proof}
\hideoff
See exercises.
\hideon
\end{proof}


%----------------------------------------------------------------------
%\section{Examples}
%----------------------------------------------------------------------
% example: uniform distribution
\begin{example}\label{ex:continuous_uniform}
Let $X$ be a continuous random variable with the following CDF:
\[
F(x) = \begin{cases}
	\displaystyle\frac{x-a}{b-a}	& \text{if }\ a\leq x\leq b \\[2ex]
	0							& \text{otherwise.}
\end{cases}	
\]
Find the PDF of $X$, and compute its expected value and variance.
\end{example}

\begin{solution}
The PDF of $X$ is given by $F'(x)$:
\[
f(x) = \begin{cases}
	\displaystyle\frac{1}{b-a}	& \text{if }\ a\leq x\leq b, \\[0ex]
	0				& \text{otherwise.}
\end{cases}	
\]
\begin{align*}
\expe(X)
	& = \int_{-\infty}^{\infty}x f(x)\,dx
	= \frac{1}{b-a}\int_{a}^{b}x\,dx
	= \frac{1}{b-a}\left[\frac{x^2}{2}\right] _{a}^{b}
	= \frac{b^2-a^2}{2(b-a)}
	= \frac{b+a}{2}. \\[2ex]
\expe(X^2)
	& = \int_{-\infty}^{\infty} x^2 f(x)\,dx
	= \frac{1}{b-a}\int_{a}^{b} x^2\,dx 
	= \frac{1}{b-a}\left[\frac{x^3}{3}\right]_{a}^{b}
	= \frac{b^{3}-a^{3}}{3(b-a)}
	= \frac{(b^{2}+ab+a^{2})}{3}. \\[2ex]
\var(X)
	& = \expe(X^2) - \expe(X)^2 
	= \frac{(b^{2}+ab+a^{2})}{3} - \frac{(b+a)^{2}}{4} 
	= \frac{(b^{2}-2ab+a^{2})}{12}
	= \frac{(b-a)^{2}}{12}.
\end{align*}
This is the (continuous) \emph{uniform} distribution on $[a,b]$ (see next lecture). 
\end{solution}

%% example
%\begin{example}\label{ex:continuous_exponential}
%Let $X$ be a continuous random variable with distribution function
%\[
%F(x) = \begin{cases}
%	1-e^{-\lambda x}	& \text{if }\ x\geq 0 \\ 
%	0				& \text{otherwise.}
%\end{cases}	
%\]
%Find the density function of $X$, and hence find its expected value and variance.
%\end{example}
%
%\begin{solution}
%This is the (negative) \emph{exponential} distribution with (rate) parameter $\lambda>0$. 
%
%The density function $f(x) = F'(x)$ of $X$ is
%\[
%f(x) = \begin{cases}
%	\lambda e^{-\lambda x}	& \text{if }\ x\geq 0 \\ 
%	0						& \text{otherwise.}
%\end{cases}	
%\]
%The expected value of $X$ is
%\[
%\expe(X) = \int xf(x)\,dx = \lambda\int_0^{\infty} xe^{-\lambda x}\,dx
%\]
%Integrating by parts,
%\begin{equation}\label{eq:expe_expo}
%\int_0^{\infty} xe^{-\lambda x}\,dx
%	= \left[-\frac{xe^{-\lambda x}}{\lambda}\right]_0^{\infty} + \int_0^{\infty} \frac{e^{\lambda x}}{\lambda}\,dx
%	= 0 + \frac{1}{\lambda}\int_0^{\infty} e^{-\lambda x}\,dx
%	= \frac{1}{\lambda}\left[-\frac{e^{-\lambda x}}{\lambda}\right]_0^{\infty}
%	= \frac{1}{\lambda^2} \tag{*}
%\end{equation}
%so 
%\[
%\expe(X)=\lambda\cdot\frac{1}{\lambda^2} = \frac{1}{\lambda}.
%\]
%To compute the variance, the expected value of $X^2$ is 
%\[
%\expe(X^2) = \int x^2 f(x)\,dx = \lambda\int_0^{\infty} x^2 e^{-\lambda x}\,dx
%\]
%Integrating by parts,
%\[
%\int_0^{\infty} x^2 e^{-\lambda x}\,dx
%	= \left[x^2\left(-\frac{e^{-\lambda x}}{\lambda}\right)\right]_0^{\infty} + \int_0^\infty 2x\left(\frac{e^{-\lambda x}}{\lambda}\right)\,dx 
%	= \frac{2}{\lambda}\int_0^{\infty} xe^{-\lambda x}\,dx 
%	= \frac{2}{\lambda^3} \qquad \text{by Eq.~(\ref{eq:expe_expo}).}
%\]
%so 
%\[
%\expe(X^2)=\lambda\cdot\frac{2}{\lambda^3} = \frac{2}{\lambda^2},
%\]
%and the variance is 
%\[
%\var(X) = \expe(X^2) - \expe(X)^2 = \frac{2}{\lambda^2} - \left(\frac{1}{\lambda}\right)^2 = \frac{1}{\lambda^2}.
%\]
%\end{solution}


%----------------------------------------------------------------------
\section{Exercises}
\input{ex17_continuous}
%----------------------------------------------------------------------

%======================================================================
\endinput
%======================================================================
