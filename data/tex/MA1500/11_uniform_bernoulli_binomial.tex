% !TEX root = main.tex
%======================================================================
\chapter{The Uniform, Bernoulli and Binomial Distributions}\label{chap:unif-bern-bino}
%======================================================================

In this lecture we look at some well-known distributions which take values in a finite subset of the non-negative integers.
%----------------------------------------------------------------------
\section{Uniform distribution}
%----------------------------------------------------------------------

The uniform distribution on $\{1,2,\ldots,n\}$ assigns an equal probability to each value, and as such underpins the classical model of probability (Lecture~\ref{chap:classical}).
 
\begin{center}
\begin{tabular}{ll}\hline
Notation		& $X\sim\text{Uniform}\{1,2,\ldots,n\}$ \\
Parameter(s)	& $n\in\mathbb{N}$ \\
Range			& $\{1,2,\ldots,n\}$ \\
PMF				& $f(k) = 1/n$ for all $k=1,2,\ldots,n$ \\ \hline
\end{tabular}
\end{center}

\begin{lemma}
If $X\sim\text{Uniform}\{1,2,\ldots,n\}$, then 
\[
\expe(X)= \frac{n+1}{2} \quad\text{and}\quad \var(X) = \frac{n^2-1}{12}.
\]
\end{lemma}

\begin{proof}
\begin{align*}
\expe(X) 
	& = \sum_{k=1}^n k\,f(k)
	= \frac{1}{n}\sum_{k=1}^n k 
	= \frac{1}{n}\left(\frac{n(n+1)}{2}\right)
	= \frac{n+1}{2}. \\
\expe(X^2) 
	& = \sum_{k=1}^n k^2\,f(k)
	= \frac{1}{n}\sum_{k=1}^n k^2 
	= \frac{1}{n}\left(\frac{n(n+1)(2n+1)}{6}\right)
	= \frac{(n+1)(2n+1)}{6}. \\
\var(X)
	& = \expe(X^2)-\expe(X)^2  
	= \frac{(n+1)(2n+1)}{6} - \frac{(n+1)^2}{4} 
	  = \frac{(n-1)(n+1)}{12}
	  = \frac{n^2-1}{12}.
\end{align*}
\end{proof}

\newpage
\begin{example}[Entropy]
Entropy is a way of quantifying the uncertainty associated with a random variable.

\smallskip
Let $X$ be a random variable taking values in $\{1,2,\ldots,n\}$, and let $f(k)=\prob(X=k)$ be its PMF. The \emph{entropy} of $X$ is defined by
\[
H(X) = -\sum_{k=1}^n f(k)\log f(k)
\]

If $X$ is a non-random, say $f(j)=1$ and $f(k)=0$ for all $k\neq j$, then
\[
H(X) = -\sum_{k=1}^n f(k)\log f(k) = - f(j)\log f(j) = -\log 1 = 0.
\]

If $X$ is uniformly distributed, so that \ $f(k)=1/n$ for all $k\in\{1,2,\ldots,n\}$, then
\[
H(X) = -\sum_{k=1}^n \frac{1}{n}\log\left(\frac{1}{n}\right) = -\log\left(\frac{1}{n}\right) = \log n.
\]

In general, for any random variable $X$ taking values in $\{1,2,\ldots,n\}$, it can be shown that 
\[
0\leq H(X)\leq \log n.
\]
Among all probability distributions on $\{1,2,\ldots,n\}$, the uniform distribution has \emph{maximum entropy}.
\end{example}

%----------------------------------------------------------------------
\section{Bernoulli distribution}
%----------------------------------------------------------------------
The Bernoulli distribution is the distribution of an indicator variable.

\begin{center}
\begin{tabular}{ll}\hline
Notation			& $X\sim\text{Bernoulli}(p)$ \\
Parameter(s)		& $p \in [0,1]$ \quad (probability of success) \\
Range			& $\{0,1\}$ \\
PMF				& $f(0) = 1-p$ and $f(1) = p$ \\ \hline
\end{tabular}
\end{center}

\begin{lemma}
If $X\sim\text{Bernoulli}(p)$, then 
\[
\expe(X)= p \quad\text{and}\quad \var(X) = p(1-p).
\]
\end{lemma}

\begin{proof}
\begin{align*}
\expe(X) 
	& 	= \sum_{k=0}^1 k\,f(k)
	 	= \big[0\times(1-p)\big] + \big[1\times p\big]
		= p. \\
\expe(X^2) 
	& 	= \sum_{k=0}^1 k^2\,f(k)
		= \big[0^2\times(1-p)\big] + \big[1^2\times p\big]
		= p. \\
\var(X)	
	& 	= \expe(X^2)-\expe(X)^2
		= p - p^2
		= p(1-p).
\end{align*}
\end{proof}

\newpage
% example
\begin{example}
Find the entropy of the $\text{Bernoulli}(p)$ distribution, and show that it reaches its maximum value when $p=1/2$.
\end{example}

\begin{solution}
Let $X\sim\text{Bernoulli}(p)$. Then $f(0)=1-p$ and $f(1)=p$, so
\[
H(X) = -\sum_{k=1}^n f(k)\log f(k) = -(1-p)\log(1-p) - p\log p.
\]
To find the maximum, we first differentiate with respect to p:
\begin{align*}
\frac{dH}{dp} 
	& = \log(1-p) -(1-p)\frac{1}{1-p}(-1) - \log p - p\,\frac{1}{p} \\
	& = \log(1-p) - \log p = \log\left(\frac{1-p}{p}\right).
\end{align*}
Setting this equal to zero,
\[
\log\left(\frac{1-p}{p}\right) = 0
\quad\Rightarrow\quad
\frac{1-p}{p} = 1
\quad\Rightarrow\quad
p = 1/2, \quad\text{as required.}
\]
\end{solution}


%----------------------------------------------------------------------
\section{Binomial distribution}
%----------------------------------------------------------------------

%The binomial distribution has two parameters, 
%\bit
%\it the \emph{number of trials} $n$, and 
%\it the \emph{probability of success} $p$. 
%\eit
%It is the distribution of the number of successes in a sequence of $n$ independent Bernoulli trials, where $p$ the probability of success in each trial.

The binomial distribution is the distribution of the number of successes in a sequence of independent Bernoulli trials.

\begin{center}
\begin{tabular}{ll}\hline
Notation			& $X\sim\text{Binomial}(n,p)$ \\
Parameter(s)		& $n \in\mathbb{N}$ \qquad (number of trials) \\
				& $p \in[0,1]$ \quad (probability of success) \\
Range			& $\{0,1,2,\ldots,n\}$ \\
PMF				& $f(k) = \displaystyle\binom{n}{k}p^k(1-p)^{n-k}$\\[2ex] \hline
\end{tabular}
\end{center}

\begin{lemma}\label{lem:binom-mean-var}
If $X\sim\text{Binomial}(n,p)$, then 
\[
\expe(X)= np \quad\text{and}\quad \var(X) = np(1-p).
\]
\end{lemma}

\begin{proof}
Consider a sequence of independent Bernoulli trials in which the probability of success is $p$. Let $X_i$ be the indicator variable of the event that success occurs on the $i$th trial, and let $X$ be the total number of successes in $n$ trials:
\[
X = \sum_{i=1}^n X_i \qquad\text{where}\quad X_i =
  \begin{cases}
   1 & \text{if success on the $i$th trial,} \\
   0 & \text{otherwise.}
  \end{cases}
\]
Then $X_i\sim\text{Bernoulli}(p)$ for each $i=1,2,3,\ldots$, and $X\sim\text{Binomial}(n,p)$. Now,
\[
\expe(X_i) = p \text{\quad and\quad} \var(X_i)=p(1-p),
\]
so by the linearity of expectation,
\begin{align*}
\expe(X) & = \expe(X_1)+\expe(X_2)+\ldots+\expe(X_n) = np, \\
\intertext{and because the trials are independent,}
\var(X) & = \var(X_1)+\var(X_2)+\ldots+\var(X_n) = np(1-p).
\end{align*}
\end{proof}

%We show that if $X\sim\text{Binomial}(n,p)$, then $\expe(X)=np$ and $\var(X)=np(1-p)$.
%
%\vspace*{2ex}
%Let $q=1-p$. Then the PMF of $X$ can be written as 
%\[
%f(k) = \binom{n}{k} p^k q^k,\qquad k=0,1,\ldots,n.
%\]
%The expected value is
%\begin{align*}
%\expe(X)
%	= \sum_{k=0}^{n} kf(k)
%	& = \sum_{k=0}^{n}\binom{n}{k} k p^{k}q^{n-k} \\
%	& = \sum_{k=1}^{n}\frac{n!}{(k-1)!(n-k)!} p^{k} q^{n-k} \\
%	& = np\sum_{k=1}^{n}\frac{(n-1)!}{(k-1)!(n-k)!} p^{k-1} q^{n-k}.
%\end{align*}
%
%\textbf{Claim}: $\displaystyle\sum_{k=1}^{n}\frac{(n-1)!}{(k-1)!(n-k)!} p^{k-1} q^{n-k} = 1$.
%
%Let $j=k-1$. Then
%\begin{align*}
%\sum_{k=1}^{n}\frac{(n-1)!}{(k-1)!(n-k)!} p^{k-1} q^{n-k}
%	& = \sum_{j=0}^{n-1}\frac{(n-1)!}{j!(n-j-1)!} p^{j} q^{n-j-1} \\
%	& = \sum_{j=0}^{n-1}\binom{n-1}{j} p^{j} q^{n-j-1} \\
%	& = (p+q)^{n-1} = 1 \qquad\text{(by the binomial theorem).}
%%	& = 1.%\qquad\text{(since $p+q=1$).}
%\end{align*}
%Hence $\expe(X)=np$.
%To compute the variance, we use the identity 
%\[
%\expe(X^2) = \expe\big[X(X-1]\big]+\expe(X).
%\]
%%First,
%\begin{align*}
%\expe[X(X - 1)]
%	= \sum_{k=0}^{n} k(k-1)f(k) 
%	& = \sum_{k=0}^{n} \binom{n}{k} k(k-1)p^{k} q^{n-k} \\
%	& = \sum_{k=2}^{n}\frac{n!}{(k-2)!(n-k)!} p^{k} q^{n-k}  \\
%	& = n(n-1)p^{2} \sum_{k=2}^{n}\frac{(n-2)!}{(k-2)!(n-k)!} p^{k-2} q^{n-k}.
%\end{align*}
%
%\textbf{Claim}: $\displaystyle\sum_{k=2}^{n}\frac{(n-2)!}{(k-2)!(n-k)!} p^{k-2} q^{n-k} = 1$.
%
%\vspace*{2ex}
%Let $j=k-2$. Then
%\begin{align*}
%\sum_{k=2}^{n}\frac{(n-2)!}{(k-2)!(n-k)!} p^{k-2} q^{n-k} 
%	& = \sum_{j=0}^{n-2}\frac{(n-2)!}{j!(n-2-j)!} p^{j} q^{n-2-j} \\
%	& = \sum_{j=0}^{n-2}\binom{n-2}{j} p^{j} q^{n-2-j} \\
%	& = (p+q)^{n-2} = 1\qquad\text{(by the binomial theorem).}
%%	& = n(n-1)p^2 \qquad\text{(since $p+q=1$).}
%\end{align*}
%
%%\begin{align*}
%%\expe(X(X - 1))
%%	& = n(n-1)p^{2} \sum_{j=0}^{n-2}\frac{(n-2)!}{j!(n-2-j)!} p^{j} q^{n-2-j} \qquad\text{where $j=k-2$,} \\
%%	& = n(n-1)p^{2} \sum_{j=0}^{n-2}\binom{n-2}{j} p^{j} q^{n-2-j} \\
%%	& = n(n-1)p^{2}(p+q)^{n-2}\qquad\text{(by the binomial theorem)} \\
%%	& = n(n-1)p^2 \qquad\text{(since $p+q=1$).}
%%\end{align*}
%Thus $\expe[X(X - 1)]=n(n-1)p^2$, so
%\[
%\expe(X^2) 
%	= \expe[X(X-1)] + \expe(X)
%	= n(n-1)p^2 + np,
%\]
%and hence
%\begin{align*}
%\var(X)	
%	& = \expe(X^2)-\expe(X)^2 \\
%	& = n(n-1)p^2 + np - n^2p^2 \\
%	& = np(1-p).
%\end{align*}

\newpage
% example
\begin{example}
A multiple choice test consists of ten questions, each with a choice of three different answers. A student decides to choose the answers at random. Find the mean and variance of the number of correct answers.
\end{example}

\begin{solution}
Let $X$ be the number of correct answers. Then $X\sim\text{Binomial}(n,p)$ where
\bit
\it $n=10$  (the total number of questions), and
\it $p=1/3$ (the probability of a correct answer).
\eit
Thus
\[
\expe(X)=np=\frac{10}{3}\quad\text{and}\quad \var(X)=np(1-p)=\frac{20}{9}.
\]
\end{solution}

%% skewness and kurtosis
%\subsubsection*{Skewness and kurtosis}
%\ben
%\it The skewness of $X\sim\text{Binomial}(n,p)$ is
%\[
%\gamma_1 = \frac{1-2p}{\sqrt{np(1-p)}} = \begin{cases}
%	< 0 	& \text{if } p > \frac{1}{2} \\
%	= 0 	& \text{if } p = \frac{1}{2} \\
%	> 0 	& \text{if } p < \frac{1}{2} \\
%\end{cases}	
%\]
%\it The kurtosis of $X\sim\text{Binomial}(n,p)$ is
%\[
%\gamma_2 = \frac{1-6p(1-p)}{np(1-p)} = \begin{cases}
%	< 0 				& \text{if } \left|p -\frac{1}{2}\right| < \frac{1}{2\sqrt{3}} \\
%	> 0 				& \text{if } \left|p -\frac{1}{2}\right| > \frac{1}{2\sqrt{3}} \\
%\end{cases}
%\]
%\een
%
%\bit
%\it $X$ has \emph{negative kurtosis} (low and wide peak) when p is close to $1/2$.
%\it $X$ has \emph{positive kurtosis} (tall and narrow peak) when $p$ is close to $0$ of $1$.
%\eit
%
%\begin{remark}
%\bit
%\it The kurtosis $\gamma_2$ reaches its minimum value when $p=\frac{1}{2}$. 
%\it For any fixed $p\in(0,1)$, $\gamma_2\to 0$ as $n\to\infty$.
%\it The kurtosis of the \emph{normal distribution} is $0$.
%\eit
%\end{remark}
%
%The \emph{de\,Moivre\,--\,Laplace} theorem states that the binomial distribution converges to a normal distribution as the number of trials $n\to\infty$.
%\bit
%%\it The binomial distribution converges to a normal distribution as $n\to\infty$.
%\it This is a special case of the \emph{central limit theorem} (CLT).
%\eit


%----------------------------------------------------------------------
\section{Exercises}
\input{ex11_uniform_bernoulli_binomial}
%----------------------------------------------------------------------

\newpage
%----------------------------------------------------------------------
\section*{Long proof of Lemma~\ref{lem:binom-mean-var}$^{*}$}
%----------------------------------------------------------------------

\underline{To show}: if $X\sim\text{Binomial}(n,p)$, then $\expe(X)=np$ and $\var(X)=np(1-p)$.

\vspace*{2ex}
Let $q=1-p$. The PMF of $X$ can be written as 
\[
f(k) = \binom{n}{k} p^k q^k,\qquad k=0,1,\ldots,n.
\]
The expected value is
\begin{align*}
\expe(X)
	= \sum_{k=0}^{n} kf(k)
	= \sum_{k=0}^{n}\binom{n}{k} k p^{k}q^{n-k} 
	& = \sum_{k=1}^{n}\frac{n!}{(k-1)!(n-k)!} p^{k} q^{n-k} \\
	& = np\sum_{k=1}^{n}\frac{(n-1)!}{(k-1)!(n-k)!} p^{k-1} q^{n-k}.
\end{align*}

%\textbf{To show}: \[\displaystyle\sum_{k=1}^{n}\frac{(n-1)!}{(k-1)!(n-k)!} p^{k-1} q^{n-k} = 1.\]
%
Letting $j=k-1$,
\begin{align*}
\sum_{k=1}^{n}\frac{(n-1)!}{(k-1)!(n-k)!} p^{k-1} q^{n-k}
	= \sum_{j=0}^{n-1}\frac{(n-1)!}{j!(n-j-1)!} p^{j} q^{n-j-1} 
	& = \sum_{j=0}^{n-1}\binom{n-1}{j} p^{j} q^{n-j-1} \\
	& = (p+q)^{n-1} = 1 \qquad\text{(by the binomial theorem).}
%	& = 1.%\qquad\text{(since $p+q=1$).}
\end{align*}
Hence $\expe(X)=np$.
To compute the variance, we use the identity 
\[
\expe(X^2) = \expe\big[X(X-1)\big]+\expe(X).
\]
First,
\begin{align*}
\expe[X(X - 1)]
	= \sum_{k=0}^{n} k(k-1)f(k) 
	& = \sum_{k=0}^{n} \binom{n}{k} k(k-1)p^{k} q^{n-k} \\
	& = \sum_{k=2}^{n}\frac{n!}{(k-2)!(n-k)!} p^{k} q^{n-k}  \\
	& = n(n-1)p^{2} \sum_{k=2}^{n}\frac{(n-2)!}{(k-2)!(n-k)!} p^{k-2} q^{n-k}.
\end{align*}

%\textbf{Claim}: $\displaystyle\sum_{k=2}^{n}\frac{(n-2)!}{(k-2)!(n-k)!} p^{k-2} q^{n-k} = 1$.

\vspace*{2ex}
Letting $j=k-2$,
\begin{align*}
\sum_{k=2}^{n}\frac{(n-2)!}{(k-2)!(n-k)!} p^{k-2} q^{n-k} 
	= \sum_{j=0}^{n-2}\frac{(n-2)!}{j!(n-2-j)!} p^{j} q^{n-2-j}
	& = \sum_{j=0}^{n-2}\binom{n-2}{j} p^{j} q^{n-2-j} \\
	& = (p+q)^{n-2} = 1\qquad\text{(by the binomial theorem).}
%	& = n(n-1)p^2 \qquad\text{(since $p+q=1$).}
\end{align*}

%\begin{align*}
%\expe(X(X - 1))
%	& = n(n-1)p^{2} \sum_{j=0}^{n-2}\frac{(n-2)!}{j!(n-2-j)!} p^{j} q^{n-2-j} \qquad\text{where $j=k-2$,} \\
%	& = n(n-1)p^{2} \sum_{j=0}^{n-2}\binom{n-2}{j} p^{j} q^{n-2-j} \\
%	& = n(n-1)p^{2}(p+q)^{n-2}\qquad\text{(by the binomial theorem)} \\
%	& = n(n-1)p^2 \qquad\text{(since $p+q=1$).}
%\end{align*}
Hence $\expe[X(X - 1)]=n(n-1)p^2$. This yields
\[
\expe(X^2) 
	= \expe[X(X-1)] + \expe(X)
	= n(n-1)p^2 + np,
\]
so the variance of $X$ is 
\begin{align*}
\var(X)	
	& = \expe(X^2)-\expe(X)^2 \\
	& = n(n-1)p^2 + np - n^2p^2 \\
	& = np(1-p).
\end{align*}
as required.

%======================================================================
\endinput
%======================================================================
