% !TEX root = main.tex
%======================================================================
\chapter{Joint Distributions}\label{chap:joint}
%======================================================================

Let $X$ and $Y$ be simple random variables, let $\{x_1,x_2,\ldots,x_m\}$ be the range of $X$, and let $\{y_1,y_2,\ldots,y_n\}$ be the range of $Y$.

%----------------------------------------------------------------------
\section{Joint distributions}
%----------------------------------------------------------------------
% defn: joint cdf
\begin{definition}
\ben
\it The \emph{joint CDF} of $X$ and $Y$ is the function
\[
\begin{array}{cccc}
F_{X,Y}:	& \R^2	& \to		& [0,1] \\[1ex]
			& (x,y) & \mapsto	& \prob(X\leq x, Y\leq y).
\end{array}
\]
\it The \emph{marginal CDF} of $X$ is the function
\[
\begin{array}{cccc}
F_X:	& \mathbb{R} 	& \to		& [0,1] \\[1ex]
		& x				& \mapsto	& \prob(X\leq x),
\end{array}
\]
and the marginal CDF of $Y$ is
\[
%\qquad\text{and}\qquad
\begin{array}{cccc}
F_Y:	& \mathbb{R}	& \to		& [0,1] \\[1ex]
		& y				& \mapsto	& \prob(Y\leq y).
\end{array}
%\qquad\text{respectively.}
\]

%\it The \emph{marginal} CDFs of $X$ and $Y$ are the functions
%\[
%\begin{array}{cccc}
%F_X:	& \mathbb{R} 	& \to		& [0,1] \\[1ex]
%		& x				& \mapsto	& \prob(X\leq x)
%\end{array}
%\qquad\text{and}\qquad
%\begin{array}{cccc}
%F_Y:	& \mathbb{R}	& \to		& [0,1] \\[1ex]
%		& y				& \mapsto	& \prob(Y\leq y)
%\end{array}
%\qquad\text{respectively.}
%\]
%respectively.
\een
\end{definition}

\begin{remark}%[Notation]
$\prob(X\leq x, Y\leq y)$ is the probability of the event $\{\omega: X(\omega)\leq x \text{ and } Y(\omega)\leq y\}$,
%Recall the following notation:
%\bit
%\it $\prob(X\leq x)$ is the probability of the event $\{\omega: X(\omega)\leq x\}$,
%\it $\prob(Y\leq y)$ is the probability of the event $\{\omega: Y(\omega)\leq y\}$.
%\it $\prob(X\leq x, Y\leq y)$ is the probability of the event $\{\omega: X(\omega)\leq x \text{ and } Y(\omega)\leq y\}$,
%\eit
%Note that the joint CDF is a function of two variables:
%\[
%\begin{array}{cccl}
%F_{X,Y}:	& \mathbb{R}^2	& \longrightarrow	& [0,1] \\
%		& (x,y) 			& \mapsto			& \prob(X\leq x, Y\leq y).
%\end{array}
%\]
\end{remark}

For simple random variables, it is often easier to work with joint PMFs:
% defn: joint pmf
\begin{definition}
%Let $X,Y:\Omega\to\R$ be two random variables on a finite probability space $(\Omega,\prob)$.
\ben
\it The \emph{joint PMF} of $X$ and $Y$ is the function
\[
\begin{array}{cccc}
F_{X,Y}:	& \R^2	& \to		& [0,1] \\[1ex]
			& (x,y) & \mapsto	& \prob(X=x, Y=y).
\end{array}
\]
\it The \emph{marginal PMF} of $X$ is the function
\[
\begin{array}{cccc}
f_X:	& \mathbb{R} 	& \to		& [0,1] \\[1ex]
		& x				& \mapsto	& \prob(X=x),
\end{array}
\]
and the marginal PMF of $Y$ is
\[
%\qquad\text{and}\qquad
\begin{array}{cccc}
f_Y:	& \mathbb{R}	& \to		& [0,1] \\[1ex]
		& y				& \mapsto	& \prob(Y=y).
\end{array}
%\qquad\text{respectively.}
\]
%\it The \emph{marginal} PMFs of $X$ and $Y$ are the functions
%\[
%\begin{array}{cccc}
%f_X:	& \mathbb{R} 	& \to		& [0,1] \\[1ex]
%		& x				& \mapsto	& \prob(X=x)
%\end{array}
%\qquad\text{and}\qquad
%\begin{array}{cccc}
%f_Y:	& \mathbb{R}	& \to		& [0,1] \\[1ex]
%		& y				& \mapsto	& \prob(Y=y).
%\end{array}
%\qquad\text{respectively.}
%\]
%\begin{align*}
%f_X(x)	& = \prob(X=x) = \prob(\{\omega\in\Omega : X(\omega)=x\}), \\
%f_Y(y)	& = \prob(Y=y) = \prob(\{\omega\in\Omega : Y(\omega)=y\})
%\end{align*}
%respectively.
\een
\end{definition}

% lemma
\begin{lemma}
%Let $f_{X,Y}$ be the joint PMF of two simple random variables $X$ and $Y$. 
The marginal PMFs of $X$ and $Y$ satisfy
\[
f_X(x_i) = \sum_{j=1}^n f_{X,Y}(x_i,y_j) \quad\text{and}\quad f_Y(y_j) = \sum_{i=1}^m f_{X,Y}(x_i,y_j).
\]
\end{lemma}

% proof
\begin{proof}
By definition, the joint PMF of $X$ and $Y$ can be written as $f_{X,Y}(x_i,y_j) = \prob(A_{i,j})$, where
\[
A_{i,j} = \{X=x_i, Y=y_j\} \equiv \{\omega: X(\omega)=x_i \text{ and } Y(\omega) = y_j\}.
\]
The sets $A_{i,1}, A_{i,2},\ldots,A_{i,n}$ form a partition of the event $\{X=x_i\}$, so by the partition theorem,
\[
%f_X(x_i) = \prob(X=x_i) = \sum_{j=1}^n \prob(X=x_i,Y=y_j) = \sum_{j=1}^n f_{X,Y}(x_i,y_j), \\
f_X(x_i) = \prob(X=x_i) = \sum_{j=1}^n \prob(A_{i,j}) = \sum_{j=1}^n f_{X,Y}(x_i,y_j). \\
\]
Similarly, $A_{1,j}, A_{2,j},\ldots,A_{m,j}$ form a partition of the event $\{Y=y_j\}$, so by the partition theorem,
\[
%f_Y(y_j) = \prob(Y=y_j) = \sum_{i=1}^m \prob(X=x_i,Y=y_j) = \sum_{i=1}^m f_{X,Y}(x_i,y_j).
f_Y(y_j) = \prob(Y=y_j) = \sum_{i=1}^m \prob(A_{i,j}) = \sum_{i=1}^m f_{X,Y}(x_i,y_j).
\]
\qed
\end{proof}

% example: dice
\begin{example}\label{ex:joint:dice}
A fair die is rolled once. Let $\omega$ denote the outcome, and consider the random variables
\[
X(\omega) = \left\{\begin{array}{cl}
	1 & \text{ if $\omega$ is odd,} \\
	2 & \text{ if $\omega$ is even,}
\end{array}\right. 
\quad\mbox{ and }\quad
Y(\omega) = \left\{\begin{array}{cl}
	1 & \text{ if $\omega\leq 3$,} \\
	2 & \text{ if $\omega\geq 4$.}
\end{array}\right.
\]
Find the joint PMF of $X$ and $Y$.
\end{example}

\begin{solution}
\[
\begin{array}{c|cccccc}
\omega 	& 1 & 2 & 3 & 4 & 5 & 6 \\ \hline
X(\omega) 	& 1 & 2 & 1 & 2 & 1 & 2 \\
Y(\omega) 	& 1 & 1 & 1 & 2 & 2 & 2 \\
\end{array}
\]
The joint PMF of $X$ and $Y$ is shown in the following table.
\[
\begin{array}{|c|c|c|} \hline
	& Y=1 	& Y=2 \\ \hline
X=1	& 1/3	& 1/6 \\ \hline
X=2	& 1/6 	& 1/3 \\ \hline
\end{array}
\]
The marginal PMFs are recovered by summing the rows and columns of the table.
\end{solution}

%----------------------------------------------------------------------
\section{Independent random variables}
%----------------------------------------------------------------------
%\bit
%\it 
%Recall that two events $A$ and $B$ are \emph{independent} if the fact that $B$ occurs does not affect the probability that $A$ occurs, i.e.\ if and only if $\prob(A\cap B)=\prob(A)\prob(B)$.
%\it
%Similarly, we say that two random variables $X$ and $Y$ are independent if the value taken by $X$ does not affect the distribution of $Y$.
%\eit
%Let $(\Omega,\prob)$ be a finite probability space. Two random variables $X,Y:\Omega\to\R$ 
Two random variables are said to be \emph{independent} if the value taken by one does not affect the distribution of the other.% (and vice versa).
%\bit
%\it Recall that two events $A$ and $B$ are called \emph{independent} if $\prob(A\cap B)=\prob(A)\prob(B)$.
%\eit



%% defn: independence (general rvs)
%\begin{definition}
%Two random variables $X,Y:\Omega\to\R$ are said to be \emph{independent} if the events 
%\begin{align*}
%\{X\leq x\} & = \{\omega\,:\, X(\omega)\leq x\} \\
%\{Y\leq y\} & = \{\omega\,:\, Y(\omega)\leq y\}
%\end{align*}
%are independent for all $x,y\in\R$.
%\end{definition}

% defn: independence
\begin{definition}
$X$ and $Y$ are said to be \emph{independent} if the events $\{X = x\}$ and $\{Y = y\}$ are independent for all $x,y\in\R$.
%\begin{align*}
%\{X = x\} & = \{\omega\,:\, X(\omega) = x\} \\
%\{Y = y\} & = \{\omega\,:\, Y(\omega) = y\}
%\end{align*}
\end{definition}

%The following is a trivial consequence of the definition.
% lemma: independence => joint = product of marginals.
%If $X$ and $Y$ are independent, their joint PMF is equal to the product of their marginal PMFs:
The joint PMF of two independent random variables is equal to the product of their marginal PMFs:
\begin{lemma}
$X$ and $Y$ are independent if and only if
\[
f_{X,Y}(x,y) = f_X(x)\,f_Y(y) \quad\text{for all}\quad x,y\in\R.
%\prob(X=x,Y=y) = \prob(X=x)\prob(Y=y)\quad\text{for all}\quad x,y\in\R.
\]
\end{lemma}

\begin{proof} 
Recall that two events $A$ and $B$ are said to be independent if $\prob(A\cap B)=\prob(A)\prob(B)$. Thus $X$ and $Y$ are independent if and only if $\prob(X=x, Y=y) = \prob(X=x)\prob(Y=y)$ for all $x,y\in\R$, or equivalently
\[
f_{X,Y}(x,y) = f_X(x)\,f_Y(y) \quad\text{for all}\quad x,y\in\R.
%\prob(X=x,Y=y) = \prob(X=x)\prob(Y=y)\quad\text{for all}\quad x,y\in\R.
\]
Note that if $x\notin\{x_1,x_2,\ldots,x_m\}$ or $y\notin\{y_1,y_2,\ldots,y_n\}$, both sides are equal to zero.
\end{proof} 

%% remark
%\begin{remark}
%To check whether two random variables $X$ and $Y$ are independent, we first calculate the marginal PMFs, then check whether or not 
%$\prob(X=x,Y=y) = \prob(X=x)\prob(Y=y)$ for all $x,y\in\R$.
%%$f_{X,Y}(x,y) = f_X(x)f_Y(y)$.
%\end{remark}

% example
\begin{example}\label{ex:tedious}
Let $X$ and $Y$ be random variables with a joint PMF shown in the following table. 
Find the marginal PMFs of $X$ and $Y$, and decide whether or not $X$ and $Y$ are independent.
\[\begin{array}{|cc|ccc|}\hline
	&	& 		& y 		&			\\ 
	&	& 2		& 3		& 4		\\\hline
	& 1	& 1/12	& 1/6	& 0 		\\
x	& 2	& 1/6	& 0		& 1/3 	\\
	& 3	& 1/12	& 1/6  	& 0		\\ \hline
\end{array}\]
\end{example}

\begin{solution}
The marginal distributions are obtained by summing the rows and columns of the table:
\[\begin{array}{|c|ccc|}\hline
x			& 1		& 2		& 3	\\ \hline
f_X(x)		& 1/4	& 1/2	& 1/4	\\ \hline \hline
y			& 2		& 3		& 4 	\\ \hline			
f_Y(y)		& 1/3	& 1/3	& 1/3	\\ \hline
\end{array}\]
We have (for example) that $\prob(X=2,Y=3)=0$ but $\prob(X=2)\prob(Y=3) = 1/6$, so $X$ and $Y$ are not independent.
\end{solution}

% theorem: functions of rvs
\begin{theorem}
Let $X$ and $Y$ be independent, and let $g,h:\R\to\R$ be any two functions. Then $g(X)$ and $h(Y)$ are also independent.
\end{theorem}
\begin{proof}

Let $\{c_1,c_2,\ldots,c_M\}$ be the range of $g(X)$, let $\{d_1,d_2,\ldots,d_N\}$ be the range of $h(Y)$, and consider the following partitions.
\bit
\it The sets $C_k=\{x_i:g(x_i)=c_k\}$ for $k=1,2,\ldots,M$. 
\par These partition the set $\{x_i:i=1,2,\ldots,m\}$, which is the range of $X$.
\it The sets $D_l=\{y_j:h(y_j)=d_l\}$ for $l=1,2,\ldots,N$. 
\par These partition the set $\{y_j:j=1,2,\ldots,n\}$, which is the range of $Y$.
\it The sets $E_{k,l} = \{(x_i,y_j):g(x_i)=c_k,h(y_j)=d_l\}$ for $k=1,2,\ldots,M$ and $l=1,2,\ldots,N$.
\par These partition the set of pairs $\{(x_i,y_j):i=1:2,\ldots,m ; j=1,2,\ldots,n\}$.
\eit
By construction, $g(x)=c_k$ and $h(y)=d_l$ if and only if $(x,y)\in E_{k,l}$, so by the partition theorem,
\begin{align*}
\prob\big[g(X)=c_k,h(Y)=d_l\big]
%	& = \sum_{\stackss{x,y:}{g(x)=a,h(y)=b}}\prob(X=x,Y=y) \\
	& = \sum_{(x,y)\in E_{k,l}}\prob(X=x,Y=y) \\
	& = \sum_{(x,y)\in E_{k,l}}\prob(X=x)\prob(Y=y)\qquad\text{(by independence),}\\
	& = \sum_{x\in C_k}\prob(X=x) \sum_{y\in D_l}\prob(Y=y) \\
	& = \prob\big[g(X)=c_k\big]\,\prob\big[h(Y)=d_l\big].
\end{align*}
%as required.
%\par

%
%Let $a,b\in\R$. We sum over all pairs $(x_i,y_j)$ for which $g(x_i)=a$ and $h(y_j)=b$.
%\par
%Let $\{c_1,c_2,\ldots,c_M\}$ be the range of $g(X)$, let $\{d_1,d_2,\ldots,d_N\}$ be the range of $h(Y)$, and define
%\bit
%\it $J_k=\{i:g(x_i)=c_k\}$,
%\it $J_l=\{j:h(y_j)=d_l\}$ and 
%\it $J_{k,l} = \{(j,k):g(x_i)=c_k,h(y_j)=d_l\}$.
%\eit
%Then
%\begin{align*}
%\prob\big[g(X)=c_k,h(Y)=d_l\big]
%%	& = \sum_{\stackss{x,y:}{g(x)=a,h(y)=b}}\prob(X=x,Y=y) \\
%	& = \sum_{(i,j)\in J_{k,l}}\prob(X=x_i,Y=y_j) \\
%	& = \sum_{(i,j)\in J_{k,l}}\prob(X=x_i)\prob(Y=y_j)\qquad\text{(by independence),}\\
%	& = \sum_{i\in J_k}\prob(X=x_i) \sum_{j\in J_l}\prob(Y=y_j) \\
%	& = \prob\big[g(X)=c_k\big]\,\prob\big[h(Y)=d_l\big].
%\end{align*}
%\par
%Let 
%\bit
%\it $J(a)=\{x:g(x)=a\}$,
%\it $J(b)=\{y:h(y)=b\}$ and 
%\it $J(a,b) = \{(x,y):g(x)=a,h(y)=b\}$.
%\eit
%Then
%\begin{align*}
%\prob\big[g(X)=a,h(Y)=b\big]
%%	& = \sum_{\stackss{x,y:}{g(x)=a,h(y)=b}}\prob(X=x,Y=y) \\
%	& = \sum_{(x,y)\in J(a,b)}\prob(X=x,Y=y) \\
%	& = \sum_{(x,y)\in J(a,b)}\prob(X=x)\prob(Y=y) \qquad\text{(by independence),}\\
%	& = \sum_{x\in J(a)}\prob(X=x) \sum_{y\in J(b)}\prob(Y=y) \\
%	& = \prob\big[g(X)=a\big]\,\prob\big[h(Y)=b\big].
%\end{align*}
\end{proof}

%----------------------------------------------------------------------
\newpage
\section{Exercises}
\input{ex12_joint_distributions}
%----------------------------------------------------------------------

%======================================================================
\endinput
%======================================================================
