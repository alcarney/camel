% !TEX root = main.tex
%======================================================================
\chapter{Random Variables}\label{chap:rvs}
%======================================================================


%----------------------------------------------------------------------
\section{Random variables}
%----------------------------------------------------------------------
We are not always directly interested in the outcome of a random experiment, but rather in some \emph{consequence} of the outcome. For example, a gambler is often more interested in his losses than in the outcomes of the individual games which led to them.

% example
\begin{example}\label{ex:rv1}
A fair coin is tossed twice. Let $\Omega=\{HH,HT,TH,TT\}$ denote the sample space.
\ben
\it The number of heads observed is a function $X:\Omega\to\R$, defined by
\[
X(HH)=2,\quad X(HT) = X(TH) = 1,\quad X(TT)=0.
\]
\it Suppose we bet $\pounds 1$ on the outcome of this random experiment, and stand to win $\pounds 4$ if two heads occur, but otherwise lose our stake. Our winnings can be represented by a function $Y:\Omega\to\R$, defined by 
\[
Y(HH)=4,\quad Y(HT) = Y(TH) = Y(TT) = -1.
\]
\een
\end{example}

% blah-blah
\bit
\it A random variable $X$ associates a real number $X(\omega)$ with each outcome $\omega\in\Omega$.
\it Random variables are often used to pick out particular features of an experiment that are of interest.
\eit

% definition: rv
\begin{definition}
Let $(\Omega,\mathcal{F},\prob)$ be a probability space. A \emph{random variable} on $(\Omega,\mathcal{F})$ is a function,
\[
\begin{array}{rccl}
X: 	& \Omega & \longrightarrow 		& \R \\
	& \omega & \mapsto	& X(\omega)
\end{array}
\]	
with the property that $\{\omega:X(\omega)\leq x\} \in \mathcal{F}$ for every $x\in\R$. 
\end{definition}

\subsubsection*{Discussion}
\bit
\it The set $\{\omega:X(\omega)\leq x\}$ contains those outcomes that are mapped by $X$ into the interval $(-\infty, x]$.
\it Why do we insist on the condition that these sets are included in the field of events $\mathcal{F}$?
\it Recall the definition of a probability measure:
\[
\begin{array}{rccl}
	\prob:	& \mathcal{F}	& \to	& [0,1] \\[1ex]
			& A				& \mapsto	& \prob(A)
\end{array}
\]
\it The condition ensures that the events $\{\omega:X(\omega)\leq x\}$ have a well-defined probability.
\it But why do we need this to be the case? 
\eit

%----------------------------------------------------------------------
\subsection{Notational conventions}
%----------------------------------------------------------------------
% remark
\bit
\it Random variables are denoted by upper-case letters such as $X$ and $Y$.
\it Values taken by random variables are denoted by the corresponding lower-case letters, such as $x$ and $y$.
\eit

Let $B$ be any subset of the real numbers. We define the following abbreviations:
%\begin{align*}
%\{X\in B\}		& := \{\omega: X(\omega) \in B\}. \\
%				& \qquad\text{This is the event that $X$ takes a value in the set $B$.} \\
%\prob(X\in B)	& := \prob\big(\{\omega: X(\omega) \in B\} \\
%				& \qquad\text{This is the probability that $X$ takes a value in the set $B$.}
%\end{align*}
\ben
\it $\{X\in B\}$ for the event $\{\omega: X(\omega) \in B\}$.
	\bit
	\it This is the event that $X$ takes a value in the set $B$.
	\eit
\it $\prob(X\in B)$ for the probability $\prob\big(\{\omega: X(\omega) \in B\}$.
	\bit
	\it This is the probability that $X$ takes a value in the set $B$.
	\eit
\it $\{X = x\}$ for the event $\{\omega:X(\omega)=x\}$.
	\bit
	\it This is the event that $X$ takes the value $x$.
	\eit
\it $\prob(X = x)$ for the probability $\prob\big(\{\omega: X(\omega) = x\}$.
	\bit
	\it This is the probability that $X$ takes the value $x$.
	\eit
\it $\{X\leq x\}$ for the event $\{\omega:X(\omega)\leq x\}$
	\bit
	\it This is the event that $X$ takes a value at most equal to $x$.
	\eit
\it $\prob(X \leq x)$ for the probability $\prob\big(\{\omega: X(\omega) \leq x\}$.
	\bit
	\it This is the probability that $X$ takes a value at most equal to $x$.
	\eit
\een

%We will also use the following notation:
%\begin{align*}
%\{X = x\}	& := \{\omega:X(\omega)=x\}  \quad\text{for the event that $X$ takes the value $x$, and} \\
%\{X\leq x\}	& := \{\omega:X(\omega)\leq x\} \quad\text{for the event that $X$ takes a value at most equal to $x$.}
%\end{align*}

%% example
%\begin{example}
%Two fair six-sided dice are rolled independently. Let $\Omega$ denote the sample space of this random experiment, and let $\mathcal{F}$ be a field of events over $\Omega$. The sample space can be represented by
%\[
%\Omega=\{(a,b):1\leq a,b\leq 6\}.
%\]
%Let $X:\Omega\to\R$ be the total number obtained:
%\[
%\begin{array}{cccc}
%X:	& \Omega		& \to 		& \R \\
%	& (a,b) 		& \mapsto	& a+b
%\end{array}
%\]
%$X$ is a random variable on $(\Omega,\mathcal{F})$ provided that $\{\omega:X(\omega)\leq x\} \in \mathcal{F}$ for every $x\in\R$. In particular, we require that 
%\begin{align*}
%\{\omega:X(\omega)\leq 2\}	& = \{(1,1)\} \in \mathcal{F}, \\
%\{\omega:X(\omega)\leq 3\}	& = \{(1,1),(1,2),(2,1)\} \in \mathcal{F}, \\
%\{\omega:X(\omega)\leq 4\}	& = \{(1,1),(1,2),(2,1),(1,3),(2,2),(3,1)\} \in \mathcal{F}, \quad\text{and so on.}
%\end{align*}
%\end{example}


%----------------------------------------------------------------------
\subsection{Indicator variables}
%----------------------------------------------------------------------
% defn: indicator
\begin{definition}
The \emph{indicator variable} $I_A:\Omega\to\R$ of an event $A$ is defined by
\[
I_A(\omega) = 
	\left\{
	\begin{array}{ll}
	1 & \text{if } \omega\in A, \\
	0 & \text{if } \omega\notin A.
	\end{array}
	\right.
\]
\end{definition}

% theorem: properties of indicator variables
\begin{theorem}
Let $A$ and $B$ be any two events. Then
\ben
\it $I_{A^c} = 1 - I_A$
\it $I_{A\cap B} = I_A I_B$
\it $I_{A\cup B} = I_A + I_B - I_{A\cap B}$
\een
\end{theorem}

% proof
\begin{proof}
\hideoff
Exercise. 
\par
Note that for two functions to be equal, they must have the same domain, and be equal at every point of that domain, so for the first part we need to show that $I_{A^c}(\omega) = 1-I_A(\omega)$ for every $\omega\in\Omega$, and similarly for parts (2) and (3).
\hideon
\end{proof}

\newpage

%----------------------------------------------------------------------
\subsection{Simple random variables}
%----------------------------------------------------------------------
% defn: simple rv
\begin{definition}
A \emph{simple random variable} is one that takes only finitely many values.
\end{definition}

Let $X:\Omega\to\R$ be a simple random variable, let $\{x_1,x_2,\ldots,x_n\}$ be the range of values that $X$ can take, and consider the finite partition $\{A_1,A_2,\ldots,A_n\}$ of the sample space, defined by
\[
A_i = \{\omega:X(\omega) = x_i\}.
\]
Then $X$ can be written as
\[
X(\omega)= \sum_{i=1}^n x_i I_{A_i}(\omega)
\]
where $I_{A_i}$ is the indicator variable of the event $A_i$.


% example: simple rv
\begin{example}
The random variables $X$ and $Y$ of example~\ref{ex:rv1} are both simple random variables:
\end{example}
\begin{hidebox}
\ben
\it Let $A_1=\{TT\}$, $A_2=\{TH,HT\}$, $A_3=\{HH\}$, and let $x_1=0$, $x_2=1$, $x_3=2$. Then
\[
X(\omega) = \sum_{i=1}^3 x_i I_{A_i}(\omega) = I_{A_2}(\omega) + 2I_{A_3}(\omega).
\]
\it Let $B_1=\{TT, TH, HT\}$, $B_2 = \{HH\}$, and let $y_1=-1$, $y_1=4$. Then 
\[
Y(\omega) = \sum_{i=1}^2 x_i I_{A_i}(\omega) = -I_{B_1}(\omega) + 4I_{B_2}(\omega).
\]
\een
\end{hidebox}

%----------------------------------------------------------------------
\section{Cumulative distribution functions (CDFs)}
%----------------------------------------------------------------------
A random variable is completely described by its CDF:

% definition: cdf
\begin{definition}
Let $(\Omega,\mathcal{F},\prob)$ be a probability space, and let $X:\Omega\to\R$ be a random variable on $(\Omega,\mathcal{F})$. The \emph{cumulative distribution function} (CDF) of $X$ is defined to be the function
\[
\begin{array}{cccl}
F:	& \mathbb{R}	& \longrightarrow	& [0,1] \\
	& x 			& \mapsto			& \prob(X\leq x) 
\end{array}
\]
\end{definition}

\begin{remark}
\bit
\it Recall that $\prob(X\leq x)$ is shorthand notation for $\prob\big(\{\omega:X(\omega) \leq x\}\big)$.
\it To ensure that this probability is well-defined, we need that $\{\omega:X(\omega) \leq x\}\in\mathcal{F}$.
\it The fact that $X$ is a random variable on $(\Omega,\mathcal{F})$ guarantees that this is the case.
\eit
\end{remark}

% example (cont)
\begin{example}
The CDFs of the random variables $X$ and $Y$ of example~\ref{ex:rv1} are given by
\[
F_X(x) = \begin{cases}
0	& x < 0, \\
1/4	& 0 \leq x < 1, \\
3/4	& 1 \leq x < 2, \\
1	& x \geq 2,
\end{cases}
\qquad\text{and}\qquad
F_Y(Y) = \begin{cases}
0	& y < -1, \\
3/4	& -1 \leq y < 4, \\
1	& y \geq 4,
\end{cases}
%\quad\text{respectively.}
\]
respectively.
\end{example}


%----------------------------------------------------------------------
\section{Probability mass functions (PMFs)}
%----------------------------------------------------------------------
% definition: PMF
\begin{definition}\label{def:pmf_of_simple_rv}
Let $X:\Omega\to\R$ be a simple random variable. The \emph{probability mass function} (PMF) of $X$ is the function
\[
\begin{array}{cccl}
f:	& \mathbb{R}	& \longrightarrow	& [0,1] \\
	& x 			& \mapsto			& \prob(X = x).
\end{array}
\]
\end{definition}

%\bit
%\it Recall that $\prob(X=x)$ is shorthand notation for the probability of the event $\{\omega:X(\omega) = x\}$.
%\eit

% example (cont)
\begin{example}
In example~\ref{ex:rv1}, because the coin is \emph{fair} the PMFs of $X$ and $Y$ are, respectively,
\[
f_X(x) = 
\left\{\begin{array}{ll}
1/4	& \text{if }x = 0, \\
1/2	& \text{if }x = 1, \\
1/4 & \text{if }x = 2, \\
0	& \text{otherwise,}
\end{array}\right.
\text{\qquad and\qquad}
f_Y(y) = 
\left\{\begin{array}{ll}
3/4	& \text{if }y = -1, \\
1/4	& \text{if }y = 4. \\
0	& \text{otherwise.}
\end{array}\right.
\]
\end{example}

%----------------------------------------------------------------------
\section{Finite probability spaces}
%----------------------------------------------------------------------
Let $\Omega$ be a finite sample space, let $p:\Omega\to [0,1]$ be a probability mass function on $\Omega$, and let $\prob$ be the associated probability measure on the power set of $\Omega$. We often refer to the pair $(\Omega,\prob)$ as a \emph{finite probability space}, without explicit reference to the fact that we take our field of events to be the power set $\mathcal{P}(\Omega)$.

%\begin{definition}
%The pair $(\Omega,\prob)$ is called a \emph{finite probability space}, with the implicit assumption that our field of events is taken to be the power set $\mathcal{P}(\Omega)$.
%\end{definition}

\begin{lemma}
If $\Omega$ is a finite sample space, any function $X:\Omega\to\R$ is a simple random variable on $\Omega$.
\end{lemma}

\begin{proof}
\bit
\it $X$ is a random variable because for any $x\in\R$, the condition $\{\omega:X(\omega)\leq x\} \in \mathcal{P}(\Omega)$ is automatically satisfied, because the power set contains \emph{all} subsets of $\Omega$.
\it $X$ is a simple random variable because its domain is finite, so it can take at most finitely many values.
\eit
\end{proof}

Let $(\Omega,\prob)$ be a finite probability space, let $p:\Omega\to [0,1]$ be the associated probability mass function, let $X:\Omega\to\R$ be a random variable on $\Omega$, and let $\{x_1,x_2,\ldots,x_n\}$ be the range of values taken by $X$.
%, and consider the events
%\[
%A_i = \{\omega:X(\omega) = x_i\} \quad\text{for $i=1,2,\ldots,n$}.
%\]
Then the PMF of $X$ can be written as
\[
\prob(X=x_i) = \sum_{\omega\in A_i} p(\omega) \quad\text{where}\quad A_i = \{\omega:X(\omega) = x_i\} \quad\text{for $i=1,2,\ldots,n$}.
\]
Note that the events $A_1,A_2,\ldots,A_n$ form a partition of $\Omega$.

\newpage

% exmple: binomial
\begin{example}\label{ex:binomial}
A biased coin is independently tossed three times, each time having probability $p$ of showing heads and $q=1-p$ of showing tails. Let $X$ be the number of heads that occur. Find the PMF of $X$.
\end{example}

\begin{solution}
The sample space is $\Omega=\{TTT,HTT,THT,TTH,THH,HTH,HHT,HHH\}$.
\par
By independence, the associated probability mass function $p(\omega)$ is 
\[
\begin{array}{|c|cccccccc|} \hline
\omega 		& TTT & HTT & THT & TTH & THH & HTH & HHT & HHH \\ \hline
p(\omega) 	& q^3 & pq^2 & pq^2 & pq^2 & p^2q & p^2q & p^2q & p^3 \\  \hline
\end{array}
\]
The number of heads is a random variable $X:\Omega\to\R$, given by
\[
\begin{array}{|c|cccccccc|} \hline
\omega 		& TTT & HTT & THT & TTH & THH & HTH & HHT & HHH \\ \hline
X(\omega) 	& 0 & 1 & 1 & 1 & 2 & 2 & 2 & 3 \\ \hline
\end{array}
\]

$X$ takes values in the range $\{0,1,2,3\}$, with probabilities
\bit
\it $\prob(X=0) = \prob(\{TTT\}) = q^3$,
\it $\prob(X=1) = \prob(\{HTT,THT,TTH\}) = 3pq^2$,
\it $\prob(X=2) = \prob(\{THH,HTH,HHT\}) = 3p^2q$,
\it $\prob(X=3) = \prob(\{HHH\}) = p^3$.
\eit

The PMF of $X$ can be represented in tabular form:
\[
\begin{array}{|c|cccc|} \hline
x 		& 0 & 1 & 2 & 3 \\ \hline
f(x)	& q^3 & 3pq^2 & 3p^2q & p^3 \\ \hline
\end{array}
\]
\end{solution}

%----------------------------------------------------------------------
\newpage
\section{Exercises}
\input{ex08_random_variables}
%----------------------------------------------------------------------

%======================================================================
\endinput
%======================================================================
