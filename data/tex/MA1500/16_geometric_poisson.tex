% !TEX root = main.tex
%======================================================================
\chapter{The Geometric and Poisson Distributions}\label{chap:geo-poisson}
%======================================================================

In this lecture we look at some well-known distributions which take values in the non-negative integers.

%----------------------------------------------------------------------
\section{Geometric distribution}
%----------------------------------------------------------------------
The geometric distribution on $\{1,2,3,\ldots\}$ is the distribution of the number of trials up to and including the first success in a sequence of independent Bernoulli trials. 

\begin{center}
\begin{tabular}{ll}\hline
Notation			& $X\sim\text{Geometric}(p)$ \\
Parameter(s)	& $p \in[0,1]$ \quad (probability of success) \\
Range				& $\{1,2,\ldots\}$ \\
PMF				& $fk) = (1-p)^{k-1} p$\\ 
CDF				& $F(k) = 1-(1-p)^k$\\ \hline
\end{tabular}
\end{center}

% mean and variance
\begin{lemma}
If $X\sim\text{Geometric}(p)$, then 
\[
\expe(X)= \frac{1}{p} \quad\text{and}\quad \var(X) = \frac{1-p}{p^2}.
\]
\end{lemma}

\begin{proof}
We make use of the geometric series
\[
\sum_{k=0}^\infty r^k = 1 + r + r^2 + \ldots = \frac{1}{1-r} \qquad\text{for }|r|<1
\]
Let $q = 1-p$. The PDF of $X\sim\text{Geometric}(p)$ is
\[
f(k)= q^{k-1}p.
\] 

% mean
\begin{align*}
\expe(X) = \sum_{k=1}^{\infty} k \prob(X=k)
	& = \sum_{k=1}^\infty k q^{k-1} p \\
	& = \sum_{j=0}^\infty (j+1)q^j p 		\qquad \qquad\text{(where we have set $j=k-1$)} \\
	& = q\sum_{j=0}^\infty j q^{j-1} p + p\sum_{j=0}^\infty q^j \\
	& = q\sum_{j=1}^\infty j\,\prob(X=j) + \frac{p}{1-q} \\
	& = q\expe(X) + 1.
\end{align*}
Solving this equation, we get $\displaystyle \expe(X) = \frac{1}{p}$.

% mean square
\begin{align*}
\expe(X^2) = \sum_{k=1}^\infty k^2 q^{k-1}p 
	& = \sum_{j=0}^\infty (j+1)^2 q^j p \qquad \qquad\text{(where we have set $j=k-1$)} \\
	& = \sum_{j=0}^\infty (j^2+2j+1) q^j p \\
	& = \sum_{j=0}^\infty j^2\,q^j p + 2\sum_{j=0}^\infty j\,q^j p + \sum_{j=0}^\infty q^j p \\
	& = q\sum_{j=1}^\infty j^2\,q^{j-1} p \ +\  2q\sum_{j=1}^\infty j\,q^{j-1} p \ +\  p\sum_{j=0}^\infty q^j \\
	& = q\sum_{j=1}^\infty j^2\,\prob(X=j) \ +\  2q\sum_{j=1}^\infty j\,\prob(X=j) \ +\  \frac{p}{1-q} \\
	& = q\expe(X^2) + 2q\expe(X) + 1.
\end{align*}

Substituting for $\expe(X)$,
\begin{align*}
\expe(X^2)	& = \frac{2q\expe(X)+1}{1-q} = \frac{(2-p)}{p^2}, \text{ so}\\
\var(X) 		& = \expe(X^2)-\expe(X)^2 = \frac{2-p}{p^2} \ -\  \frac{1}{p^2} = \frac{1-p}{p^2}.
\end{align*}
as required.
\end{proof}

% example
\begin{example}
Let $p=0.001$ be the probability that a certain type of lightbulb fails on any given day. 
\ben
\it What is the expected lifetime of this type of lightbulb?
\it What is the probability that a lightbulb lasts for more than 30 days?
\een
\end{example}

\begin{solution}
Let $X$ denote the lifetime of a bulb. Then $X\sim\text{Geometric}(p)$ where $p=0.001$. 
\ben
\it $\expe(X) = \displaystyle\frac{1}{p} = 1000\text{ days}$.
\it The distribution function of $X$ is 
\[
\prob(X\leq k) = 1-(1-p)^k,
\]
so $\prob(X> k) = (1-p)^k$ and hence
\[
\prob(X > 30) = (1 - 0.001)^{30} = 0.999^{30} = 0.9704.
\]
\een
\end{solution}

\begin{remark}
Let $X$ be the number of failures before the first success in a sequence of independent Bernoulli trials (i.e. the number of trials up to but \emph{not} including the first success). Confusingly, the distribution of $X$ is also called the geometric distribution. In this case, 
\bit
\it $X$ takes values in the non-negative integers $\{0,1,2,\ldots\}$, 
\it its PMF is $f(k) = (1-p)^{k}p$, and
\it $\expe(X)=\displaystyle\frac{1-p}{p}$ and $\var(X)=\displaystyle\frac{1-p}{p^2}$ (as before).
\eit
\end{remark}

%%----------------------------------------------------------------------
%\section{Negative binomial distribution}
%%----------------------------------------------------------------------
%The negative binomial distribution is the distribution of the number of trials required to get a fixed number of successes in a sequence of independent Bernoulli trials. 
%
%\begin{center}
%\begin{tabular}{ll}\hline
%Notation			& $X\sim\text{NegBin}(r,p)$ \\
%Parameter(s)		& $p \in[0,1]$ \quad (probability of success) \\
%				& $r \in\N$ \qquad (stopping parameter) \\
%Range			& $\{r,r+1,r+2\ldots\}$ \\[1ex]
%PMF				& $\prob(X=k) = \displaystyle \binom{k-1}{r-1}(1-p)^{k-r}p^{r}$\\[2ex] \hline
%%CDF				& $F(k) = \displaystyle \sum_{j=r}^k \binom{j-1}{r-1}(1-p)^{j-r}p^{r}$\\[2ex] \hline
%\end{tabular}
%\end{center}
%
%If $X\sim\text{NegBin}(r,p)$, then $X$ can be written as the sum of $r$ independent geometric variables,
%\[
%X = \sum_{i=1}^r Y_i \quad\text{where}\quad Y_i\sim\text{Geometric}(p)\quad\text{for each}\quad i=1,2,\ldots,r.
%\]
%
%\begin{remark}[Waiting times]
%As with the geometric and Poisson distributions, the negative binomial can be used to model situations in which we are waiting for the occurence of an event (namely, the event that a specified number of successes occur).
%\end{remark}
%
%% mean and variance
%\subsubsection*{Mean and variance}
%
%\begin{hidebox}
%Let $Y_1,\ldots,Y_r$ be independent with $Y_i\sim\text{Geometric}(p)$, and let $X = \sum_{i=1}^r Y_i$.
%\par
%Then $X\sim\text{NegBin}(r,p)$, and by the linearity of expectation,
%\[
%\expe(X) = \expe(Y_1)+\expe(Y_2)+\ldots+\expe(Y_r) = \frac{r}{p}
%\]
%and because the trials are independent,
%\[
%\var(X) = \var(Y_1)+\var(Y_2)+\ldots+\var(Y_r) = \frac{r(1-p)}{p^2}
%\]
%\vspace*{-4ex}
%\end{hidebox}
%
%% example
%\begin{example}
%Biological populations are often sampled using a technique called \emph{inverse binomial sampling}. Let $p$ be the proportion of individuals having a particular characteristic. We sample from the population until we obtain $r$ such individuals. The total number of individuals selected has negative binomial distribution with parameters $r$ and $p$. 
%
%Suppose that a biologist wishes to obtain a sample of $100$ fruit flies having a certain genetic trait that occurs at a rate of one in every twenty fruit files in the population. What is the probability that the biologist has to examine at least $k$ flies?
%\end{example}
%
%\begin{solution}
%Let $X\sim\text{NegBin}(r,p)$ with $r=100$ and $p=0.05$. Then
%\[
%\prob(X\geq k) = 1 - \prob(X < k) = 1 - \sum_{j=100}^{k-1} \binom{j-1}{99} (0.95)^{j-100}(0.05)^{100} 
%\]
%\end{solution}
%
%% remark: statistical tables
%\begin{remark}[Statistical tables]
%If $r$ is large and/or $p$ is small, it is not easy to compute these probabilities. Instead, we can use \emph{statistical tables} find (approximate) values of $\prob(X\geq k)$ for various values of $k$, and various values of the distribution parameters $r$ and $p$. Statistical tables are available for many useful probability distributions. 
%\end{remark}


%----------------------------------------------------------------------
\section{Poisson distribution}
%----------------------------------------------------------------------
Consider an event that occurs repeatedly at random times. If the time interval between successive occurences are independent of each other, the number of occurences per unit time can be modelled by a Poisson distribution, named after Sim\'{e}on Poisson (1781-1840). This distribution has a single parameter, called the \emph{rate parameter}, which is the mean number of occurences per unit time.

\begin{center}
\begin{tabular}{ll}\hline
Notation			& $X\sim\text{Poisson}(\lambda)$ \\
Parameter(s)	& $\lambda > 0$\quad (rate parameter) \\
Range				& $\{0,1,2,\ldots\ldots\}$ \\
PMF				& $f(k) = \displaystyle\frac{\lambda^k}{k!}e^{-\lambda}$ \\[2ex] \hline
\end{tabular}
\end{center}

% mean and variance
\begin{lemma}
If $X\sim\text{Poisson}(\lambda)$, then 
\[
\expe(X)= \lambda \quad\text{and}\quad \var(X) = \lambda.
\]
\end{lemma}

\begin{proof}
\begin{align*}
\expe(X) = \sum_{k=0}^\infty k \prob(X=k)
	& = e^{-\lambda}\sum_{k=0}^\infty \frac{k\lambda^k}{k!} \\
	& = e^{-\lambda}\sum_{k=1}^\infty \frac{k\lambda^k}{k!} \qquad\qquad\text{because the term for $k=0$ is zero,}\\
	& = \lambda e^{-\lambda}\sum_{k=1}^\infty \frac{\lambda^{k-1}}{(k-1)!} \qquad\text{because $k!=k(k-1)!$,}\\
	& = \lambda e^{-\lambda}\sum_{j=0}^\infty \frac{\lambda^j}{j!} \qquad\qquad\text{where we have set $j=k-1$,}\\
	& = \lambda \quad\qquad\qquad\qquad\qquad\text{because}\quad \sum_{j=0}^\infty \frac{\lambda^j}{j!}= e^{\lambda}.
\end{align*}

Similarly,
\begin{align*}
\expe(X^2) = \sum_{k=0}^\infty k^2 \prob(X=k)
	& = e^{-\lambda}\sum_{k=0}^\infty \frac{k^2\lambda^k}{k!} \\
	& = e^{-\lambda}\sum_{k=1}^\infty \frac{k^2\lambda^k}{k!} \quad\qquad\qquad\qquad\text{because the term for $k=0$ is zero,}\\
	& = \lambda e^{-\lambda}\sum_{k=1}^\infty \frac{k\lambda^{k-1}}{(k-1)!} \quad\qquad\qquad\text{because $k!=k(k-1)!$,}\\
	& = \lambda e^{-\lambda}\sum_{j=0}^\infty \frac{(j+1)\lambda^j}{j!} \qquad\qquad\text{where we have set $j=k-1$,}\\
	& = \lambda e^{-\lambda}\left[\sum_{j=0}^\infty \frac{j\lambda^j}{j!} + \sum_{j=0}^\infty \frac{\lambda^j}{j!}\right]\\
	& = \lambda e^{-\lambda}\big[\lambda e^{\lambda} + e^{\lambda}\big] \quad\qquad\qquad\text{because $e^{-\lambda}\sum_{j=0}^\infty \frac{j\lambda^j}{j!} = \expe(X) = \lambda$,}\\
	& = \lambda(\lambda+1).
\end{align*}
Thus
\[
\var(X) = \expe(X^2)-\expe(X)^2 = \lambda(\lambda+1)-\lambda^2 = \lambda.
\]
\end{proof}
% example
\begin{example}
A call centre receives an average of five calls every three minutes. What is the probability that there will be
\ben
\it no calls in the next minute, and
\it at least two calls in the next minute?
\een
\end{example}
\begin{solution}
Let $X$ be the number of calls received in any given minute. The average number of calls per minute is $\lambda=\frac{5}{3}$. If we model $X$ by a Poisson distribution rate parameter $\lambda$, then the probability that $k$ calls are received in any given minute is
\[
\prob(X=k) = \frac{1}{k!}\left(\frac{5}{3}\right)^ke^{-5/3}
\]
\ben
\it $\prob(X=0) = e^{-5/3} = 0.189$.
\it $\prob(X\geq 2) = 1 - \prob(X=0) - \prob(X=1) = 1 - e^{-5/3} - \displaystyle\frac{5}{3}e^{-5/3} = 0.496$.
\een
\end{solution}

%----------------------------------------------------------------------
\section{The law of rare events}
%----------------------------------------------------------------------
The \emph{law of rare events}, also known as the \emph{Poisson limit theorem}, shows that the binomial distribution can be approximated by a Poisson distribution when the number of trials ($n$) is large, and the probability of success ($p$) is small.

% theorem
\begin{theorem}[The law of rare events]
Let $X\sim\text{Binomial}(n,p)$, and suppose that $p\to 0$ as $n\to\infty$ in such a way that the product $\lambda=np$ remains constant. Then 
\[
\prob(X=k)\to \frac{\lambda^k}{k!}e^{-\lambda} \quad\text{as}\quad n\to\infty.
\]
\end{theorem}

% proof
\begin{proof}
To prove the theorem, we need the fact that for any $c\in\R$,
\[
\left(1-\frac{c}{n}\right)^n \to e^{-c}\quad\text{as}\quad n\to\infty
\]
Let $\lambda=np$.
\begin{align*}
\prob(X=k) = \binom{n}{k}p^k(1-p)^k 
	& = \frac{n!}{(n-k)!k!} p^k (1-p)^{n-k} \\
	& = \frac{n!}{(n-k)!k!} \left(\frac{\lambda}{n}\right)^k \left(1-\frac{\lambda}{n}\right)^{n-k} \\
	& =  \frac{n!}{(n-k)!n^k}\left(\frac{\lambda^k}{k!}\right) \left(1-\frac{\lambda}{n}\right)^n \left(1-\frac{\lambda}{n}\right)^{-k}
\end{align*}
Now,
\begin{align*}
\frac{n!}{(n-k)!n^k} 
	& = \frac{n(n-1)(n-2)\ldots(n-k+1)}{n^k} \\
	& = 1\cdot\left(1-\frac{1}{n}\right)\cdot\left(1-\frac{2}{n}\right)\cdots\left(1-\frac{k-1}{n}\right) \\
	& \to 1 \quad\text{as}\quad n\to\infty 
\end{align*}

Furthermore,
\[
\left(1-\frac{\lambda}{n}\right)^n \to e^{-\lambda} \quad\text{and}\quad \left(1-\frac{\lambda}{n}\right)^{-k} \to 1 \quad\text{as}\quad n\to\infty.
\]

Thus we have that 
\[
\prob(X=k) = \binom{n}{k}p^k(1-p)^k \to \frac{\lambda^k}{k!} e^{-\lambda}\quad\text{as}\quad n\to\infty.
\]
which is the PDF of a $\text{Poisson}(\lambda)$ random variable.
\end{proof}

% example: typist
\begin{example}
On average, a typist makes one error in every 500 words. A typical page contains 300 words. What is the probability that there will be no more than two errors in five pages?
\end{example}

\begin{solution}
Assume that typing a single word incorrectly is a Bernoulli trial with probability of `success' equal to $\frac{1}{500}$, and that whether any given word is typed incorrectly is independent of any other word being typed incorrectly.

Let $X$ be the number of errors in five pages, or 1500 words. 

Then $X\sim\text{Binomial}\left(1500,\frac{1}{500}\right)$, so
\[
P(X\leq 2)
	= \sum_{k=0}^2\binom{1500}{k}\left(\frac{1}{500}\right)^k\left(\frac{499}{500}\right)^{1500-k} = 0.4230.
\]
Using the Poisson approximation with $\lambda=1500\times \frac{1}{500} = 3$,
\[
\prob(X\leq 2) \approx e^{-3}\left(1 + 3 + \frac{3^2}{2}\right) = 0.4232.
\]
\end{solution}


%----------------------------------------------------------------------
\newpage
\section{Exercises}
\input{ex16_geometric_poisson}
%----------------------------------------------------------------------

%======================================================================
\endinput
%======================================================================
