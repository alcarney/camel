% !TEX root = main.tex
%======================================================================
\chapter{Conditional Distributions}\label{chap:cond-dist}
%======================================================================

Let $X$ and $Y$ be simple random variables, let $\{x_1,x_2,\ldots,x_m\}$ be the range of $X$, and let $\{y_1,y_2,\ldots,y_n\}$ be the range of $Y$.

%----------------------------------------------------------------------
\section{Conditional distributions}
%----------------------------------------------------------------------
Let $A$ and $B$ be two events. Recall that the conditional probability of $A$ given $B$ is
\[
\prob(A|B) = \frac{\prob(A\cap B)}{\prob(B)}
\]
This idea extends to random variables. 


% defn: conditional cdf/pmf
\begin{definition}
Let $x\in\R$ be a fixed number, and suppose that $\prob(X=x)>0$.
\ben
\it The \emph{conditional CDF} of $Y$ given $X=x$ is the function
\[
\begin{array}{cccc}
F_{Y|X}	:	& \R	& \to		& [0,1] \\[1ex]
			& y & \mapsto	& \prob(Y\leq y \,|\, X=x).
\end{array}
\]
\it The \emph{conditional PMF} of $Y$ given $X=x$ is the function
\[
\begin{array}{cccc}
f_{Y|X}	:	& \R		& \to		& [0,1] \\[1ex]
			& y 		& \mapsto	& \prob(Y=y \,|\, X=x).
\end{array}
\]
\een
\end{definition}

%The conditional probability that event $\{Y=y\}$ occurs given that event $\{X=x\}$ occurs is:
%\[
%\prob(Y=y\,|\,X=x) = \displaystyle\frac{\prob(X=x,Y=y)}{\prob(X=x)}.
%\]

% lemma 
\begin{lemma}\label{lem:cond-pmf}
The conditional PMF of $Y$ given $X=x$ can be written as 
\[
f_{Y|X}(y|x) = \displaystyle \frac{f_{X,Y}(x,y)}{f_X(x)}.
\]
where $f_{X,Y}$ is the joint PMF of $X$ and $Y$, and $f_X$ is the marginal PMF of $X$.
\end{lemma}

\begin{proof}
\[
f_{Y|X}(y|x) = \prob(Y=y\,|\,X=x) = \frac{\prob(X=x,Y=y)}{\prob(X=x)} = \frac{f_{X,Y}(x,y)}{f_X(x)}.
\]
\end{proof}

\begin{remark}[Independence]
Recall that $X$ and $Y$ are independent if and only if 
\[
f_{X,Y}(x,y)=f_X(x)f_Y(y) \quad\text{for all}\quad x,y\in\R.
\]
Hence by Lemma~\ref{lem:cond-pmf}, $X$ and $Y$ are independent if and only if 
\[
f_{Y|X}(y|x) = f_Y(y) \quad\text{for all}\quad x,y\in\R.
\]
where $f_Y$ is the marginal PMF of $Y$. Thus if $X$ and $Y$ are independent, the value taken by $X$ does not affect the distribution of $Y$.
\end{remark}

%The conditional probability that event $\{Y=y\}$ occurs given that event $\{X=x\}$ occurs is:
%\[
%\prob(Y=y\,|\,X=x) = \displaystyle\frac{\prob(X=x,Y=y)}{\prob(X=x)}.
%\]
%\bit
%\it Let $f_{X,Y}(x,y)$ be the joint PMF of $X$ and $Y$.
%\it Let $f_X(x)$ and $f_Y(y)$ be the marginal PMFs of $X$ and $Y$ respectively.
%\it The conditional PMF of $Y$ given $X=x$ can be written as 
%\[
%f_{Y|X}(y|x) = \displaystyle \frac{p(x,y)}{f_X(x)}.
%\]
%\it $X$ and $Y$ are independent if and only if $f_{X,Y}(x,y)=f_X(x)f_Y(y)$ for all $x,y\in\R$.
%\it Hence $X$ and $Y$ are independent if and only if $f_{Y|X}(y|x) = f_Y(y)$ for all $x,y\in\R$.
%\eit

% tedious example
\begin{example}\label{ex:cond-dist-tedious}
Let $X$ and $Y$ be random variables with joint PMF shown in the following table. 
\[
\begin{array}{|cc|ccc|}\hline
	&       &       & y     &   \\
    &       & 2     & 3     & 4 \\ \hline
	& 1		& 1/12	& 1/6	& 0 		\\ 
x	& 2		& 1/6	& 0		& 1/3 	\\ 
	& 3		& 1/12	& 1/6  	& 0		\\ \hline
\end{array}
\]
%\[\begin{array}{|c|c|c|c|}\hline
%	& Y=2	& Y=3	& Y=4	\\ \hline
%X=1	& 1/12	& 1/6	& 0 		\\ \hline
%X=2	& 1/6	& 0		& 1/3 	\\ \hline
%X=3	& 1/12	& 1/6  	& 0		\\ \hline
%\end{array}\]
Find the conditional distribution of $Y$ given that (i) $X=1$, (ii) $X=2$, (iii) $X=3$.
\end{example}

\begin{solution}
 The conditional distributions are obtained by re-scaling the rows of the table.
%\[\begin{array}{|cc|c|c|c|}\hline
%				&			& Y=2	& Y=3	& Y=4	\\ \hline 
%f_{Y|X}(y\,|\,0)	=& \prob(Y=y\,|\,X=0)	& 1/3	& 2/3	& 0		\\ \hline
%f_{Y|X}(y\,|\,1)	=& \prob(Y=y\,|\,X=1)	& 1/3	& 0		& 2/3	\\ \hline
%f_{Y|X}(y\,|\,2)	=& \prob(Y=y\,|\,X=2)	& 1/3	& 2/3	& 0		\\ \hline
%\end{array}\]
\[\begin{array}{|c|c|c|c|}\hline
					& Y=2	& Y=3	& Y=4	\\ \hline 
f_{Y|X}(y\,|\,0)	& 1/3	& 2/3	& 0		\\ \hline
f_{Y|X}(y\,|\,1)	& 1/3	& 0		& 2/3	\\ \hline
f_{Y|X}(y\,|\,2)	& 1/3	& 2/3	& 0		\\ \hline
\end{array}\]
\end{solution}

%----------------------------------------------------------------------
\section{Conditional expectation}
%----------------------------------------------------------------------
Let $x$ be a value such that $\prob(X=x)>0$. 
\begin{definition}
\ben
\it The \emph{conditional expectation of $Y$ given $X=x$} is a number,
\[
\expe(Y|X=x) 
	= \sum_{j=1}^n y_j\,f_{Y|X}(y_j|x).
%	= \sum_y y\,\prob(Y=y|X=x).
\]
\it The \emph{conditional expectation of $Y$ given $X$} is a random variable,
\[\begin{array}{llll}
\expe(Y|X):	& \Omega 	& \to 		& \R \\
			& \omega		& \mapsto 	& \expe\big(Y|X=X(\omega)\big).
\end{array}\]
\een
\end{definition}

% remark
\begin{remark}
Let $g(x) = \expe(Y|X=x)$. The distribution of the random variable $g(X)=\expe(Y|X)$ depends only on the distribution of $X$, and by Theorem~\ref{thm:lus}, 
%\par
%Then $g(X) = \expe(Y|X)$ is a transformation of $X$, and by Theorem~\ref{thm:lus}, 
%Let $\psi(x) = \expe(Y|X=x)$. Then $\expe(Y|X)=\psi(X)$ is a random variable, whose distribution depends only on the distribution of $X$, and whose expected value is
%The distribution of $\expe(Y|X)$ depends only on the distribution of $X$, and its expected value is given by
%\it By Theorem~\ref{thm:lus}, 
\[
\expe\big[\expe(Y|X)\big] \equiv \expe\big[g(X)\big] = \sum_{i=1}^m g(x_i) f_X(x_i) \equiv \sum_{i=1}^m \expe(Y|X=x_i) f_X(x_i)
\]
where $f_X$ is the marginal distribution of $X$.
%\eit
\end{remark}

%----------------------------------------------------------------------
\section{Law of total expectation}
%----------------------------------------------------------------------

% theorem
\begin{theorem}[The law of total expectation]\label{thm:law_of_total_expectation}
For any two simple random variables $X$ and $Y$,
\[
\expe\big[\expe(Y|X)\big] = \expe(Y).
\]
\end{theorem}
\begin{proof}
%Let $\psi(x) = \expe(Y|X=x)$.
\begin{align*}
\expe\big[\expe(Y|X)\big] 
	= \sum_{i=1}^m \expe(Y|X=x_i) f_X(x_i)
	& = \sum_{i=1}^m \left(\sum_{j=1}^n y_j f_{Y|X}(y_j|x_i)\right) f_X(x_i) \\
	& = \sum_{j=1}^n y_j \left(\sum_{i=1}^m f_{Y|X}(y_j|x_i)f_X(x_i)\right) \\
	& = \sum_{j=1}^n y_j \left(\sum_{i=1}^m f_{X,Y}(x_i,y_j)\right)
	= \sum_{j=1}^n y_j f_Y(y_j)
	= \expe(Y).
\end{align*}
\qed
\end{proof}

\begin{remark}
The law of total expectation provides a useful way of computing $\expe(Y)$:
\[
\expe(Y) = \sum_{i=1}^m \expe(Y|X=x_i)\prob(X=x_i).
\]
This is analogous to the \emph{partition theorem}.
\end{remark}

% tedious example (continued)
\begin{example}
Consider the random variables $X$ and $Y$ of Example~\ref{ex:cond-dist-tedious}.
\ben
\it Find the conditional expectation $Y$ given that (i) $X=1$, (ii) $X=2$ and (iii) $X=3$.
\it Find the distribution of $\expe(Y|X)$, and verify that the law of total expectation holds.
\een
\end{example}

\begin{solution}
The conditional expectation of $Y$ given that $X=1$, $X=2$ and $X=3$ are, respectively,
\bit
\it $\expe(Y|X=1) = \left(2\times\frac{1}{3}\right) + \left(3\times\frac{2}{3}\right) + \left(4\times 0\right) = \frac{8}{3}$,
\it $\expe(Y|X=2) = \left(2\times\frac{1}{3}\right) + \left(3\times 0\right) + \left(4\times\frac{2}{3}\right) = \frac{10}{3}$,
\it $\expe(Y|X=3) = \left(2\times\frac{1}{3}\right) + \left(3\times\frac{2}{3}\right) + \left(4\times 0\right) = \frac{8}{3}$.
\eit

The marginal distribution of $X$, along with the associated values of $\expe(Y|X=x)$, are shown in the following table.
\[\begin{array}{|c|ccc|}\hline
x				& 1		& 2		& 3		\\ \hline
\prob(X=x)		& 1/4	& 1/2	& 1/4	\\ \hline \hline
\expe(Y|X=x)		& 8/3	& 10/3	& 8/3	\\ \hline
\end{array}\]
Hence the distribution of $\expe(Y|X)$ is
\[\begin{array}{|c|cc|}\hline
z							& 8/3   & 10/3 \\ \hline
\prob\big(\expe(Y|X)=z\big)	& 1/2   & 1/2  \\ \hline
\end{array}\]
and the expected value of $\expe(Y|X)$ is therefore 
\[
\expe\big[\expe(Y\,|\,X)\big] = \left(\frac{8}{3}\times\frac{1}{2}\right) + \left(\frac{10}{3}\times\frac{1}{2}\right) = 3.
\]
This agrees with the value of $\expe(Y)$ computed from the marginal distribution of $Y$, which verifies that the law of total expectation holds in this case. 
\end{solution}


%----------------------------------------------------------------------
\section{Exercises}
\input{ex14_conditional_distributions}
%----------------------------------------------------------------------

%======================================================================
\endinput
%======================================================================
