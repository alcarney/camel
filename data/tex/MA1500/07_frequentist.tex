% !TEX root = main.tex
%======================================================================
\chapter{The Frequentist Model}\label{chap:freq}
%======================================================================

%----------------------------------------------------------------------
\section{Finite probability spaces}
%----------------------------------------------------------------------
For the next few lectures, we will consider random experiments which have only finitely many outcomes.
\bit
\it Let $\Omega = \{\omega_1,\omega_2,\ldots.\omega_n\}$ denote the sample space.
\eit
The power set $\mathcal{P}(\Omega)$ contains all events of interest. To define a probability measure on $\mathcal{P}(\Omega)$, we first define a \emph{probability mass function} on the individual outcomes:
\begin{definition}\label{def:pmf_on_sample_space}
A \emph{probability mass function} on $\Omega$ is a function 
\[
\begin{array}{rccl}
	p :	& \Omega		& \to		& [0,1] \\
		& \omega		& \mapsto	& p(\omega)
\end{array}
%\quad\text{with the property that}
%\sum_{\omega\in\Omega} p(\omega) = 1.
\]
with the property that $\displaystyle\sum_{\omega\in\Omega} p(\omega) = 1$.
\end{definition}

\begin{remark}
In classical probability, we choose a sample space in which all outcomes are equally likely. The probability mass function is then taken to be $p(\omega)=1/n$, where $n$ is the cardinality of the sample space. 
\end{remark}

The probability mass function on $\Omega$ can be extended to a probability measure on $\mathcal{P}(\Omega)$ by defining the probability of an event to be the sum of the probabilities of the outcomes it contains:

\begin{theorem}
Let $\Omega$ be a finite sample space, and let $p$ be a probability mass function on $\Omega$. Then the function 
\[
\begin{array}{rccl}
	\prob:	& \mathcal{P}(\Omega)	& \to		& [0,1] \\[1ex]
			& A						& \mapsto	& \displaystyle\sum_{\omega\in A} p(\omega)
\end{array}
\]
is a probability measure on subsets of $\Omega$.
\end{theorem}

\newpage

\begin{proof}
First, because $p$ is a probability mass function, we see that
\[
\prob(\Omega) = \sum_{\omega\in\Omega}p(\omega) = 1 
\]
To prove additivity, let $A_1,A_2,\ldots,A_n$ be pairwise disjoint subsets of $\Omega$. (Note that because $\Omega$ is finite, it has only finitely many subsets.) Then
\begin{align*}
\prob\left(\bigcup_{i=1}^n A_i\right)
	& = \sum_{\omega\in A_1\cup A_2\cup\ldots\cup A_n} p(\omega) \\
	& = \sum_{\omega\in A_1}p(\omega) + \sum_{\omega\in A_2}p(\omega) + \ldots + \sum_{\omega\in A_n}p(\omega) \\
	& = \prob(A_1) + \prob(A_2) + \ldots +\prob(A_n) \\
	& = \sum_{i=1}^n \prob(A_i).
\end{align*}
Thus $\prob$ satisfies the conditions required for it to be a probability measure.
\end{proof}

%----------------------------------------------------------------------
\section{Relative frequency}
%----------------------------------------------------------------------
We have seen that probability is a measure of how likely an event is to occur.

\bit
\it The \emph{symmetry} that exists in many random experiments (e.g. those involving dice and cards) allows us to choose sensible values for the probability of various events.
\it How can we define probability in more general situations?
\eit

If a random experiment can be repeated many times under the same conditions, it is natural to think of probability as the number of times an event occurs expressed as a proportion of the total number times the experiment is repeated.
\bit
\it Let $N$ be the number of times the experiment is repeated. 
\it Let $N(A)$ be the number of times event $A$ occurs during these $N$ repetitions. 
\eit

\begin{definition}
The ratio $N(A)/N$ is called the \emph{relative frequency} of event $A$. 
\end{definition}

% defn: frequentist probability
\begin{definition}%[Frequentist probability]
Under the \emph{frequentist model}, the probability of an event $A$ is defined to be the limit of its relative frequency as the number of trials increases to infinity:
\[
\prob(A) = \lim_{N\to\infty} \frac{N(A)}{N}.
\]
\end{definition}

The frequentist model dominates in many areas of science (e.g.\ medical trials). 
%In practical applications, the experiment is repeated many times, then the relative frequency $N(A)/N$ of an event $A$ is taken as an approximation of its ``true'' probability.

\bit
%\it The frequentist model dominates in many areas of science (e.g.\ medical trials).
\it In practical applications, the experiment is repeated many times, then the relative frequency $N(A)/N$ of an event $A$ is taken as an approximation of its ``true'' probability.
\eit

Frequentist probability has nice properties:
\bit
\it $0\leq \prob(A)\leq 1$ for every event $A\in\mathcal{P}(\Omega)$.
\it $\prob(\emptyset)=0$, because $N(\emptyset)=0$ for any number of repetitions $N$.
\it $\prob(\Omega)=1$, because $N(\Omega)=N$ for any number of repetitions $N$.
\eit

\begin{theorem}
Let $\Omega$ be a finite sample space. Then the function 
\[
\begin{array}{rccl}
\prob:	& \mathcal{P}(\Omega)	& \to		& [0,1] \\[1ex]
	& A						& \mapsto	& \displaystyle\lim_{N\to\infty}\frac{N(A)}{N}
\end{array}
\]
is a probability measure on subsets of $\Omega$.
\end{theorem}
\begin{proof}
Clearly, $\prob(\Omega)=1$, because $N(\Omega)=N$ for any number of repetitions $N$.
\par
To prove additivity, let $A_1,A_2,\ldots,A_n$ be pairwise disjoint subsets of $\Omega$. (Note that because $\Omega$ is finite, it has only finitely many subsets.) Then
\begin{align*}
\prob\left(\bigcup_{i=1}^n A_i\right)
	& = \lim_{N\to\infty}\frac{N(A_1\cup  A_2\cup\ldots\cup A_n)}{N}  \\
	& = \lim_{N\to\infty}\frac{N(A_1) + N(A_2) + \ldots + N(A_n)}{N} \qquad\text{(because the $A_i$ are disjoint)}  \\
	& = \lim_{N\to\infty}\frac{N(A_1)}{N} + \lim_{N\to\infty}\frac{N(A_2)}{N} + \ldots + \lim_{N\to\infty}\frac{N(A_n)}{N} \\
	& = \prob(A_1) + \prob(A_2) + \ldots +\prob(A_n) \\
	& = \sum_{i=1}^n \prob(A_i).
\end{align*}
Thus $\prob$ is a probability measure.
\end{proof}

\vspace*{2ex}
% corollary
Because $\prob$ is a probability measure, it has the following properties:
\begin{corollary}\label{cor:prop_freq_prob}
\ben
\it Complementarity: $\prob(A^c) = 1 - \prob(A)$.
\it $\prob(\emptyset) = 0$,
\it Monotonicity: if $A\subseteq B$ then $\prob(A)\leq \prob(B)$.
\it Addition rule: $\prob(A\cup B) = \prob(A) + \prob(B) - \prob(A\cap B)$.
\een
\end{corollary}

\begin{remark}
These properties can be proved directly. For example:
\[
\prob(A^c) = \lim_{N\to\infty}\frac{N(A^c)}{N}= \lim_{N\to\infty}\frac{N-N(A)}{N} = 1-\lim_{N\to\infty}\frac{N(A)}{N} = 1 - \prob(A).
\]
\end{remark}

%----------------------------------------------------------------------
\section{Conditional probability}
%----------------------------------------------------------------------
Under the frequentist model, a natural definition of $\prob(A|B)$ is the number of trials in which $A$ and $B$ both occur, expressed as a proportion of the number of trials in which $B$ occurs.

\bit
\it Let $N$ be the number of times the experiment is repeated. 
\it Let $N(A)$ be the number of times that event $A$ occurs. 
\it Let $N(B)$ be the number of times that event $B$ occurs. 
\it Let $N(A,B)$ be the number of times that events $A$ and $B$ both occur. 
\eit

Then
\[
\prob(A|B)	= \lim_{N\to\infty}\frac{N(A,B)}{N(B)} 
		= \lim_{N\to\infty}\frac{N(A,B)/N}{N(B)/N} 
		= \frac{\prob(A\cap B)}{\prob(B)},
\]
which agrees with the definition of conditional probability given previously.
  
%----------------------------------------------------------------------
\newpage
\section{Infinite sample spaces}
%----------------------------------------------------------------------
The frequentist model does not extend to random experiments that have infinitely many outcomes.

\begin{example}\label{ex:infinitesamplespace}
Consider a random experiment in which a coin is tossed repeatedly until the first head occurs. 
%\bit
%\it Let the outcome of the experiment be the number of times that the coin is tossed.
%\eit
The sample space for this experiment is an infinite set. Two possible representations are:
\bit
\it $\Omega = \{H,TH,TTH,TTTH,TTTTH,TTTTTH,\ldots\}$, or alternatively,
\it $\Omega = \{1,2,3,4,5,\ldots\}$.
\eit
\end{example}

\par\vspace*{2ex}
Let $N$ be the number of times that the random experiment is repeated.
\bit
\it For finite sample spaces, we can choose $N$ sufficiently large to ensure that every possible outcome can occur at least once.
%$p(\omega) \gg 1/N$ for all $\omega\in\Omega$.
\it For infinite sample spaces, there are plausible outcomes that will not occur, regardless of how large we choose $N$. 
%\bit
%\it We cannot define their probability in terms of relative frequeency.
\eit

Thus we cannot define probability on infinite sample spaces in terms of relative frequency.


%\break % <<
%
%\begin{examplecont}{\ref{ex:infinitesamplespace}}
%Let $p$ be is the probability of observing a head in any particular trial.  
%If we assume that the trials are independent, we can assign a probability to each outcome:
%\[
%P(\{H\})= p,\quad P(\{TH\})=p(1-p), \quad P(\{TTH\}) = p(1-p)^2, \quad\text{and so on.}
%\]
%When $0<p<1$, each of these probabilities is non-zero, and their sum is equal to 1:
%\begin{align*}
%P(\{H\}) + P(\{TH\}) + P(\{TTH\}) + \ldots
%	& = p + p(1-p) + p(1-p)^2 + \ldots \\
%	& = p\sum_{k=0}^{\infty}(1-p)^k \\
%	& = \frac{p}{1-(1-p)} = 1.
%\end{align*}
%\end{examplecont}
%
%This is all very well, but what does ``the probability of observing a head'' actually mean?

%%----------------------------------------------------------------------
%\section{The Bayesian model}
%%----------------------------------------------------------------------
%\subsubsection*{Frequentist model (Neymann/Pearson/Wald)}
%
%Data are variable, parameters are fixed:
%\bit
%\it Data are observed from repeatable random experiments.
%\it Unknown parameters are assumed to be constant across repetitions.
%\eit
%
%\subsubsection*{Bayesian model (Bayes/Laplace/de Finetti)}
%
%Data are fixed, parameters are variable
%\bit
%\it Data are observed from a realized sample.
%\it Unknown parameters are described probabilistically.
%\eit

%%----------------------------------------------------------------------
%\section{Exercises}
%\input{ex07_frequentist}
%%----------------------------------------------------------------------

%======================================================================
\endinput
%======================================================================
